{"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMIUVTL4orxQa7aoNhrkqhl","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","gpuClass":"standard","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5527494,"sourceType":"datasetVersion","datasetId":3186665},{"sourceType":"datasetVersion","sourceId":8409117,"datasetId":5004504}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/CS22M029/cs6910_assignment3/blob/main/VanilaSeq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"!pip install wandb\nimport wandb\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:36:44.394189Z","iopub.execute_input":"2024-05-14T09:36:44.394850Z","iopub.status.idle":"2024-05-14T09:37:10.805141Z","shell.execute_reply.started":"2024-05-14T09:36:44.394822Z","shell.execute_reply":"2024-05-14T09:37:10.804163Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Import necessary packages\nimport os\nimport torch\nimport random\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as Function\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n# from google.colab import files \n\n# Check if CUDA is available\nuse_cuda = torch.cuda.is_available()\n\n# Set the device type to CUDA if available, otherwise use CPU\nif use_cuda:\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\n# Define constants for special cases\nStart_Symbol, End_Symbol, Unknown, Padding = 0, 1, 2, 3\n\n#Define a class for a Vocabulary that will hold mappings between characters and their indices\nclass Vocabulary:\n    def __init__(self):\n        self.char2count = {}\n        self.char2index = {}\n        self.n_chars = 4\n        self.index2char = {0: \"<\", 1: \">\", 2: \"?\", 3: \".\"}\n\n    def addWord(self, word):\n        for char in word:\n            if char not in self.char2index:\n                self.char2index[char] = self.n_chars\n                self.index2char[self.n_chars] = char\n                self.char2count[char] = 1\n                self.n_chars += 1\n            else:\n                self.char2count[char] += 1\n\n\n# Define a function to prepare the data\ndef prepareData(dir):\n    # Upload the CSV file\n#     print(\"Upload\", dir)\n#     uploaded = files.upload()\n    \n    # Read the CSV file into a DataFrame with columns \"input\" and \"target\"\n    data = pd.read_csv(dir, sep=\",\", names=[\"input\", \"target\"])\n\n\n    # Find the maximum length of input and target sequences\n    max_input_length = max([len(txt) for txt in data[\"input\"].to_list()])\n    max_target_length = max([len(txt) for txt in data[\"target\"].to_list()])\n    max_len=max(max_input_length,max_target_length)\n\n    # Create Vocabulary objects for input and output languages\n    input_lang = Vocabulary()\n    output_lang = Vocabulary()\n\n    # Create pairs of input and target sequences\n    pairs = []\n    input_list, target_list = data[\"input\"].to_list(), data[\"target\"].to_list()\n    for i in range(len(input_list)):\n        pairs.append([input_list[i], target_list[i]])\n\n    # Add words to the respective vocabularies\n    for pair in pairs:\n        input_lang.addWord(pair[0])\n        output_lang.addWord(pair[1])\n\n    # Create a dictionary containing prepared data\n    prepared_data = {\n        \"input_lang\": input_lang,\n        \"output_lang\": output_lang,\n        \"pairs\": pairs,\n        \"max_len\": max_len\n    }\n\n    return prepared_data\n\n# Define a helper function to convert a word to a tensor\ndef helpTensor(lang, word, max_length):\n    index_list = []\n    for char in word:\n        if char in lang.char2index.keys():\n            index_list.append(lang.char2index[char])\n        else:\n            index_list.append(Unknown)\n    indexes = index_list\n    indexes.append(End_Symbol)\n    indexes.extend([Padding] * (max_length - len(indexes)))\n    result = torch.LongTensor(indexes)\n    if use_cuda:\n        return result.cuda()\n    else:\n        return result\n\n# Define a function to convert pairs of input and target sequences to tensors\ndef MakeTensor(input_lang, output_lang, pairs, reach):\n    res = []\n    for pair in pairs:\n        # Convert input and target sequences to tensors using the helpTensor function\n        input_variable = helpTensor(input_lang, pair[0], reach)\n        target_variable = helpTensor(output_lang, pair[1], reach)\n        res.append((input_variable, target_variable))\n    return res\n\n#Encoder Class\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers_encoder, cell_type, drop_out, bi_directional):\n        super(EncoderRNN, self).__init__()\n\n        # Initialize the EncoderRNN with the provided parameters\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.num_layers_encoder = num_layers_encoder\n        self.cell_type = cell_type\n        self.drop_out = drop_out\n        self.bi_directional = bi_directional\n\n        # Create an embedding layer\n        self.embedding = nn.Embedding(input_size, self.embedding_size)\n        self.dropout = nn.Dropout(self.drop_out)\n\n        # Create the specified cell layer (RNN, GRU, or LSTM)\n        cell_map = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}\n        self.cell_layer = cell_map[self.cell_type](\n            self.embedding_size,\n            self.hidden_size,\n            num_layers=self.num_layers_encoder,\n            dropout=self.drop_out,\n            bidirectional=self.bi_directional,\n        )\n\n    def forward(self, input, batch_size, hidden):\n        # Apply dropout to the embedded input sequence\n        embedded = self.dropout(self.embedding(input).view(1, batch_size, -1))\n\n        # Pass the embedded input through the cell layer\n        output, hidden = self.cell_layer(embedded, hidden)\n        return output, hidden\n\n    def initHidden(self, batch_size, num_layers_enc):\n        # Initialize the hidden state with zeros\n        res = torch.zeros(num_layers_enc * 2 if self.bi_directional else num_layers_enc, batch_size, self.hidden_size)\n\n        # Move the hidden state to the GPU if use_cuda is True, else return as is\n        return res.cuda() if use_cuda else res\n\n#Decoder class\nclass DecoderRNN(nn.Module):\n    def __init__(self, embedding_size, hidden_size, num_layers_decoder, cell_type, drop_out, bi_directional, output_size):\n        super(DecoderRNN, self).__init__()\n\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.num_layers_decoder = num_layers_decoder\n        self.cell_type = cell_type\n        self.drop_out = drop_out\n        self.bi_directional = bi_directional\n\n        # Create an embedding layer\n        self.embedding = nn.Embedding(output_size, self.embedding_size)\n        self.dropout = nn.Dropout(self.drop_out)\n\n        # Create the specified cell layer (RNN, GRU, or LSTM)\n        cell_map = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}\n        self.cell_layer = cell_map[self.cell_type](\n            self.embedding_size,\n            self.hidden_size,\n            num_layers=self.num_layers_decoder,\n            dropout=self.drop_out,\n            bidirectional=self.bi_directional,\n        )\n\n        # Linear layer for output\n        self.out = nn.Linear(\n            self.hidden_size * 2 if self.bi_directional else self.hidden_size,\n            output_size,\n        )\n\n        # Softmax activation\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, batch_size, hidden):\n        # Apply dropout to the embedded input sequence and pass it through the cell layer\n        output = Function.relu(self.dropout(self.embedding(input).view(1, batch_size, -1)))\n        output, hidden = self.cell_layer(output, hidden)\n\n        # Apply softmax activation to the output\n        output = self.softmax(self.out(output[0]))\n        return output, hidden\n\n# Function to calculate loss (if is_training then training loss else validation loss)\ndef calc_loss(encoder, decoder, input_tensor, target_tensor, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training, teacher_forcing_ratio=0.5):\n    # Initialize the encoder hidden state\n    encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n    # Check if LSTM and initialize cell state\n    if cell_type == \"LSTM\":\n        encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n        encoder_hidden = (encoder_hidden, encoder_cell_state)\n\n    # Zero the gradients\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    # Get input and target sequence lengths\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n\n    # Initialize loss\n    loss = 0\n\n    # Encoder forward pass\n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(input_tensor[ei], batch_size, encoder_hidden)\n\n    # Initialize decoder input\n    decoder_input = torch.LongTensor([Start_Symbol] * batch_size)\n    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n    # Set decoder hidden state\n    decoder_hidden = encoder_hidden\n\n    # Determine if using teacher forcing\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    # Loop over target sequence\n    if is_training:\n        # Training phase\n        for di in range(target_length):\n            decoder_output, decoder_hidden = decoder(decoder_input, batch_size, decoder_hidden)\n            loss += criterion(decoder_output, target_tensor[di])\n            decoder_input = target_tensor[di] if use_teacher_forcing else decoder_output.argmax(dim=1)\n    else:\n        # Validation phase\n        with torch.no_grad():\n            for di in range(target_length):\n                decoder_output, decoder_hidden = decoder(decoder_input, batch_size, decoder_hidden)\n                loss += criterion(decoder_output, target_tensor[di])\n                decoder_input = decoder_output.argmax(dim=1)\n\n    # Backpropagation and optimization in training phase\n    if is_training:\n        loss.backward()\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n\n    # Return the average loss per target length\n    return loss.item() / target_length\n\n\n# Calculate the accuracy of the Seq2Seq model\ndef accuracy(encoder, decoder, loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang):\n    with torch.no_grad():\n        total = 0\n        correct = 0\n\n        for batch_x, batch_y in loader:\n            # Initialize encoder hidden state\n            encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n            input_variable = Variable(batch_x.transpose(0, 1))\n            target_variable = Variable(batch_y.transpose(0, 1))\n\n            # Check if LSTM and initialize cell state\n            if cell_type == \"LSTM\":\n                encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n                encoder_hidden = (encoder_hidden, encoder_cell_state)\n\n            input_length = input_variable.size()[0]\n            target_length = target_variable.size()[0]\n\n            output = torch.LongTensor(target_length, batch_size)\n\n            # Initialize encoder outputs\n            encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hidden_size))\n            encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n            # Encoder forward pass\n            for ei in range(input_length):\n                encoder_output, encoder_hidden = encoder(input_variable[ei], batch_size, encoder_hidden)\n\n            decoder_input = Variable(torch.LongTensor([Start_Symbol] * batch_size))\n            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n            decoder_hidden = encoder_hidden\n\n            # Decoder forward pass\n            for di in range(target_length):\n                decoder_output, decoder_hidden = decoder(decoder_input, batch_size, decoder_hidden)\n                topv, topi = decoder_output.data.topk(1)\n                decoder_input = torch.cat(tuple(topi))\n                output[di] = torch.cat(tuple(topi))\n\n            output = output.transpose(0, 1)\n\n            # Calculate accuracy\n            for di in range(output.size()[0]):\n                ignore = [Start_Symbol, End_Symbol, Padding]\n                sent = [output_lang.index2char[letter.item()] for letter in output[di] if letter not in ignore]\n                y = [output_lang.index2char[letter.item()] for letter in batch_y[di] if letter not in ignore]\n                if sent == y:\n                    correct += 1\n                total += 1\n\n    return (correct / total) * 100\n\n# Train and evaluate the Seq2Seq model\ndef seq2seq(encoder, decoder, train_loader, val_loader, test_loader, lr, optimizer, epochs, max_length_word, num_layers_enc, output_lang):\n    max_length = max_length_word - 1\n    # Define the optimizer and criterion\n    encoder_optimizer = optim.NAdam(encoder.parameters(), lr=lr) if optimizer == \"nadam\" else optim.Adam(encoder.parameters(), lr=lr)\n    decoder_optimizer = optim.NAdam(decoder.parameters(), lr=lr) if optimizer == \"nadam\" else optim.Adam(decoder.parameters(), lr=lr)\n    criterion = nn.NLLLoss()\n\n    for epoch in range(epochs):\n        train_loss_total = 0\n        val_loss_total = 0\n\n        # Training phase\n        for batch_x, batch_y in train_loader:\n            batch_x = Variable(batch_x.transpose(0, 1))\n            batch_y = Variable(batch_y.transpose(0, 1))\n            # Calculate the training loss\n            loss = calc_loss(encoder, decoder, batch_x, batch_y, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training=True)\n            train_loss_total += loss\n\n        train_loss_avg = train_loss_total / len(train_loader)\n        print(f\"Epoch: {epoch} | Train Loss: {train_loss_avg:.4f} |\", end=\"\")\n        \n        # Validation phase\n        for batch_x, batch_y in val_loader:\n            batch_x = Variable(batch_x.transpose(0, 1))\n            batch_y = Variable(batch_y.transpose(0, 1))\n            # Calculate the validation loss\n            loss = calc_loss(encoder, decoder, batch_x, batch_y, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training=False)\n            val_loss_total += loss\n\n        val_loss_avg = val_loss_total / len(val_loader)\n        print(f\"Val Loss: {val_loss_avg:.4f} |\", end=\"\")\n\n        # Calculate validation accuracy\n        val_acc = accuracy(encoder, decoder, val_loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang)\n        val_acc /= 100\n        print(f\"Val Accuracy: {val_acc:.4%}\")\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"training_loss\": train_loss_avg,\n            \"validation_accuracy\": val_acc,\n            \"validation_loss\": val_loss_avg\n        })\n#         if epoch == 14:\n    test_acc = accuracy(encoder, decoder, test_loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang)\n    test_acc /= 100\n    print(f\"Test Accuracy: {test_acc:.4%}\")\n    wandb.log({\"test_accuracy\": test_acc})\n\n# Define model hyperparameters\n# hidden_size = 512\n# input_lang = \"eng\"\n# target_lang = \"hin\"\n# cell_type = \"LSTM\"\n# num_layers_encoder = 3\n# num_layers_decoder = 3\n# drop_out = 0\n# epochs = 5\n# embedding_size = 64\n# bi_directional = True\n# batch_size = 512\nteacher_forcing_ratio = 0.5\noptimizer = \"Sgd\"\nlearning_rate = 0.001 \n\nhidden_size = 512\ninput_lang = \"eng\"\ntarget_lang = \"hin\"\ncell_type = \"LSTM\"\nnum_layers_encoder = 3\nnum_layers_decoder = 3\ndrop_out = 0.2\nepochs = 15\nembedding_size = 256\nbi_directional = True\nbatch_size = 1024\n\ntrain_path = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_train.csv\"\nvalidation_path = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_valid.csv\"\ntest_path = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_test.csv\"\n\n# Prepare training data\ntrain_prepared_data = prepareData(train_path)\ninput_langs, output_langs, pairs = train_prepared_data[\"input_lang\"], train_prepared_data[\"output_lang\"], train_prepared_data[\"pairs\"]\nprint(\"train:sample:\", random.choice(pairs))\nprint(f\"Number of training examples: {len(pairs)}\")\nmax_len = train_prepared_data[\"max_len\"]\n\n# Prepare validation data\nval_prepared_data = prepareData(validation_path)\nval_pairs = val_prepared_data[\"pairs\"]\nprint(\"validation:sample:\", random.choice(val_pairs))\nprint(f\"Number of validation examples: {len(val_pairs)}\")\nmax_len_val = val_prepared_data[\"max_len\"]\n\n# Prepare test data\ntest_prepared_data = prepareData(test_path)\ntest_pairs = test_prepared_data[\"pairs\"]\nprint(\"Test:sample:\", random.choice(test_pairs))\nprint(f\"Number of Test examples: {len(test_pairs)}\")\n\nmax_len_test = test_prepared_data[\"max_len\"]\nmax_len = max(max_len, max_len_val, max_len_test) + 4\nprint(max_len)\n\n# Convert data to tensors and create data loaders\npairs = MakeTensor(input_langs, output_langs, pairs, max_len)\nval_pairs = MakeTensor(input_langs, output_langs, val_pairs, max_len)\ntest_pairs = MakeTensor(input_langs, output_langs, test_pairs, max_len)\n\ntrain_loader = DataLoader(pairs, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_pairs, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_pairs, batch_size=batch_size, shuffle=True)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"-7TWV91kjVJo","outputId":"e960e157-b440-4f50-f366-eb7b2548d093","execution":{"iopub.status.busy":"2024-05-14T09:37:15.197721Z","iopub.execute_input":"2024-05-14T09:37:15.198237Z","iopub.status.idle":"2024-05-14T09:37:20.164630Z","shell.execute_reply.started":"2024-05-14T09:37:15.198201Z","shell.execute_reply":"2024-05-14T09:37:20.163559Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"train:sample: ['tripurdah', 'त्रिपुरदाह']\nNumber of training examples: 51200\nvalidation:sample: ['bachayen', 'बचाएं']\nNumber of validation examples: 4096\nTest:sample: ['nirdeshikaa', 'निर्देशिका']\nNumber of Test examples: 4096\n30\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Create the encoder and decoder models\nencoder1 = EncoderRNN(input_langs.n_chars, embedding_size, hidden_size, num_layers_encoder, cell_type, drop_out, bi_directional)\ndecoder1 = DecoderRNN(embedding_size, hidden_size, num_layers_encoder, cell_type, drop_out, bi_directional, output_langs.n_chars)\nprint(use_cuda)\nif use_cuda:\n   encoder1, decoder1 = encoder1.cuda(), decoder1.cuda()\n\nprint(\"vanilla seq2seq\")\n# Train and evaluate the Seq2Seq model\n# seq2seq(encoder1, decoder1, train_loader, val_loader, test_loader, learning_rate, optimizer, epochs, max_len, num_layers_encoder, output_langs)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MyfWMRLipryL","outputId":"f00aeb79-0fd4-433b-a73b-eb5cf59e91e3","execution":{"iopub.status.busy":"2024-05-14T09:37:35.090326Z","iopub.execute_input":"2024-05-14T09:37:35.090703Z","iopub.status.idle":"2024-05-14T09:37:35.444779Z","shell.execute_reply.started":"2024-05-14T09:37:35.090672Z","shell.execute_reply":"2024-05-14T09:37:35.443850Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"True\nvanilla seq2seq\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loader","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:37:40.207099Z","iopub.execute_input":"2024-05-14T09:37:40.207539Z","iopub.status.idle":"2024-05-14T09:37:40.213516Z","shell.execute_reply.started":"2024-05-14T09:37:40.207511Z","shell.execute_reply":"2024-05-14T09:37:40.212611Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<torch.utils.data.dataloader.DataLoader at 0x7875632e37c0>"},"metadata":{}}]},{"cell_type":"code","source":"sweep_configuration = {\n    \"method\": \"bayes\",\n    \"metric\": {\n        \"name\": \"validation_accuracy\",\n        \"goal\": \"maximize\"\n    },\n    \"parameters\": {\n        \"embed_size\": {\n            \"values\": [32,64,128]\n        },\n        \"hidden_size\": {\n            \"values\": [128, 256, 512]\n        },\n        \"cell_type\": {\n            \"values\": [\"GRU\", \"LSTM\", \"RNN\"]\n        },\n        \"num_layers\": {\n            \"values\": [1, 2, 3]\n        },\n        \"dropout\": {\n            \"values\": [0, 0.1, 0.2]\n        },\n        \"learning_rate\": {\n            \"values\": [0.0005, 0.001, 0.005]\n        },\n        \"optimizer\": {\n            \"values\": [\"Sgd\", \"Adam\",\"Nadam\"]\n        },\n        \"teacher_forcing_ratio\": {\n            \"values\": [0.5, 0.75, 0.25]\n        }\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:37:43.775801Z","iopub.execute_input":"2024-05-14T09:37:43.776699Z","iopub.status.idle":"2024-05-14T09:37:43.785236Z","shell.execute_reply.started":"2024-05-14T09:37:43.776655Z","shell.execute_reply":"2024-05-14T09:37:43.784196Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 0\n\ndef train_sweep():\n    global count\n    count += 1\n\n    run = wandb.init()\n    config = wandb.config\n    run.name = \"embed_size {}_hidden_size {}_cell_type {}_num_layers {} _dropout {} _learning_rate {} _optimizer {} _teacher_forcing_ratio {}\".format(config.embed_size, config.hidden_size, config.cell_type, config.num_layers, config.dropout, config.learning_rate, config.optimizer, config.teacher_forcing_ratio)\n    \n                                                                                                                                                      \n    optimizer = config.optimizer\n    learning_rate = config.learning_rate\n    teacher_forcing_ratio = config.teacher_forcing_ratio\n    hidden_size = config.hidden_size\n    input_lang = \"eng\"\n    target_lang = \"hin\"\n    cell_type = \"LSTM\"\n    num_layers_encoder = config.num_layers\n    num_layers_decoder = config.num_layers\n    drop_out = config.dropout\n    epochs = 60\n    embedding_size = config.embed_size\n    bi_directional = False\n    batch_size = 1024\n    \n    # Create the encoder and decoder models\n    encoder1 = EncoderRNN(input_langs.n_chars, embedding_size, hidden_size, num_layers_encoder, cell_type, drop_out, bi_directional)\n    decoder1 = DecoderRNN(embedding_size, hidden_size, num_layers_encoder, cell_type, drop_out, bi_directional, output_langs.n_chars)\n    print(use_cuda)\n    if use_cuda:\n       encoder1, decoder1 = encoder1.cuda(), decoder1.cuda()\n    seq2seq(encoder1, decoder1, train_loader, val_loader, test_loader, learning_rate, optimizer, epochs, max_len, num_layers_encoder, output_langs)\n    run.finish()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:37:50.405103Z","iopub.execute_input":"2024-05-14T09:37:50.405467Z","iopub.status.idle":"2024-05-14T09:37:50.414693Z","shell.execute_reply.started":"2024-05-14T09:37:50.405440Z","shell.execute_reply":"2024-05-14T09:37:50.413804Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"wandb_id = wandb.sweep(sweep_configuration, project=\"DL_A-03_RNN\")\nwandb.agent(wandb_id, train_sweep, count=10)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T09:38:01.467978Z","iopub.execute_input":"2024-05-14T09:38:01.468810Z","iopub.status.idle":"2024-05-14T14:06:47.691446Z","shell.execute_reply.started":"2024-05-14T09:38:01.468775Z","shell.execute_reply":"2024-05-14T14:06:47.690655Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Create sweep with ID: gcp6d86c\nSweep URL: https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wkjhuht8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Sgd\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.75\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma22c019\u001b[0m (\u001b[33mbeliever12\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_093804-wkjhuht8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/wkjhuht8' target=\"_blank\">comic-sweep-1</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/wkjhuht8' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/wkjhuht8</a>"},"metadata":{}},{"name":"stdout","text":"True\nEpoch: 0 | Train Loss: 1.2286 |Val Loss: 0.9960 |Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.0089 |Val Loss: 0.8654 |Val Accuracy: 0.0244%\nEpoch: 2 | Train Loss: 0.9201 |Val Loss: 0.8301 |Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 0.8787 |Val Loss: 0.8300 |Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 0.8305 |Val Loss: 0.7674 |Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 0.7892 |Val Loss: 0.7136 |Val Accuracy: 0.0977%\nEpoch: 6 | Train Loss: 0.7532 |Val Loss: 0.6855 |Val Accuracy: 0.3174%\nEpoch: 7 | Train Loss: 0.6844 |Val Loss: 0.6493 |Val Accuracy: 0.5127%\nEpoch: 8 | Train Loss: 0.6579 |Val Loss: 0.6104 |Val Accuracy: 0.9521%\nEpoch: 9 | Train Loss: 0.6143 |Val Loss: 0.6409 |Val Accuracy: 2.7100%\nEpoch: 10 | Train Loss: 0.6309 |Val Loss: 0.5685 |Val Accuracy: 1.8799%\nEpoch: 11 | Train Loss: 0.5802 |Val Loss: 0.5663 |Val Accuracy: 4.3945%\nEpoch: 12 | Train Loss: 0.5727 |Val Loss: 0.5372 |Val Accuracy: 4.8340%\nEpoch: 13 | Train Loss: 0.5559 |Val Loss: 0.5160 |Val Accuracy: 5.5908%\nEpoch: 14 | Train Loss: 0.5364 |Val Loss: 0.5036 |Val Accuracy: 5.7129%\nEpoch: 15 | Train Loss: 0.5450 |Val Loss: 0.4876 |Val Accuracy: 5.7617%\nEpoch: 16 | Train Loss: 0.5082 |Val Loss: 0.4801 |Val Accuracy: 7.5195%\nEpoch: 17 | Train Loss: 0.4871 |Val Loss: 0.4704 |Val Accuracy: 8.9600%\nEpoch: 18 | Train Loss: 0.4722 |Val Loss: 0.4696 |Val Accuracy: 8.2520%\nEpoch: 19 | Train Loss: 0.4880 |Val Loss: 0.4517 |Val Accuracy: 8.3008%\nEpoch: 20 | Train Loss: 0.4163 |Val Loss: 0.4827 |Val Accuracy: 11.5479%\nEpoch: 21 | Train Loss: 0.4369 |Val Loss: 0.4688 |Val Accuracy: 12.4268%\nEpoch: 22 | Train Loss: 0.4681 |Val Loss: 0.4468 |Val Accuracy: 12.1826%\nEpoch: 23 | Train Loss: 0.4357 |Val Loss: 0.4318 |Val Accuracy: 10.5225%\nEpoch: 24 | Train Loss: 0.4397 |Val Loss: 0.4423 |Val Accuracy: 13.2568%\nEpoch: 25 | Train Loss: 0.4046 |Val Loss: 0.4336 |Val Accuracy: 13.2568%\nEpoch: 26 | Train Loss: 0.4357 |Val Loss: 0.4299 |Val Accuracy: 14.0137%\nEpoch: 27 | Train Loss: 0.4157 |Val Loss: 0.4358 |Val Accuracy: 15.5762%\nEpoch: 28 | Train Loss: 0.3950 |Val Loss: 0.4358 |Val Accuracy: 16.0645%\nEpoch: 29 | Train Loss: 0.4165 |Val Loss: 0.4066 |Val Accuracy: 13.7207%\nEpoch: 30 | Train Loss: 0.4339 |Val Loss: 0.4041 |Val Accuracy: 14.4775%\nEpoch: 31 | Train Loss: 0.4046 |Val Loss: 0.4123 |Val Accuracy: 16.7969%\nEpoch: 32 | Train Loss: 0.4278 |Val Loss: 0.3931 |Val Accuracy: 13.3057%\nEpoch: 33 | Train Loss: 0.3795 |Val Loss: 0.4264 |Val Accuracy: 19.1162%\nEpoch: 34 | Train Loss: 0.3939 |Val Loss: 0.4021 |Val Accuracy: 16.1621%\nEpoch: 35 | Train Loss: 0.3627 |Val Loss: 0.4098 |Val Accuracy: 18.2617%\nEpoch: 36 | Train Loss: 0.3702 |Val Loss: 0.4164 |Val Accuracy: 19.6045%\nEpoch: 37 | Train Loss: 0.3724 |Val Loss: 0.4130 |Val Accuracy: 19.1895%\nEpoch: 38 | Train Loss: 0.3630 |Val Loss: 0.3931 |Val Accuracy: 16.9434%\nEpoch: 39 | Train Loss: 0.3822 |Val Loss: 0.3913 |Val Accuracy: 16.5039%\nEpoch: 40 | Train Loss: 0.3649 |Val Loss: 0.4064 |Val Accuracy: 19.5312%\nEpoch: 41 | Train Loss: 0.3929 |Val Loss: 0.3756 |Val Accuracy: 13.6475%\nEpoch: 42 | Train Loss: 0.3717 |Val Loss: 0.3732 |Val Accuracy: 17.8955%\nEpoch: 43 | Train Loss: 0.3508 |Val Loss: 0.3788 |Val Accuracy: 16.6504%\nEpoch: 44 | Train Loss: 0.3640 |Val Loss: 0.3888 |Val Accuracy: 19.7998%\nEpoch: 45 | Train Loss: 0.3705 |Val Loss: 0.3883 |Val Accuracy: 18.5303%\nEpoch: 46 | Train Loss: 0.3383 |Val Loss: 0.3906 |Val Accuracy: 20.3369%\nEpoch: 47 | Train Loss: 0.3436 |Val Loss: 0.3769 |Val Accuracy: 18.5059%\nEpoch: 48 | Train Loss: 0.3530 |Val Loss: 0.3728 |Val Accuracy: 19.4336%\nEpoch: 49 | Train Loss: 0.3521 |Val Loss: 0.3712 |Val Accuracy: 18.5059%\nEpoch: 50 | Train Loss: 0.3425 |Val Loss: 0.3977 |Val Accuracy: 20.1660%\nEpoch: 51 | Train Loss: 0.3386 |Val Loss: 0.3701 |Val Accuracy: 20.2637%\nEpoch: 52 | Train Loss: 0.3324 |Val Loss: 0.3693 |Val Accuracy: 19.5557%\nEpoch: 53 | Train Loss: 0.3173 |Val Loss: 0.3764 |Val Accuracy: 20.5078%\nEpoch: 54 | Train Loss: 0.3325 |Val Loss: 0.3784 |Val Accuracy: 21.8018%\nEpoch: 55 | Train Loss: 0.3503 |Val Loss: 0.3640 |Val Accuracy: 19.6533%\nEpoch: 56 | Train Loss: 0.3358 |Val Loss: 0.3505 |Val Accuracy: 19.1650%\nEpoch: 57 | Train Loss: 0.3313 |Val Loss: 0.3661 |Val Accuracy: 21.3379%\nEpoch: 58 | Train Loss: 0.3165 |Val Loss: 0.3580 |Val Accuracy: 20.8252%\nEpoch: 59 | Train Loss: 0.3303 |Val Loss: 0.3556 |Val Accuracy: 21.7285%\nTest Accuracy: 18.1885%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▆▇▇▆▇▇▆▇█▇▇█▇█▇██</td></tr><tr><td>validation_loss</td><td>█▇▆▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.18188</td></tr><tr><td>training_loss</td><td>0.33032</td></tr><tr><td>validation_accuracy</td><td>0.21729</td></tr><tr><td>validation_loss</td><td>0.35564</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">comic-sweep-1</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/wkjhuht8' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/wkjhuht8</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_093804-wkjhuht8/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1lwpca6z with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.75\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_095937-1lwpca6z</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/1lwpca6z' target=\"_blank\">icy-sweep-2</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/1lwpca6z' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/1lwpca6z</a>"},"metadata":{}},{"name":"stdout","text":"True\nEpoch: 0 | Train Loss: 1.6915 |Val Loss: 0.9951 |Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.1045 |Val Loss: 0.9631 |Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.0442 |Val Loss: 0.8880 |Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 0.9878 |Val Loss: 0.8470 |Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 0.9680 |Val Loss: 0.8432 |Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 0.9456 |Val Loss: 0.8349 |Val Accuracy: 0.0244%\nEpoch: 6 | Train Loss: 0.9428 |Val Loss: 0.8245 |Val Accuracy: 0.0244%\nEpoch: 7 | Train Loss: 0.9191 |Val Loss: 0.8176 |Val Accuracy: 0.0244%\nEpoch: 8 | Train Loss: 0.9067 |Val Loss: 0.8036 |Val Accuracy: 0.0000%\nEpoch: 9 | Train Loss: 0.8923 |Val Loss: 0.7942 |Val Accuracy: 0.0000%\nEpoch: 10 | Train Loss: 0.8767 |Val Loss: 0.7835 |Val Accuracy: 0.0244%\nEpoch: 11 | Train Loss: 0.8446 |Val Loss: 0.7542 |Val Accuracy: 0.0732%\nEpoch: 12 | Train Loss: 0.8315 |Val Loss: 0.7231 |Val Accuracy: 0.1465%\nEpoch: 13 | Train Loss: 0.7990 |Val Loss: 0.6942 |Val Accuracy: 0.1953%\nEpoch: 14 | Train Loss: 0.7757 |Val Loss: 0.6601 |Val Accuracy: 0.3662%\nEpoch: 15 | Train Loss: 0.7461 |Val Loss: 0.6304 |Val Accuracy: 0.4639%\nEpoch: 16 | Train Loss: 0.7029 |Val Loss: 0.6453 |Val Accuracy: 0.8545%\nEpoch: 17 | Train Loss: 0.6713 |Val Loss: 0.5896 |Val Accuracy: 0.9766%\nEpoch: 18 | Train Loss: 0.6422 |Val Loss: 0.5750 |Val Accuracy: 1.2207%\nEpoch: 19 | Train Loss: 0.6371 |Val Loss: 0.5653 |Val Accuracy: 1.5869%\nEpoch: 20 | Train Loss: 0.6265 |Val Loss: 0.5449 |Val Accuracy: 2.1240%\nEpoch: 21 | Train Loss: 0.5784 |Val Loss: 0.5198 |Val Accuracy: 2.9053%\nEpoch: 22 | Train Loss: 0.5794 |Val Loss: 0.5142 |Val Accuracy: 4.0283%\nEpoch: 23 | Train Loss: 0.5495 |Val Loss: 0.4897 |Val Accuracy: 4.9072%\nEpoch: 24 | Train Loss: 0.5372 |Val Loss: 0.4774 |Val Accuracy: 5.2002%\nEpoch: 25 | Train Loss: 0.5251 |Val Loss: 0.4695 |Val Accuracy: 7.2754%\nEpoch: 26 | Train Loss: 0.5052 |Val Loss: 0.4591 |Val Accuracy: 5.6641%\nEpoch: 27 | Train Loss: 0.4819 |Val Loss: 0.4469 |Val Accuracy: 8.7158%\nEpoch: 28 | Train Loss: 0.4672 |Val Loss: 0.4385 |Val Accuracy: 9.5215%\nEpoch: 29 | Train Loss: 0.4418 |Val Loss: 0.4402 |Val Accuracy: 10.9619%\nEpoch: 30 | Train Loss: 0.4506 |Val Loss: 0.4307 |Val Accuracy: 11.6699%\nEpoch: 31 | Train Loss: 0.4324 |Val Loss: 0.4143 |Val Accuracy: 12.3535%\nEpoch: 32 | Train Loss: 0.4560 |Val Loss: 0.4090 |Val Accuracy: 11.2549%\nEpoch: 33 | Train Loss: 0.4153 |Val Loss: 0.4009 |Val Accuracy: 13.9404%\nEpoch: 34 | Train Loss: 0.4210 |Val Loss: 0.3871 |Val Accuracy: 13.7939%\nEpoch: 35 | Train Loss: 0.4212 |Val Loss: 0.3970 |Val Accuracy: 13.0859%\nEpoch: 36 | Train Loss: 0.3895 |Val Loss: 0.4041 |Val Accuracy: 13.5498%\nEpoch: 37 | Train Loss: 0.3794 |Val Loss: 0.3940 |Val Accuracy: 17.0654%\nEpoch: 38 | Train Loss: 0.4070 |Val Loss: 0.3878 |Val Accuracy: 14.9414%\nEpoch: 39 | Train Loss: 0.3817 |Val Loss: 0.3743 |Val Accuracy: 17.9688%\nEpoch: 40 | Train Loss: 0.3800 |Val Loss: 0.3804 |Val Accuracy: 16.3574%\nEpoch: 41 | Train Loss: 0.3651 |Val Loss: 0.3668 |Val Accuracy: 18.1885%\nEpoch: 42 | Train Loss: 0.3694 |Val Loss: 0.3584 |Val Accuracy: 19.3604%\nEpoch: 43 | Train Loss: 0.3675 |Val Loss: 0.3869 |Val Accuracy: 17.0654%\nEpoch: 44 | Train Loss: 0.3537 |Val Loss: 0.3624 |Val Accuracy: 19.3848%\nEpoch: 45 | Train Loss: 0.3637 |Val Loss: 0.3949 |Val Accuracy: 14.0137%\nEpoch: 46 | Train Loss: 0.3624 |Val Loss: 0.3683 |Val Accuracy: 17.8467%\nEpoch: 47 | Train Loss: 0.3599 |Val Loss: 0.3501 |Val Accuracy: 19.4580%\nEpoch: 48 | Train Loss: 0.3440 |Val Loss: 0.3485 |Val Accuracy: 20.1904%\nEpoch: 49 | Train Loss: 0.3587 |Val Loss: 0.3471 |Val Accuracy: 19.4824%\nEpoch: 50 | Train Loss: 0.3198 |Val Loss: 0.3637 |Val Accuracy: 22.5098%\nEpoch: 51 | Train Loss: 0.3439 |Val Loss: 0.3543 |Val Accuracy: 21.4355%\nEpoch: 52 | Train Loss: 0.3134 |Val Loss: 0.3514 |Val Accuracy: 22.8027%\nEpoch: 53 | Train Loss: 0.3447 |Val Loss: 0.3373 |Val Accuracy: 22.0215%\nEpoch: 54 | Train Loss: 0.3463 |Val Loss: 0.3312 |Val Accuracy: 21.5088%\nEpoch: 55 | Train Loss: 0.3124 |Val Loss: 0.3355 |Val Accuracy: 23.1201%\nEpoch: 56 | Train Loss: 0.3300 |Val Loss: 0.3326 |Val Accuracy: 21.0693%\nEpoch: 57 | Train Loss: 0.3207 |Val Loss: 0.3280 |Val Accuracy: 22.7783%\nEpoch: 58 | Train Loss: 0.2966 |Val Loss: 0.3407 |Val Accuracy: 23.8770%\nEpoch: 59 | Train Loss: 0.3207 |Val Loss: 0.3287 |Val Accuracy: 21.8994%\nTest Accuracy: 20.2148%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▅▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▄▄▅▅▅▅▅▆▆▆▇▆▅▆▇▇▇█████</td></tr><tr><td>validation_loss</td><td>██▆▆▆▆▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.20215</td></tr><tr><td>training_loss</td><td>0.32072</td></tr><tr><td>validation_accuracy</td><td>0.21899</td></tr><tr><td>validation_loss</td><td>0.32867</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">icy-sweep-2</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/1lwpca6z' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/1lwpca6z</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_095937-1lwpca6z/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 03sprav4 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Sgd\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.25\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_102815-03sprav4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/03sprav4' target=\"_blank\">ruby-sweep-3</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/03sprav4' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/03sprav4</a>"},"metadata":{}},{"name":"stdout","text":"True\nEpoch: 0 | Train Loss: 1.5416 |Val Loss: 0.9827 |Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.1029 |Val Loss: 0.9578 |Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.0934 |Val Loss: 0.9717 |Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 1.0779 |Val Loss: 0.9336 |Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 1.0387 |Val Loss: 0.8468 |Val Accuracy: 0.0244%\nEpoch: 5 | Train Loss: 0.9708 |Val Loss: 0.8366 |Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 0.9472 |Val Loss: 0.8442 |Val Accuracy: 0.0000%\nEpoch: 7 | Train Loss: 0.9305 |Val Loss: 0.8807 |Val Accuracy: 0.0000%\nEpoch: 8 | Train Loss: 0.9270 |Val Loss: 0.8185 |Val Accuracy: 0.0000%\nEpoch: 9 | Train Loss: 0.9029 |Val Loss: 0.8035 |Val Accuracy: 0.0000%\nEpoch: 10 | Train Loss: 0.8650 |Val Loss: 0.7821 |Val Accuracy: 0.0000%\nEpoch: 11 | Train Loss: 0.8739 |Val Loss: 0.7480 |Val Accuracy: 0.0244%\nEpoch: 12 | Train Loss: 0.8218 |Val Loss: 0.7310 |Val Accuracy: 0.0732%\nEpoch: 13 | Train Loss: 0.7736 |Val Loss: 0.6717 |Val Accuracy: 0.1953%\nEpoch: 14 | Train Loss: 0.7216 |Val Loss: 0.6163 |Val Accuracy: 0.4883%\nEpoch: 15 | Train Loss: 0.6636 |Val Loss: 0.5704 |Val Accuracy: 0.9766%\nEpoch: 16 | Train Loss: 0.6160 |Val Loss: 0.5164 |Val Accuracy: 1.8799%\nEpoch: 17 | Train Loss: 0.5554 |Val Loss: 0.4981 |Val Accuracy: 5.1758%\nEpoch: 18 | Train Loss: 0.5036 |Val Loss: 0.4594 |Val Accuracy: 5.7129%\nEpoch: 19 | Train Loss: 0.4928 |Val Loss: 0.4240 |Val Accuracy: 6.8848%\nEpoch: 20 | Train Loss: 0.4450 |Val Loss: 0.4206 |Val Accuracy: 10.2783%\nEpoch: 21 | Train Loss: 0.4204 |Val Loss: 0.3965 |Val Accuracy: 13.4521%\nEpoch: 22 | Train Loss: 0.3965 |Val Loss: 0.4003 |Val Accuracy: 13.0127%\nEpoch: 23 | Train Loss: 0.4047 |Val Loss: 0.3662 |Val Accuracy: 15.2344%\nEpoch: 24 | Train Loss: 0.3821 |Val Loss: 0.3607 |Val Accuracy: 16.3330%\nEpoch: 25 | Train Loss: 0.3378 |Val Loss: 0.3751 |Val Accuracy: 18.9209%\nEpoch: 26 | Train Loss: 0.3572 |Val Loss: 0.3412 |Val Accuracy: 20.1416%\nEpoch: 27 | Train Loss: 0.3454 |Val Loss: 0.3394 |Val Accuracy: 19.7754%\nEpoch: 28 | Train Loss: 0.3174 |Val Loss: 0.3548 |Val Accuracy: 21.3135%\nEpoch: 29 | Train Loss: 0.3141 |Val Loss: 0.3367 |Val Accuracy: 21.9482%\nEpoch: 30 | Train Loss: 0.2983 |Val Loss: 0.3314 |Val Accuracy: 24.3652%\nEpoch: 31 | Train Loss: 0.2790 |Val Loss: 0.3405 |Val Accuracy: 24.1699%\nEpoch: 32 | Train Loss: 0.3164 |Val Loss: 0.3131 |Val Accuracy: 23.5596%\nEpoch: 33 | Train Loss: 0.2826 |Val Loss: 0.3114 |Val Accuracy: 25.6836%\nEpoch: 34 | Train Loss: 0.2810 |Val Loss: 0.3251 |Val Accuracy: 23.9014%\nEpoch: 35 | Train Loss: 0.2674 |Val Loss: 0.3097 |Val Accuracy: 26.8066%\nEpoch: 36 | Train Loss: 0.2808 |Val Loss: 0.3098 |Val Accuracy: 25.8301%\nEpoch: 37 | Train Loss: 0.2612 |Val Loss: 0.3063 |Val Accuracy: 27.8076%\nEpoch: 38 | Train Loss: 0.2452 |Val Loss: 0.3064 |Val Accuracy: 28.7354%\nEpoch: 39 | Train Loss: 0.2560 |Val Loss: 0.3025 |Val Accuracy: 27.1484%\nEpoch: 40 | Train Loss: 0.2333 |Val Loss: 0.3027 |Val Accuracy: 29.7363%\nEpoch: 41 | Train Loss: 0.2324 |Val Loss: 0.3229 |Val Accuracy: 28.8086%\nEpoch: 42 | Train Loss: 0.2305 |Val Loss: 0.3151 |Val Accuracy: 26.4404%\nEpoch: 43 | Train Loss: 0.2316 |Val Loss: 0.2940 |Val Accuracy: 29.2725%\nEpoch: 44 | Train Loss: 0.2033 |Val Loss: 0.3040 |Val Accuracy: 30.8105%\nEpoch: 45 | Train Loss: 0.1846 |Val Loss: 0.3139 |Val Accuracy: 30.9570%\nEpoch: 46 | Train Loss: 0.2134 |Val Loss: 0.3006 |Val Accuracy: 29.1992%\nEpoch: 47 | Train Loss: 0.1735 |Val Loss: 0.3271 |Val Accuracy: 31.8604%\nEpoch: 48 | Train Loss: 0.1914 |Val Loss: 0.3102 |Val Accuracy: 30.4443%\nEpoch: 49 | Train Loss: 0.2055 |Val Loss: 0.2988 |Val Accuracy: 30.0781%\nEpoch: 50 | Train Loss: 0.1850 |Val Loss: 0.2977 |Val Accuracy: 29.5898%\nEpoch: 51 | Train Loss: 0.1720 |Val Loss: 0.3084 |Val Accuracy: 30.3467%\nEpoch: 52 | Train Loss: 0.1662 |Val Loss: 0.3078 |Val Accuracy: 30.3223%\nEpoch: 53 | Train Loss: 0.1599 |Val Loss: 0.3029 |Val Accuracy: 30.9326%\nEpoch: 54 | Train Loss: 0.1482 |Val Loss: 0.3126 |Val Accuracy: 30.3955%\nEpoch: 55 | Train Loss: 0.1433 |Val Loss: 0.3257 |Val Accuracy: 30.9082%\nEpoch: 56 | Train Loss: 0.1459 |Val Loss: 0.3220 |Val Accuracy: 29.9072%\nEpoch: 57 | Train Loss: 0.1489 |Val Loss: 0.3172 |Val Accuracy: 30.5176%\nEpoch: 58 | Train Loss: 0.1395 |Val Loss: 0.3245 |Val Accuracy: 30.4932%\nEpoch: 59 | Train Loss: 0.1224 |Val Loss: 0.3319 |Val Accuracy: 30.7373%\nTest Accuracy: 28.0029%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▆▆▆▅▅▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▃▄▄▅▅▅▆▇▆▇▆▇▇▇█▇███████████</td></tr><tr><td>validation_loss</td><td>███▇▇▇▆▆▅▅▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.28003</td></tr><tr><td>training_loss</td><td>0.12244</td></tr><tr><td>validation_accuracy</td><td>0.30737</td></tr><tr><td>validation_loss</td><td>0.33191</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">ruby-sweep-3</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/03sprav4' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/03sprav4</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_102815-03sprav4/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wdlx3oro with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Sgd\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.75\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_105303-wdlx3oro</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/wdlx3oro' target=\"_blank\">blooming-sweep-4</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/wdlx3oro' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/wdlx3oro</a>"},"metadata":{}},{"name":"stdout","text":"True\nEpoch: 0 | Train Loss: 1.4950 |Val Loss: 0.9876 |Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.0950 |Val Loss: 0.9709 |Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.0435 |Val Loss: 0.8919 |Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 0.9667 |Val Loss: 0.8467 |Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 0.9413 |Val Loss: 0.8658 |Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 0.9535 |Val Loss: 0.8339 |Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 0.9204 |Val Loss: 0.8366 |Val Accuracy: 0.0000%\nEpoch: 7 | Train Loss: 0.9151 |Val Loss: 0.8280 |Val Accuracy: 0.0000%\nEpoch: 8 | Train Loss: 0.8676 |Val Loss: 0.8117 |Val Accuracy: 0.0000%\nEpoch: 9 | Train Loss: 0.8867 |Val Loss: 0.7685 |Val Accuracy: 0.0000%\nEpoch: 10 | Train Loss: 0.8231 |Val Loss: 0.7247 |Val Accuracy: 0.1465%\nEpoch: 11 | Train Loss: 0.7658 |Val Loss: 0.6651 |Val Accuracy: 0.3174%\nEpoch: 12 | Train Loss: 0.7056 |Val Loss: 0.6033 |Val Accuracy: 0.7324%\nEpoch: 13 | Train Loss: 0.6338 |Val Loss: 0.5394 |Val Accuracy: 2.0264%\nEpoch: 14 | Train Loss: 0.5863 |Val Loss: 0.4895 |Val Accuracy: 3.1494%\nEpoch: 15 | Train Loss: 0.5206 |Val Loss: 0.4946 |Val Accuracy: 7.3730%\nEpoch: 16 | Train Loss: 0.5012 |Val Loss: 0.4331 |Val Accuracy: 6.9336%\nEpoch: 17 | Train Loss: 0.4734 |Val Loss: 0.4155 |Val Accuracy: 9.1797%\nEpoch: 18 | Train Loss: 0.4264 |Val Loss: 0.4181 |Val Accuracy: 12.5000%\nEpoch: 19 | Train Loss: 0.4206 |Val Loss: 0.3828 |Val Accuracy: 13.2324%\nEpoch: 20 | Train Loss: 0.3856 |Val Loss: 0.3850 |Val Accuracy: 15.5762%\nEpoch: 21 | Train Loss: 0.3880 |Val Loss: 0.3648 |Val Accuracy: 15.3320%\nEpoch: 22 | Train Loss: 0.3747 |Val Loss: 0.3608 |Val Accuracy: 16.9922%\nEpoch: 23 | Train Loss: 0.3405 |Val Loss: 0.3633 |Val Accuracy: 18.8721%\nEpoch: 24 | Train Loss: 0.3362 |Val Loss: 0.3502 |Val Accuracy: 18.9941%\nEpoch: 25 | Train Loss: 0.3322 |Val Loss: 0.3388 |Val Accuracy: 19.9951%\nEpoch: 26 | Train Loss: 0.3192 |Val Loss: 0.3339 |Val Accuracy: 20.8252%\nEpoch: 27 | Train Loss: 0.3261 |Val Loss: 0.3294 |Val Accuracy: 20.8984%\nEpoch: 28 | Train Loss: 0.3201 |Val Loss: 0.3242 |Val Accuracy: 22.1436%\nEpoch: 29 | Train Loss: 0.3275 |Val Loss: 0.3135 |Val Accuracy: 22.6562%\nEpoch: 30 | Train Loss: 0.3138 |Val Loss: 0.3266 |Val Accuracy: 23.0469%\nEpoch: 31 | Train Loss: 0.3067 |Val Loss: 0.3109 |Val Accuracy: 23.6572%\nEpoch: 32 | Train Loss: 0.2991 |Val Loss: 0.3172 |Val Accuracy: 24.7803%\nEpoch: 33 | Train Loss: 0.2927 |Val Loss: 0.3155 |Val Accuracy: 25.2197%\nEpoch: 34 | Train Loss: 0.2919 |Val Loss: 0.3017 |Val Accuracy: 24.8535%\nEpoch: 35 | Train Loss: 0.2742 |Val Loss: 0.3240 |Val Accuracy: 24.2676%\nEpoch: 36 | Train Loss: 0.2624 |Val Loss: 0.3244 |Val Accuracy: 27.3193%\nEpoch: 37 | Train Loss: 0.2750 |Val Loss: 0.3053 |Val Accuracy: 25.8545%\nEpoch: 38 | Train Loss: 0.2823 |Val Loss: 0.2914 |Val Accuracy: 25.8301%\nEpoch: 39 | Train Loss: 0.2731 |Val Loss: 0.3040 |Val Accuracy: 26.4160%\nEpoch: 40 | Train Loss: 0.2373 |Val Loss: 0.3229 |Val Accuracy: 27.8564%\nEpoch: 41 | Train Loss: 0.2562 |Val Loss: 0.2986 |Val Accuracy: 26.7090%\nEpoch: 42 | Train Loss: 0.2404 |Val Loss: 0.3067 |Val Accuracy: 28.1006%\nEpoch: 43 | Train Loss: 0.2550 |Val Loss: 0.2994 |Val Accuracy: 29.9561%\nEpoch: 44 | Train Loss: 0.2398 |Val Loss: 0.2954 |Val Accuracy: 28.6133%\nEpoch: 45 | Train Loss: 0.2262 |Val Loss: 0.3102 |Val Accuracy: 30.1025%\nEpoch: 46 | Train Loss: 0.2343 |Val Loss: 0.2922 |Val Accuracy: 29.5166%\nEpoch: 47 | Train Loss: 0.2308 |Val Loss: 0.2992 |Val Accuracy: 29.6387%\nEpoch: 48 | Train Loss: 0.2481 |Val Loss: 0.2951 |Val Accuracy: 29.2725%\nEpoch: 49 | Train Loss: 0.2220 |Val Loss: 0.2998 |Val Accuracy: 29.8828%\nEpoch: 50 | Train Loss: 0.2258 |Val Loss: 0.2881 |Val Accuracy: 29.7852%\nEpoch: 51 | Train Loss: 0.2211 |Val Loss: 0.2896 |Val Accuracy: 30.9570%\nEpoch: 52 | Train Loss: 0.2344 |Val Loss: 0.2905 |Val Accuracy: 30.2002%\nEpoch: 53 | Train Loss: 0.2241 |Val Loss: 0.2853 |Val Accuracy: 29.8096%\nEpoch: 54 | Train Loss: 0.2027 |Val Loss: 0.3130 |Val Accuracy: 30.5176%\nEpoch: 55 | Train Loss: 0.2076 |Val Loss: 0.3042 |Val Accuracy: 28.8818%\nEpoch: 56 | Train Loss: 0.2021 |Val Loss: 0.3142 |Val Accuracy: 29.2725%\nEpoch: 57 | Train Loss: 0.2158 |Val Loss: 0.2889 |Val Accuracy: 29.9316%\nEpoch: 58 | Train Loss: 0.1915 |Val Loss: 0.2996 |Val Accuracy: 31.2744%\nEpoch: 59 | Train Loss: 0.1977 |Val Loss: 0.2986 |Val Accuracy: 30.6152%\nTest Accuracy: 27.2705%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▆▅▅▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▃▃▄▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇███████████</td></tr><tr><td>validation_loss</td><td>██▇▇▆▆▆▅▄▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.27271</td></tr><tr><td>training_loss</td><td>0.19771</td></tr><tr><td>validation_accuracy</td><td>0.30615</td></tr><tr><td>validation_loss</td><td>0.29861</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">blooming-sweep-4</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/wdlx3oro' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/wdlx3oro</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_105303-wdlx3oro/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pb7j0wcr with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.75\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_111110-pb7j0wcr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/pb7j0wcr' target=\"_blank\">stellar-sweep-5</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/pb7j0wcr' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/pb7j0wcr</a>"},"metadata":{}},{"name":"stdout","text":"True\nEpoch: 0 | Train Loss: 1.5251 |Val Loss: 0.9801 |Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.0930 |Val Loss: 0.9636 |Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.0742 |Val Loss: 0.9740 |Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 1.0444 |Val Loss: 0.9423 |Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 1.0179 |Val Loss: 0.8729 |Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 0.9804 |Val Loss: 0.8515 |Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 0.9344 |Val Loss: 0.8299 |Val Accuracy: 0.0244%\nEpoch: 7 | Train Loss: 0.9290 |Val Loss: 0.8196 |Val Accuracy: 0.0000%\nEpoch: 8 | Train Loss: 0.9051 |Val Loss: 0.8086 |Val Accuracy: 0.0000%\nEpoch: 9 | Train Loss: 0.8815 |Val Loss: 0.7823 |Val Accuracy: 0.0488%\nEpoch: 10 | Train Loss: 0.8480 |Val Loss: 0.7256 |Val Accuracy: 0.0244%\nEpoch: 11 | Train Loss: 0.7923 |Val Loss: 0.6886 |Val Accuracy: 0.1465%\nEpoch: 12 | Train Loss: 0.7400 |Val Loss: 0.6516 |Val Accuracy: 0.4150%\nEpoch: 13 | Train Loss: 0.6870 |Val Loss: 0.5759 |Val Accuracy: 0.6592%\nEpoch: 14 | Train Loss: 0.6315 |Val Loss: 0.5336 |Val Accuracy: 1.6602%\nEpoch: 15 | Train Loss: 0.5742 |Val Loss: 0.5010 |Val Accuracy: 2.9053%\nEpoch: 16 | Train Loss: 0.5391 |Val Loss: 0.4910 |Val Accuracy: 4.8096%\nEpoch: 17 | Train Loss: 0.5320 |Val Loss: 0.4499 |Val Accuracy: 4.2969%\nEpoch: 18 | Train Loss: 0.4939 |Val Loss: 0.4200 |Val Accuracy: 6.7871%\nEpoch: 19 | Train Loss: 0.4627 |Val Loss: 0.4117 |Val Accuracy: 7.1289%\nEpoch: 20 | Train Loss: 0.4223 |Val Loss: 0.4098 |Val Accuracy: 10.6934%\nEpoch: 21 | Train Loss: 0.4134 |Val Loss: 0.3862 |Val Accuracy: 10.9619%\nEpoch: 22 | Train Loss: 0.4072 |Val Loss: 0.3674 |Val Accuracy: 12.1338%\nEpoch: 23 | Train Loss: 0.3831 |Val Loss: 0.3646 |Val Accuracy: 14.0381%\nEpoch: 24 | Train Loss: 0.3660 |Val Loss: 0.3594 |Val Accuracy: 13.8916%\nEpoch: 25 | Train Loss: 0.3688 |Val Loss: 0.3526 |Val Accuracy: 15.6006%\nEpoch: 26 | Train Loss: 0.3550 |Val Loss: 0.3529 |Val Accuracy: 14.5020%\nEpoch: 27 | Train Loss: 0.3370 |Val Loss: 0.3549 |Val Accuracy: 17.0166%\nEpoch: 28 | Train Loss: 0.3334 |Val Loss: 0.3354 |Val Accuracy: 20.3125%\nEpoch: 29 | Train Loss: 0.3311 |Val Loss: 0.3276 |Val Accuracy: 19.3115%\nEpoch: 30 | Train Loss: 0.3092 |Val Loss: 0.3339 |Val Accuracy: 18.9453%\nEpoch: 31 | Train Loss: 0.3016 |Val Loss: 0.3244 |Val Accuracy: 22.0459%\nEpoch: 32 | Train Loss: 0.2906 |Val Loss: 0.3246 |Val Accuracy: 23.1445%\nEpoch: 33 | Train Loss: 0.3029 |Val Loss: 0.3238 |Val Accuracy: 19.4580%\nEpoch: 34 | Train Loss: 0.2872 |Val Loss: 0.3126 |Val Accuracy: 23.6572%\nEpoch: 35 | Train Loss: 0.2991 |Val Loss: 0.3042 |Val Accuracy: 23.9746%\nEpoch: 36 | Train Loss: 0.2882 |Val Loss: 0.3039 |Val Accuracy: 24.0234%\nEpoch: 37 | Train Loss: 0.2722 |Val Loss: 0.3005 |Val Accuracy: 25.1953%\nEpoch: 38 | Train Loss: 0.2432 |Val Loss: 0.3190 |Val Accuracy: 24.8047%\nEpoch: 39 | Train Loss: 0.2422 |Val Loss: 0.3040 |Val Accuracy: 27.0752%\nEpoch: 40 | Train Loss: 0.2670 |Val Loss: 0.2932 |Val Accuracy: 25.7568%\nEpoch: 41 | Train Loss: 0.2497 |Val Loss: 0.2948 |Val Accuracy: 25.9277%\nEpoch: 42 | Train Loss: 0.2228 |Val Loss: 0.3173 |Val Accuracy: 27.2949%\nEpoch: 43 | Train Loss: 0.2390 |Val Loss: 0.3079 |Val Accuracy: 26.0498%\nEpoch: 44 | Train Loss: 0.2432 |Val Loss: 0.3077 |Val Accuracy: 25.4150%\nEpoch: 45 | Train Loss: 0.2174 |Val Loss: 0.2989 |Val Accuracy: 28.4912%\nEpoch: 46 | Train Loss: 0.2066 |Val Loss: 0.3047 |Val Accuracy: 27.9541%\nEpoch: 47 | Train Loss: 0.2237 |Val Loss: 0.2904 |Val Accuracy: 29.3457%\nEpoch: 48 | Train Loss: 0.2152 |Val Loss: 0.2960 |Val Accuracy: 29.2480%\nEpoch: 49 | Train Loss: 0.2142 |Val Loss: 0.2957 |Val Accuracy: 29.5898%\nEpoch: 50 | Train Loss: 0.2107 |Val Loss: 0.2883 |Val Accuracy: 27.9541%\nEpoch: 51 | Train Loss: 0.2062 |Val Loss: 0.2868 |Val Accuracy: 28.5400%\nEpoch: 52 | Train Loss: 0.1731 |Val Loss: 0.3104 |Val Accuracy: 29.9805%\nEpoch: 53 | Train Loss: 0.1821 |Val Loss: 0.3080 |Val Accuracy: 30.7129%\nEpoch: 54 | Train Loss: 0.1907 |Val Loss: 0.3043 |Val Accuracy: 30.3223%\nEpoch: 55 | Train Loss: 0.1898 |Val Loss: 0.2985 |Val Accuracy: 29.3701%\nEpoch: 56 | Train Loss: 0.1700 |Val Loss: 0.3018 |Val Accuracy: 31.8115%\nEpoch: 57 | Train Loss: 0.1682 |Val Loss: 0.3058 |Val Accuracy: 30.6641%\nEpoch: 58 | Train Loss: 0.1597 |Val Loss: 0.3034 |Val Accuracy: 31.3477%\nEpoch: 59 | Train Loss: 0.1488 |Val Loss: 0.3207 |Val Accuracy: 31.6162%\nTest Accuracy: 29.4189%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.022 MB uploaded\\r'), FloatProgress(value=0.061522788678743855, max=1…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▆▆▅▅▅▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▃▃▃▄▄▄▅▅▅▆▅▆▆▇▇▇▇▇▇▇▇█▇█████</td></tr><tr><td>validation_loss</td><td>███▇▆▆▆▅▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.29419</td></tr><tr><td>training_loss</td><td>0.14885</td></tr><tr><td>validation_accuracy</td><td>0.31616</td></tr><tr><td>validation_loss</td><td>0.3207</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">stellar-sweep-5</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/pb7j0wcr' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/pb7j0wcr</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_111110-pb7j0wcr/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w71crn8u with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.75\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_113641-w71crn8u</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/w71crn8u' target=\"_blank\">effortless-sweep-6</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/w71crn8u' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/w71crn8u</a>"},"metadata":{}},{"name":"stdout","text":"True\nEpoch: 0 | Train Loss: 1.5004 |Val Loss: 0.9820 |Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.1027 |Val Loss: 0.9630 |Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.0327 |Val Loss: 0.8770 |Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 0.9904 |Val Loss: 0.8372 |Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 0.9797 |Val Loss: 0.8401 |Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 0.9720 |Val Loss: 0.8309 |Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 0.9615 |Val Loss: 0.8199 |Val Accuracy: 0.0000%\nEpoch: 7 | Train Loss: 0.9446 |Val Loss: 0.8184 |Val Accuracy: 0.0000%\nEpoch: 8 | Train Loss: 0.9315 |Val Loss: 0.8126 |Val Accuracy: 0.0000%\nEpoch: 9 | Train Loss: 0.9242 |Val Loss: 0.7991 |Val Accuracy: 0.0000%\nEpoch: 10 | Train Loss: 0.9062 |Val Loss: 0.7847 |Val Accuracy: 0.0000%\nEpoch: 11 | Train Loss: 0.8745 |Val Loss: 0.7768 |Val Accuracy: 0.0000%\nEpoch: 12 | Train Loss: 0.8645 |Val Loss: 0.7571 |Val Accuracy: 0.0488%\nEpoch: 13 | Train Loss: 0.8485 |Val Loss: 0.7348 |Val Accuracy: 0.0488%\nEpoch: 14 | Train Loss: 0.8039 |Val Loss: 0.7026 |Val Accuracy: 0.0000%\nEpoch: 15 | Train Loss: 0.7792 |Val Loss: 0.7086 |Val Accuracy: 0.0732%\nEpoch: 16 | Train Loss: 0.7558 |Val Loss: 0.6765 |Val Accuracy: 0.1465%\nEpoch: 17 | Train Loss: 0.7239 |Val Loss: 0.6100 |Val Accuracy: 0.1221%\nEpoch: 18 | Train Loss: 0.6752 |Val Loss: 0.5762 |Val Accuracy: 0.8545%\nEpoch: 19 | Train Loss: 0.6311 |Val Loss: 0.5527 |Val Accuracy: 1.5381%\nEpoch: 20 | Train Loss: 0.6066 |Val Loss: 0.4989 |Val Accuracy: 1.7822%\nEpoch: 21 | Train Loss: 0.5593 |Val Loss: 0.4819 |Val Accuracy: 3.1494%\nEpoch: 22 | Train Loss: 0.5190 |Val Loss: 0.4538 |Val Accuracy: 4.3945%\nEpoch: 23 | Train Loss: 0.4930 |Val Loss: 0.4326 |Val Accuracy: 6.7383%\nEpoch: 24 | Train Loss: 0.4571 |Val Loss: 0.4159 |Val Accuracy: 8.6914%\nEpoch: 25 | Train Loss: 0.4485 |Val Loss: 0.4047 |Val Accuracy: 9.9609%\nEpoch: 26 | Train Loss: 0.4149 |Val Loss: 0.3867 |Val Accuracy: 11.4746%\nEpoch: 27 | Train Loss: 0.4094 |Val Loss: 0.3651 |Val Accuracy: 12.4756%\nEpoch: 28 | Train Loss: 0.4046 |Val Loss: 0.3566 |Val Accuracy: 12.0361%\nEpoch: 29 | Train Loss: 0.3743 |Val Loss: 0.3549 |Val Accuracy: 14.9902%\nEpoch: 30 | Train Loss: 0.3575 |Val Loss: 0.3512 |Val Accuracy: 17.2852%\nEpoch: 31 | Train Loss: 0.3287 |Val Loss: 0.3571 |Val Accuracy: 18.5059%\nEpoch: 32 | Train Loss: 0.3219 |Val Loss: 0.3439 |Val Accuracy: 18.5303%\nEpoch: 33 | Train Loss: 0.3397 |Val Loss: 0.3341 |Val Accuracy: 17.8467%\nEpoch: 34 | Train Loss: 0.3135 |Val Loss: 0.3654 |Val Accuracy: 18.5059%\nEpoch: 35 | Train Loss: 0.2900 |Val Loss: 0.3398 |Val Accuracy: 22.7783%\nEpoch: 36 | Train Loss: 0.3069 |Val Loss: 0.3284 |Val Accuracy: 22.6074%\nEpoch: 37 | Train Loss: 0.3058 |Val Loss: 0.3216 |Val Accuracy: 23.0469%\nEpoch: 38 | Train Loss: 0.2691 |Val Loss: 0.3306 |Val Accuracy: 24.6094%\nEpoch: 39 | Train Loss: 0.2997 |Val Loss: 0.3120 |Val Accuracy: 22.8516%\nEpoch: 40 | Train Loss: 0.2876 |Val Loss: 0.3134 |Val Accuracy: 21.4844%\nEpoch: 41 | Train Loss: 0.2670 |Val Loss: 0.3125 |Val Accuracy: 25.5371%\nEpoch: 42 | Train Loss: 0.2660 |Val Loss: 0.3051 |Val Accuracy: 24.0723%\nEpoch: 43 | Train Loss: 0.2450 |Val Loss: 0.3052 |Val Accuracy: 26.1963%\nEpoch: 44 | Train Loss: 0.2534 |Val Loss: 0.3088 |Val Accuracy: 24.8291%\nEpoch: 45 | Train Loss: 0.2329 |Val Loss: 0.3116 |Val Accuracy: 27.1729%\nEpoch: 46 | Train Loss: 0.2519 |Val Loss: 0.2960 |Val Accuracy: 27.1240%\nEpoch: 47 | Train Loss: 0.2398 |Val Loss: 0.3064 |Val Accuracy: 27.7832%\nEpoch: 48 | Train Loss: 0.2352 |Val Loss: 0.2926 |Val Accuracy: 28.2471%\nEpoch: 49 | Train Loss: 0.2322 |Val Loss: 0.2983 |Val Accuracy: 28.8086%\nEpoch: 50 | Train Loss: 0.2229 |Val Loss: 0.2976 |Val Accuracy: 27.9785%\nEpoch: 51 | Train Loss: 0.2238 |Val Loss: 0.3076 |Val Accuracy: 28.2959%\nEpoch: 52 | Train Loss: 0.2136 |Val Loss: 0.2949 |Val Accuracy: 28.4180%\nEpoch: 53 | Train Loss: 0.1958 |Val Loss: 0.2989 |Val Accuracy: 29.6143%\nEpoch: 54 | Train Loss: 0.1754 |Val Loss: 0.3223 |Val Accuracy: 30.2490%\nEpoch: 55 | Train Loss: 0.2085 |Val Loss: 0.2890 |Val Accuracy: 29.1260%\nEpoch: 56 | Train Loss: 0.1917 |Val Loss: 0.2946 |Val Accuracy: 28.8086%\nEpoch: 57 | Train Loss: 0.1854 |Val Loss: 0.2988 |Val Accuracy: 29.5654%\nEpoch: 58 | Train Loss: 0.1842 |Val Loss: 0.2906 |Val Accuracy: 29.2725%\nEpoch: 59 | Train Loss: 0.1830 |Val Loss: 0.2992 |Val Accuracy: 30.1025%\nTest Accuracy: 27.9785%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▆▅▅▅▅▅▅▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇████████</td></tr><tr><td>validation_loss</td><td>██▇▇▆▆▆▆▆▆▅▅▄▄▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.27979</td></tr><tr><td>training_loss</td><td>0.18297</td></tr><tr><td>validation_accuracy</td><td>0.30103</td></tr><tr><td>validation_loss</td><td>0.29922</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">effortless-sweep-6</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/w71crn8u' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/w71crn8u</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_113641-w71crn8u/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zgi0yxkx with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Sgd\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.25\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_120928-zgi0yxkx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/zgi0yxkx' target=\"_blank\">dulcet-sweep-7</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/zgi0yxkx' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/zgi0yxkx</a>"},"metadata":{}},{"name":"stdout","text":"True\nEpoch: 0 | Train Loss: 1.3613 |Val Loss: 0.9678 |Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.0694 |Val Loss: 0.8718 |Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 0.9951 |Val Loss: 0.8379 |Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 0.9786 |Val Loss: 0.8309 |Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 0.9742 |Val Loss: 0.8247 |Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 0.9687 |Val Loss: 0.8166 |Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 0.9503 |Val Loss: 0.8193 |Val Accuracy: 0.0000%\nEpoch: 7 | Train Loss: 0.9235 |Val Loss: 0.7782 |Val Accuracy: 0.0244%\nEpoch: 8 | Train Loss: 0.8754 |Val Loss: 0.7412 |Val Accuracy: 0.0244%\nEpoch: 9 | Train Loss: 0.8278 |Val Loss: 0.6977 |Val Accuracy: 0.0732%\nEpoch: 10 | Train Loss: 0.7730 |Val Loss: 0.6621 |Val Accuracy: 0.1221%\nEpoch: 11 | Train Loss: 0.7355 |Val Loss: 0.6089 |Val Accuracy: 0.3174%\nEpoch: 12 | Train Loss: 0.6602 |Val Loss: 0.5392 |Val Accuracy: 1.3428%\nEpoch: 13 | Train Loss: 0.5960 |Val Loss: 0.4914 |Val Accuracy: 4.1016%\nEpoch: 14 | Train Loss: 0.5227 |Val Loss: 0.4712 |Val Accuracy: 4.5166%\nEpoch: 15 | Train Loss: 0.4826 |Val Loss: 0.4156 |Val Accuracy: 8.9600%\nEpoch: 16 | Train Loss: 0.4349 |Val Loss: 0.3936 |Val Accuracy: 11.7920%\nEpoch: 17 | Train Loss: 0.4169 |Val Loss: 0.3663 |Val Accuracy: 15.2588%\nEpoch: 18 | Train Loss: 0.3677 |Val Loss: 0.3552 |Val Accuracy: 17.7490%\nEpoch: 19 | Train Loss: 0.3521 |Val Loss: 0.3705 |Val Accuracy: 16.6016%\nEpoch: 20 | Train Loss: 0.3448 |Val Loss: 0.3282 |Val Accuracy: 20.7520%\nEpoch: 21 | Train Loss: 0.3146 |Val Loss: 0.3363 |Val Accuracy: 19.6289%\nEpoch: 22 | Train Loss: 0.2958 |Val Loss: 0.3167 |Val Accuracy: 23.6084%\nEpoch: 23 | Train Loss: 0.2930 |Val Loss: 0.3221 |Val Accuracy: 24.0234%\nEpoch: 24 | Train Loss: 0.2978 |Val Loss: 0.3143 |Val Accuracy: 23.9502%\nEpoch: 25 | Train Loss: 0.2713 |Val Loss: 0.3149 |Val Accuracy: 24.9023%\nEpoch: 26 | Train Loss: 0.2522 |Val Loss: 0.3154 |Val Accuracy: 28.3447%\nEpoch: 27 | Train Loss: 0.2481 |Val Loss: 0.2971 |Val Accuracy: 27.6611%\nEpoch: 28 | Train Loss: 0.2483 |Val Loss: 0.2950 |Val Accuracy: 27.8564%\nEpoch: 29 | Train Loss: 0.2298 |Val Loss: 0.3074 |Val Accuracy: 29.8828%\nEpoch: 30 | Train Loss: 0.2384 |Val Loss: 0.2996 |Val Accuracy: 29.7607%\nEpoch: 31 | Train Loss: 0.2277 |Val Loss: 0.2955 |Val Accuracy: 31.1279%\nEpoch: 32 | Train Loss: 0.2113 |Val Loss: 0.2972 |Val Accuracy: 30.7373%\nEpoch: 33 | Train Loss: 0.2031 |Val Loss: 0.2969 |Val Accuracy: 28.7598%\nEpoch: 34 | Train Loss: 0.1903 |Val Loss: 0.3011 |Val Accuracy: 31.6406%\nEpoch: 35 | Train Loss: 0.1911 |Val Loss: 0.2849 |Val Accuracy: 30.2490%\nEpoch: 36 | Train Loss: 0.1777 |Val Loss: 0.2990 |Val Accuracy: 33.0566%\nEpoch: 37 | Train Loss: 0.1718 |Val Loss: 0.3028 |Val Accuracy: 30.5176%\nEpoch: 38 | Train Loss: 0.1605 |Val Loss: 0.3096 |Val Accuracy: 32.3242%\nEpoch: 39 | Train Loss: 0.1487 |Val Loss: 0.3140 |Val Accuracy: 32.7148%\nEpoch: 40 | Train Loss: 0.1550 |Val Loss: 0.3158 |Val Accuracy: 32.7148%\nEpoch: 41 | Train Loss: 0.1515 |Val Loss: 0.3164 |Val Accuracy: 32.2754%\nEpoch: 42 | Train Loss: 0.1449 |Val Loss: 0.3147 |Val Accuracy: 33.4229%\nEpoch: 43 | Train Loss: 0.1177 |Val Loss: 0.3280 |Val Accuracy: 33.7402%\nEpoch: 44 | Train Loss: 0.1256 |Val Loss: 0.3180 |Val Accuracy: 32.2510%\nEpoch: 45 | Train Loss: 0.1122 |Val Loss: 0.3185 |Val Accuracy: 33.2275%\nEpoch: 46 | Train Loss: 0.1177 |Val Loss: 0.3218 |Val Accuracy: 31.6650%\nEpoch: 47 | Train Loss: 0.0960 |Val Loss: 0.3369 |Val Accuracy: 32.4951%\nEpoch: 48 | Train Loss: 0.0965 |Val Loss: 0.3376 |Val Accuracy: 34.1064%\nEpoch: 49 | Train Loss: 0.0977 |Val Loss: 0.3439 |Val Accuracy: 33.5449%\nEpoch: 50 | Train Loss: 0.1024 |Val Loss: 0.3357 |Val Accuracy: 33.3984%\nEpoch: 51 | Train Loss: 0.0852 |Val Loss: 0.3465 |Val Accuracy: 34.3262%\nEpoch: 52 | Train Loss: 0.0861 |Val Loss: 0.3561 |Val Accuracy: 34.7656%\nEpoch: 53 | Train Loss: 0.0827 |Val Loss: 0.3475 |Val Accuracy: 33.6182%\nEpoch: 54 | Train Loss: 0.0736 |Val Loss: 0.3596 |Val Accuracy: 33.1055%\nEpoch: 55 | Train Loss: 0.0651 |Val Loss: 0.3824 |Val Accuracy: 34.3018%\nEpoch: 56 | Train Loss: 0.0567 |Val Loss: 0.3861 |Val Accuracy: 34.2773%\nEpoch: 57 | Train Loss: 0.0544 |Val Loss: 0.3993 |Val Accuracy: 32.9590%\nEpoch: 58 | Train Loss: 0.0584 |Val Loss: 0.3998 |Val Accuracy: 33.7646%\nEpoch: 59 | Train Loss: 0.0557 |Val Loss: 0.3778 |Val Accuracy: 32.8857%\nTest Accuracy: 30.8350%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.022 MB uploaded\\r'), FloatProgress(value=0.06077444760446292, max=1.…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▆▆▆▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▂▃▃▅▄▅▆▆▆▇▇▇▇▇▇█▇█████▇████████</td></tr><tr><td>validation_loss</td><td>█▇▇▇▆▆▅▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.30835</td></tr><tr><td>training_loss</td><td>0.05566</td></tr><tr><td>validation_accuracy</td><td>0.32886</td></tr><tr><td>validation_loss</td><td>0.37785</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dulcet-sweep-7</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/zgi0yxkx' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/zgi0yxkx</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_120928-zgi0yxkx/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1pagin8v with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Sgd\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.25\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_124232-1pagin8v</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/1pagin8v' target=\"_blank\">logical-sweep-8</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/1pagin8v' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/1pagin8v</a>"},"metadata":{}},{"name":"stdout","text":"True\nEpoch: 0 | Train Loss: 1.5310 |Val Loss: 0.9785 |Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.1017 |Val Loss: 0.9715 |Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.0441 |Val Loss: 0.8563 |Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 0.9878 |Val Loss: 0.8370 |Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 0.9806 |Val Loss: 0.8459 |Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 0.9677 |Val Loss: 0.8361 |Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 0.9594 |Val Loss: 0.8225 |Val Accuracy: 0.0000%\nEpoch: 7 | Train Loss: 0.9392 |Val Loss: 0.8087 |Val Accuracy: 0.0000%\nEpoch: 8 | Train Loss: 0.9259 |Val Loss: 0.7973 |Val Accuracy: 0.0000%\nEpoch: 9 | Train Loss: 0.9172 |Val Loss: 0.7944 |Val Accuracy: 0.0000%\nEpoch: 10 | Train Loss: 0.8950 |Val Loss: 0.7790 |Val Accuracy: 0.0000%\nEpoch: 11 | Train Loss: 0.8743 |Val Loss: 0.7683 |Val Accuracy: 0.0000%\nEpoch: 12 | Train Loss: 0.8593 |Val Loss: 0.7578 |Val Accuracy: 0.0244%\nEpoch: 13 | Train Loss: 0.8283 |Val Loss: 0.7182 |Val Accuracy: 0.0244%\nEpoch: 14 | Train Loss: 0.7888 |Val Loss: 0.6716 |Val Accuracy: 0.0732%\nEpoch: 15 | Train Loss: 0.7558 |Val Loss: 0.6408 |Val Accuracy: 0.1709%\nEpoch: 16 | Train Loss: 0.7121 |Val Loss: 0.6105 |Val Accuracy: 0.5371%\nEpoch: 17 | Train Loss: 0.6708 |Val Loss: 0.5656 |Val Accuracy: 1.1719%\nEpoch: 18 | Train Loss: 0.6119 |Val Loss: 0.5340 |Val Accuracy: 2.0508%\nEpoch: 19 | Train Loss: 0.5798 |Val Loss: 0.4962 |Val Accuracy: 3.6133%\nEpoch: 20 | Train Loss: 0.5375 |Val Loss: 0.4664 |Val Accuracy: 4.8340%\nEpoch: 21 | Train Loss: 0.4982 |Val Loss: 0.4421 |Val Accuracy: 5.5664%\nEpoch: 22 | Train Loss: 0.4773 |Val Loss: 0.4250 |Val Accuracy: 8.3252%\nEpoch: 23 | Train Loss: 0.4618 |Val Loss: 0.4133 |Val Accuracy: 9.4482%\nEpoch: 24 | Train Loss: 0.4383 |Val Loss: 0.4058 |Val Accuracy: 8.7402%\nEpoch: 25 | Train Loss: 0.4098 |Val Loss: 0.3799 |Val Accuracy: 12.3291%\nEpoch: 26 | Train Loss: 0.3955 |Val Loss: 0.3650 |Val Accuracy: 14.1113%\nEpoch: 27 | Train Loss: 0.3460 |Val Loss: 0.3774 |Val Accuracy: 16.6260%\nEpoch: 28 | Train Loss: 0.3715 |Val Loss: 0.3804 |Val Accuracy: 12.6221%\nEpoch: 29 | Train Loss: 0.3496 |Val Loss: 0.3453 |Val Accuracy: 17.4805%\nEpoch: 30 | Train Loss: 0.3480 |Val Loss: 0.3314 |Val Accuracy: 18.2861%\nEpoch: 31 | Train Loss: 0.3331 |Val Loss: 0.3601 |Val Accuracy: 20.6543%\nEpoch: 32 | Train Loss: 0.3180 |Val Loss: 0.3614 |Val Accuracy: 16.6504%\nEpoch: 33 | Train Loss: 0.2960 |Val Loss: 0.3478 |Val Accuracy: 22.5586%\nEpoch: 34 | Train Loss: 0.3006 |Val Loss: 0.3281 |Val Accuracy: 23.3643%\nEpoch: 35 | Train Loss: 0.3089 |Val Loss: 0.3200 |Val Accuracy: 22.4121%\nEpoch: 36 | Train Loss: 0.3099 |Val Loss: 0.3087 |Val Accuracy: 21.5576%\nEpoch: 37 | Train Loss: 0.2978 |Val Loss: 0.3108 |Val Accuracy: 24.2920%\nEpoch: 38 | Train Loss: 0.2804 |Val Loss: 0.3051 |Val Accuracy: 24.4141%\nEpoch: 39 | Train Loss: 0.2901 |Val Loss: 0.3017 |Val Accuracy: 22.7539%\nEpoch: 40 | Train Loss: 0.2606 |Val Loss: 0.3051 |Val Accuracy: 25.9277%\nEpoch: 41 | Train Loss: 0.2466 |Val Loss: 0.3065 |Val Accuracy: 27.1729%\nEpoch: 42 | Train Loss: 0.2653 |Val Loss: 0.2975 |Val Accuracy: 25.7080%\nEpoch: 43 | Train Loss: 0.2744 |Val Loss: 0.2955 |Val Accuracy: 25.6592%\nEpoch: 44 | Train Loss: 0.2604 |Val Loss: 0.2950 |Val Accuracy: 26.0986%\nEpoch: 45 | Train Loss: 0.2317 |Val Loss: 0.2950 |Val Accuracy: 27.6611%\nEpoch: 46 | Train Loss: 0.2317 |Val Loss: 0.2887 |Val Accuracy: 26.7090%\nEpoch: 47 | Train Loss: 0.2193 |Val Loss: 0.3077 |Val Accuracy: 27.9541%\nEpoch: 48 | Train Loss: 0.2278 |Val Loss: 0.2950 |Val Accuracy: 27.0996%\nEpoch: 49 | Train Loss: 0.2044 |Val Loss: 0.3070 |Val Accuracy: 29.3457%\nEpoch: 50 | Train Loss: 0.1958 |Val Loss: 0.3123 |Val Accuracy: 31.0791%\nEpoch: 51 | Train Loss: 0.2119 |Val Loss: 0.2837 |Val Accuracy: 29.3213%\nEpoch: 52 | Train Loss: 0.2108 |Val Loss: 0.2947 |Val Accuracy: 29.2725%\nEpoch: 53 | Train Loss: 0.1949 |Val Loss: 0.2959 |Val Accuracy: 29.4189%\nEpoch: 54 | Train Loss: 0.1984 |Val Loss: 0.2906 |Val Accuracy: 30.2002%\nEpoch: 55 | Train Loss: 0.1957 |Val Loss: 0.2905 |Val Accuracy: 31.0791%\nEpoch: 56 | Train Loss: 0.1941 |Val Loss: 0.2931 |Val Accuracy: 30.4688%\nEpoch: 57 | Train Loss: 0.1920 |Val Loss: 0.2893 |Val Accuracy: 30.1758%\nEpoch: 58 | Train Loss: 0.1930 |Val Loss: 0.2784 |Val Accuracy: 29.7363%\nEpoch: 59 | Train Loss: 0.1734 |Val Loss: 0.2975 |Val Accuracy: 31.0059%\nTest Accuracy: 28.6133%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▆▅▅▅▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▄▅▄▅▆▆▆▆▆▆▇▇▇▇▇▇███████</td></tr><tr><td>validation_loss</td><td>██▇▇▆▆▆▆▆▅▅▄▄▃▃▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.28613</td></tr><tr><td>training_loss</td><td>0.17345</td></tr><tr><td>validation_accuracy</td><td>0.31006</td></tr><tr><td>validation_loss</td><td>0.29753</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">logical-sweep-8</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/1pagin8v' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/1pagin8v</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_124232-1pagin8v/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qy3sttbt with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Sgd\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.75\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_131529-qy3sttbt</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/qy3sttbt' target=\"_blank\">frosty-sweep-9</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/qy3sttbt' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/qy3sttbt</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"True\nEpoch: 0 | Train Loss: 1.4386 |Val Loss: 0.9709 |Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.0943 |Val Loss: 0.9539 |Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.0765 |Val Loss: 0.9805 |Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 1.0430 |Val Loss: 0.9534 |Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 1.0468 |Val Loss: 0.9689 |Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 0.9748 |Val Loss: 0.8803 |Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 0.9537 |Val Loss: 0.8463 |Val Accuracy: 0.0000%\nEpoch: 7 | Train Loss: 0.9240 |Val Loss: 0.8454 |Val Accuracy: 0.0000%\nEpoch: 8 | Train Loss: 0.9139 |Val Loss: 0.8179 |Val Accuracy: 0.0000%\nEpoch: 9 | Train Loss: 0.8933 |Val Loss: 0.8112 |Val Accuracy: 0.0000%\nEpoch: 10 | Train Loss: 0.8879 |Val Loss: 0.7795 |Val Accuracy: 0.0000%\nEpoch: 11 | Train Loss: 0.8444 |Val Loss: 0.7462 |Val Accuracy: 0.0000%\nEpoch: 12 | Train Loss: 0.8023 |Val Loss: 0.6886 |Val Accuracy: 0.0977%\nEpoch: 13 | Train Loss: 0.7440 |Val Loss: 0.6479 |Val Accuracy: 0.5859%\nEpoch: 14 | Train Loss: 0.6945 |Val Loss: 0.6056 |Val Accuracy: 0.9766%\nEpoch: 15 | Train Loss: 0.6241 |Val Loss: 0.5673 |Val Accuracy: 2.0508%\nEpoch: 16 | Train Loss: 0.6088 |Val Loss: 0.5111 |Val Accuracy: 2.7588%\nEpoch: 17 | Train Loss: 0.5578 |Val Loss: 0.4899 |Val Accuracy: 4.9316%\nEpoch: 18 | Train Loss: 0.5226 |Val Loss: 0.4359 |Val Accuracy: 6.8359%\nEpoch: 19 | Train Loss: 0.4840 |Val Loss: 0.4202 |Val Accuracy: 8.3008%\nEpoch: 20 | Train Loss: 0.4339 |Val Loss: 0.4201 |Val Accuracy: 11.3281%\nEpoch: 21 | Train Loss: 0.4276 |Val Loss: 0.3835 |Val Accuracy: 12.3779%\nEpoch: 22 | Train Loss: 0.4052 |Val Loss: 0.3899 |Val Accuracy: 13.5254%\nEpoch: 23 | Train Loss: 0.3924 |Val Loss: 0.3585 |Val Accuracy: 15.3076%\nEpoch: 24 | Train Loss: 0.3593 |Val Loss: 0.3591 |Val Accuracy: 16.4307%\nEpoch: 25 | Train Loss: 0.3575 |Val Loss: 0.3419 |Val Accuracy: 17.2852%\nEpoch: 26 | Train Loss: 0.3246 |Val Loss: 0.3410 |Val Accuracy: 18.9697%\nEpoch: 27 | Train Loss: 0.3311 |Val Loss: 0.3282 |Val Accuracy: 17.7979%\nEpoch: 28 | Train Loss: 0.3115 |Val Loss: 0.3456 |Val Accuracy: 20.4590%\nEpoch: 29 | Train Loss: 0.2885 |Val Loss: 0.3404 |Val Accuracy: 23.4619%\nEpoch: 30 | Train Loss: 0.3087 |Val Loss: 0.3225 |Val Accuracy: 20.4590%\nEpoch: 31 | Train Loss: 0.2850 |Val Loss: 0.3247 |Val Accuracy: 20.7764%\nEpoch: 32 | Train Loss: 0.2837 |Val Loss: 0.3158 |Val Accuracy: 22.4121%\nEpoch: 33 | Train Loss: 0.2778 |Val Loss: 0.3142 |Val Accuracy: 24.4873%\nEpoch: 34 | Train Loss: 0.2880 |Val Loss: 0.3005 |Val Accuracy: 23.8525%\nEpoch: 35 | Train Loss: 0.2778 |Val Loss: 0.3023 |Val Accuracy: 24.8047%\nEpoch: 36 | Train Loss: 0.2498 |Val Loss: 0.3080 |Val Accuracy: 26.5381%\nEpoch: 37 | Train Loss: 0.2481 |Val Loss: 0.3093 |Val Accuracy: 26.4893%\nEpoch: 38 | Train Loss: 0.2456 |Val Loss: 0.3024 |Val Accuracy: 28.0273%\nEpoch: 39 | Train Loss: 0.2354 |Val Loss: 0.2979 |Val Accuracy: 27.2461%\nEpoch: 40 | Train Loss: 0.2307 |Val Loss: 0.3157 |Val Accuracy: 26.9043%\nEpoch: 41 | Train Loss: 0.2358 |Val Loss: 0.3371 |Val Accuracy: 23.8770%\nEpoch: 42 | Train Loss: 0.2462 |Val Loss: 0.3029 |Val Accuracy: 27.9053%\nEpoch: 43 | Train Loss: 0.2099 |Val Loss: 0.3024 |Val Accuracy: 27.7344%\nEpoch: 44 | Train Loss: 0.1986 |Val Loss: 0.3073 |Val Accuracy: 29.1748%\nEpoch: 45 | Train Loss: 0.1983 |Val Loss: 0.3062 |Val Accuracy: 29.6875%\nEpoch: 46 | Train Loss: 0.1862 |Val Loss: 0.3109 |Val Accuracy: 29.5654%\nEpoch: 47 | Train Loss: 0.2053 |Val Loss: 0.3069 |Val Accuracy: 27.4170%\nEpoch: 48 | Train Loss: 0.1952 |Val Loss: 0.3008 |Val Accuracy: 29.6875%\nEpoch: 49 | Train Loss: 0.1929 |Val Loss: 0.2963 |Val Accuracy: 28.8086%\nEpoch: 50 | Train Loss: 0.1820 |Val Loss: 0.3106 |Val Accuracy: 29.1504%\nEpoch: 51 | Train Loss: 0.1940 |Val Loss: 0.2939 |Val Accuracy: 28.9062%\nEpoch: 52 | Train Loss: 0.1828 |Val Loss: 0.2953 |Val Accuracy: 28.3447%\nEpoch: 53 | Train Loss: 0.1639 |Val Loss: 0.3147 |Val Accuracy: 29.6143%\nEpoch: 54 | Train Loss: 0.1631 |Val Loss: 0.3036 |Val Accuracy: 30.1025%\nEpoch: 55 | Train Loss: 0.1702 |Val Loss: 0.3094 |Val Accuracy: 29.3701%\nEpoch: 56 | Train Loss: 0.1653 |Val Loss: 0.3110 |Val Accuracy: 28.7842%\nEpoch: 57 | Train Loss: 0.1478 |Val Loss: 0.3087 |Val Accuracy: 30.0293%\nEpoch: 58 | Train Loss: 0.1420 |Val Loss: 0.3252 |Val Accuracy: 28.6133%\nEpoch: 59 | Train Loss: 0.1441 |Val Loss: 0.3172 |Val Accuracy: 29.9561%\nTest Accuracy: 27.5146%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▆▆▆▅▅▅▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▃▃▄▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>validation_loss</td><td>████▇▇▆▆▅▅▄▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.27515</td></tr><tr><td>training_loss</td><td>0.1441</td></tr><tr><td>validation_accuracy</td><td>0.29956</td></tr><tr><td>validation_loss</td><td>0.31718</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">frosty-sweep-9</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/qy3sttbt' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/qy3sttbt</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_131529-qy3sttbt/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o4g3kc1r with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Sgd\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.75\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_133413-o4g3kc1r</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/o4g3kc1r' target=\"_blank\">restful-sweep-10</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/sweeps/gcp6d86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/o4g3kc1r' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/o4g3kc1r</a>"},"metadata":{}},{"name":"stdout","text":"True\nEpoch: 0 | Train Loss: 1.5124 |Val Loss: 0.9800 |Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.1015 |Val Loss: 0.9603 |Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.0462 |Val Loss: 0.8552 |Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 0.9900 |Val Loss: 0.8659 |Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 0.9746 |Val Loss: 0.8348 |Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 0.9619 |Val Loss: 0.8537 |Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 0.9582 |Val Loss: 0.8331 |Val Accuracy: 0.0000%\nEpoch: 7 | Train Loss: 0.9557 |Val Loss: 0.8325 |Val Accuracy: 0.0000%\nEpoch: 8 | Train Loss: 0.9458 |Val Loss: 0.8612 |Val Accuracy: 0.0000%\nEpoch: 9 | Train Loss: 0.9511 |Val Loss: 0.8419 |Val Accuracy: 0.0000%\nEpoch: 10 | Train Loss: 0.9307 |Val Loss: 0.8141 |Val Accuracy: 0.0000%\nEpoch: 11 | Train Loss: 0.9106 |Val Loss: 0.8038 |Val Accuracy: 0.0000%\nEpoch: 12 | Train Loss: 0.8958 |Val Loss: 0.7940 |Val Accuracy: 0.0000%\nEpoch: 13 | Train Loss: 0.8688 |Val Loss: 0.8162 |Val Accuracy: 0.0000%\nEpoch: 14 | Train Loss: 0.8666 |Val Loss: 0.7722 |Val Accuracy: 0.0000%\nEpoch: 15 | Train Loss: 0.8337 |Val Loss: 0.7420 |Val Accuracy: 0.0000%\nEpoch: 16 | Train Loss: 0.8236 |Val Loss: 0.7602 |Val Accuracy: 0.0244%\nEpoch: 17 | Train Loss: 0.7816 |Val Loss: 0.6881 |Val Accuracy: 0.0732%\nEpoch: 18 | Train Loss: 0.7515 |Val Loss: 0.6708 |Val Accuracy: 0.0977%\nEpoch: 19 | Train Loss: 0.7269 |Val Loss: 0.6381 |Val Accuracy: 0.4395%\nEpoch: 20 | Train Loss: 0.6748 |Val Loss: 0.6021 |Val Accuracy: 1.0986%\nEpoch: 21 | Train Loss: 0.6241 |Val Loss: 0.5449 |Val Accuracy: 2.1240%\nEpoch: 22 | Train Loss: 0.5811 |Val Loss: 0.5212 |Val Accuracy: 2.6123%\nEpoch: 23 | Train Loss: 0.5526 |Val Loss: 0.4668 |Val Accuracy: 4.4678%\nEpoch: 24 | Train Loss: 0.5197 |Val Loss: 0.4408 |Val Accuracy: 5.9814%\nEpoch: 25 | Train Loss: 0.4726 |Val Loss: 0.4251 |Val Accuracy: 8.2520%\nEpoch: 26 | Train Loss: 0.4484 |Val Loss: 0.4047 |Val Accuracy: 10.7178%\nEpoch: 27 | Train Loss: 0.4122 |Val Loss: 0.3952 |Val Accuracy: 12.0117%\nEpoch: 28 | Train Loss: 0.3985 |Val Loss: 0.3768 |Val Accuracy: 12.9395%\nEpoch: 29 | Train Loss: 0.3855 |Val Loss: 0.3705 |Val Accuracy: 15.1367%\nEpoch: 30 | Train Loss: 0.3726 |Val Loss: 0.3578 |Val Accuracy: 14.9658%\nEpoch: 31 | Train Loss: 0.3795 |Val Loss: 0.3449 |Val Accuracy: 15.0879%\nEpoch: 32 | Train Loss: 0.3440 |Val Loss: 0.3441 |Val Accuracy: 18.2373%\nEpoch: 33 | Train Loss: 0.3275 |Val Loss: 0.3506 |Val Accuracy: 19.6777%\nEpoch: 34 | Train Loss: 0.3412 |Val Loss: 0.3302 |Val Accuracy: 18.8721%\nEpoch: 35 | Train Loss: 0.3169 |Val Loss: 0.3311 |Val Accuracy: 20.8984%\nEpoch: 36 | Train Loss: 0.3132 |Val Loss: 0.3255 |Val Accuracy: 22.8271%\nEpoch: 37 | Train Loss: 0.2964 |Val Loss: 0.3181 |Val Accuracy: 22.2656%\nEpoch: 38 | Train Loss: 0.2842 |Val Loss: 0.3194 |Val Accuracy: 23.6084%\nEpoch: 39 | Train Loss: 0.2575 |Val Loss: 0.3425 |Val Accuracy: 26.2695%\nEpoch: 40 | Train Loss: 0.2848 |Val Loss: 0.3084 |Val Accuracy: 24.6826%\nEpoch: 41 | Train Loss: 0.2702 |Val Loss: 0.3000 |Val Accuracy: 25.8301%\nEpoch: 42 | Train Loss: 0.2629 |Val Loss: 0.3264 |Val Accuracy: 24.5361%\nEpoch: 43 | Train Loss: 0.2458 |Val Loss: 0.3140 |Val Accuracy: 26.7090%\nEpoch: 44 | Train Loss: 0.2319 |Val Loss: 0.3109 |Val Accuracy: 27.9785%\nEpoch: 45 | Train Loss: 0.2538 |Val Loss: 0.3063 |Val Accuracy: 27.2949%\nEpoch: 46 | Train Loss: 0.2441 |Val Loss: 0.3175 |Val Accuracy: 27.5879%\nEpoch: 47 | Train Loss: 0.2362 |Val Loss: 0.2975 |Val Accuracy: 28.1250%\nEpoch: 48 | Train Loss: 0.2338 |Val Loss: 0.2978 |Val Accuracy: 28.8330%\nEpoch: 49 | Train Loss: 0.2167 |Val Loss: 0.2994 |Val Accuracy: 29.7852%\nEpoch: 50 | Train Loss: 0.2271 |Val Loss: 0.3349 |Val Accuracy: 25.9033%\nEpoch: 51 | Train Loss: 0.2123 |Val Loss: 0.2979 |Val Accuracy: 29.6875%\nEpoch: 52 | Train Loss: 0.2075 |Val Loss: 0.3016 |Val Accuracy: 29.9805%\nEpoch: 53 | Train Loss: 0.1969 |Val Loss: 0.2991 |Val Accuracy: 29.8584%\nEpoch: 54 | Train Loss: 0.2001 |Val Loss: 0.2932 |Val Accuracy: 30.6641%\nEpoch: 55 | Train Loss: 0.2001 |Val Loss: 0.2923 |Val Accuracy: 30.7373%\nEpoch: 56 | Train Loss: 0.1694 |Val Loss: 0.3057 |Val Accuracy: 31.2256%\nEpoch: 57 | Train Loss: 0.1791 |Val Loss: 0.2848 |Val Accuracy: 29.5166%\nEpoch: 58 | Train Loss: 0.1750 |Val Loss: 0.3003 |Val Accuracy: 31.0547%\nEpoch: 59 | Train Loss: 0.1710 |Val Loss: 0.3058 |Val Accuracy: 31.2744%\nTest Accuracy: 28.5889%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▆▅▅▅▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▄▄▄▄▅▅▆▆▇▇▆▇▇▇▇███████</td></tr><tr><td>validation_loss</td><td>██▇▇▇▇▇▆▆▆▆▆▅▅▄▃▃▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.28589</td></tr><tr><td>training_loss</td><td>0.17101</td></tr><tr><td>validation_accuracy</td><td>0.31274</td></tr><tr><td>validation_loss</td><td>0.30577</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">restful-sweep-10</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_RNN/runs/o4g3kc1r' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN/runs/o4g3kc1r</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_RNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_RNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_133413-o4g3kc1r/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention Model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#import necessary libraries\nimport os\nimport wandb\nimport torch\nimport torch.nn as nn\nimport random\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as Function\nimport argparse\n\n# Check if CUDA is available\nuse_cuda = torch.cuda.is_available()\n\n# Set the device type to CUDA if available, otherwise use CPU\nif use_cuda:\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n  \nhidden_size = 256\ninput_lang = \"eng\"\ntarget_lang = \"hin\"\ncell_type = \"LSTM\"\nnum_layers_encoder = 2\nnum_layers_decoder = 2\ndrop_out = 0.2\nepochs = 5\nembedding_size = 256\nbi_directional = False\nbatch_size = 32\n\n  \nF=Function\nStart_Symbol, End_Symbol, Unknown, Padding = 0, 1, 2, 3\n\nclass Vocabulary:\n    def __init__(self):\n        self.char2count = {}\n        self.char2index = {}\n        self.n_chars = 4\n        self.index2char = {0: \"<\", 1: \">\", 2: \"?\", 3: \".\"}\n\n\n    def addWord(self, word):\n        for char in word:\n            if char not in self.char2index:\n                self.char2index[char] = self.n_chars\n                self.index2char[self.n_chars] = char\n                self.char2count[char] = 1\n                self.n_chars += 1\n            else:\n                self.char2count[char] += 1\n\n# Define a function to prepare the data\ndef prepareDataWithoutAttn(dir):\n    # Read the CSV file into a DataFrame with columns \"input\" and \"target\"\n    data = pd.read_csv(dir, sep=\",\", names=[\"input\", \"target\"])\n\n    # Find the maximum length of input and target sequences\n    # max_input_length = max([len(txt) for txt in data[\"input\"].to_list()])\n    max_input_length = 0\n    for txt in data[\"input\"].to_list():\n        max_input_length = max(max_input_length, len(txt))\n    \n    max_target_length = 0\n    for txt in data[\"target\"].to_list():\n        max_target_length = max(max_target_length, len(txt))\n    \n    max_len=0\n    if max_input_length > max_target_length:\n        max_len = max_input_length\n    else:\n        max_len = max_target_length\n    # max(max_input_length,max_target_length)\n\n    # Create Vocabulary objects for input and output languages\n    input_lang = Vocabulary()\n    output_lang = Vocabulary()\n\n    # Create pairs of input and target sequences\n    pairs = []\n    input_list, target_list = data[\"input\"].to_list(), data[\"target\"].to_list()\n    for i in range(len(input_list)):\n        pairs.append([input_list[i], target_list[i]])\n\n    # Add words to the respective vocabularies\n    for pair in pairs:\n        input_lang.addWord(pair[0])\n        output_lang.addWord(pair[1])\n\n    # Create a dictionary containing prepared data\n    # prepared_data = {\n    #     \"input_lang\": input_lang,\n    #     \"output_lang\": output_lang,\n    #     \"pairs\": pairs,\n    #     \"max_len\": max_len\n    # }\n\n    return input_lang,output_lang,pairs,max_len\n\n# Define a helper function to convert a word to a tensor\ndef helpTensorWithoutAttn(lang, word, max_length):\n    index_list = []\n    for char in word:\n        if char in lang.char2index.keys():\n            index_list.append(lang.char2index[char])\n        else:\n            index_list.append(Unknown)\n    indexes = index_list\n    indexes.append(End_Symbol)\n    indexes.extend([Padding] * (max_length - len(indexes)))\n    result = torch.LongTensor(indexes)\n    if use_cuda:\n        return result.cuda()\n    else:\n        return result\n\n# Define a function to convert pairs of input and target sequences to tensors\ndef MakeTensorWithoutAttn(input_lang, output_lang, pairs, reach):\n    res = []\n    for pair in pairs:\n        # Convert input and target sequences to tensors using the helpTensorWithoutAttn function\n        input_variable = helpTensorWithoutAttn(input_lang, pair[0], reach)\n        target_variable = helpTensorWithoutAttn(output_lang, pair[1], reach)\n        res.append((input_variable, target_variable))\n    return res\n\n#Encoder Class\nclass EncoderRNNWithoutAttn(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers_encoder, cell_type, drop_out, bi_directional):\n        super(EncoderRNNWithoutAttn, self).__init__()\n\n        # Initialize the EncoderRNNWithoutAttn with the provided parameters\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.num_layers_encoder = num_layers_encoder\n        self.cell_type = cell_type\n        self.drop_out = drop_out\n        self.bi_directional = bi_directional\n\n        # Create an embedding layer\n        self.embedding = nn.Embedding(input_size, self.embedding_size)\n        self.dropout = nn.Dropout(self.drop_out)\n\n        # Create the specified cell layer (RNN, GRU, or LSTM)\n        cell_map = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}\n        self.cell_layer = cell_map[self.cell_type](\n            self.embedding_size,\n            self.hidden_size,\n            num_layers=self.num_layers_encoder,\n            dropout=self.drop_out,\n            bidirectional=self.bi_directional,\n        )\n\n    def forward(self, input, batch_size, hidden):\n        # Apply dropout to the embedded input sequence\n        embedded = self.dropout(self.embedding(input).view(1, batch_size, -1))\n\n        # Pass the embedded input through the cell layer\n        output, hidden = self.cell_layer(embedded, hidden)\n        return output, hidden\n\n    def initHidden(self, batch_size, num_layers_enc):\n        # Initialize the hidden state with zeros\n        res = torch.zeros(num_layers_enc * 2 if self.bi_directional else num_layers_enc, batch_size, self.hidden_size)\n\n        # Move the hidden state to the GPU if use_cuda is True, else return as is\n        return res.cuda() if use_cuda else res\n\n#Decoder class\nclass DecoderRNNWithoutAttn(nn.Module):\n    def __init__(self, embedding_size, hidden_size, num_layers_decoder, cell_type, drop_out, bi_directional, output_size):\n        super(DecoderRNNWithoutAttn, self).__init__()\n\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.num_layers_decoder = num_layers_decoder\n        self.cell_type = cell_type\n        self.drop_out = drop_out\n        self.bi_directional = bi_directional\n\n        # Create an embedding layer\n        self.embedding = nn.Embedding(output_size, self.embedding_size)\n        self.dropout = nn.Dropout(self.drop_out)\n\n        # Create the specified cell layer (RNN, GRU, or LSTM)\n        cell_map = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}\n        self.cell_layer = cell_map[self.cell_type](\n            self.embedding_size,\n            self.hidden_size,\n            num_layers=self.num_layers_decoder,\n            dropout=self.drop_out,\n            bidirectional=self.bi_directional,\n        )\n\n        # Linear layer for output\n        self.out = nn.Linear(\n            self.hidden_size * 2 if self.bi_directional else self.hidden_size,\n            output_size,\n        )\n\n        # Softmax activation\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, batch_size, hidden):\n        # Apply dropout to the embedded input sequence and pass it through the cell layer\n        output = Function.relu(self.dropout(self.embedding(input).view(1, batch_size, -1)))\n        output, hidden = self.cell_layer(output, hidden)\n\n        # Apply softmax activation to the output\n        output = self.softmax(self.out(output[0]))\n        return output, hidden\n\n# Function to calculate loss (if is_training then training loss else validation loss)\ndef calc_lossWithoutAttn(encoder, decoder, input_tensor, target_tensor, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training, teacher_forcing_ratio=0.5):\n    # Initialize the encoder hidden state\n    output_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n    # Check if LSTM and initialize cell state\n    if cell_type == \"LSTM\":\n        encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n        output_hidden = (output_hidden, encoder_cell_state)\n\n    # Zero the gradients\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    # Get input and target sequence lengths\n    # input_length = input_tensor.size(0)\n    # target_length = target_tensor.size(0)\n\n    # Initialize loss\n    loss = 0\n\n    # Encoder forward pass\n    for ei in range(input_tensor.size(0)):\n        output_hidden = encoder(input_tensor[ei], batch_size, output_hidden)[1]\n\n    # Initialize decoder input\n    decoder_input = torch.LongTensor([Start_Symbol] * batch_size)\n    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n    # Determine if using teacher forcing\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    # Loop over target sequence\n    if is_training:\n        # Training phase\n        for di in range(target_tensor.size(0)):\n            decoder_output, output_hidden = decoder(decoder_input, batch_size, output_hidden)\n            decoder_input = target_tensor[di] if use_teacher_forcing else decoder_output.argmax(dim=1)\n            loss = criterion(decoder_output, target_tensor[di]) + loss\n    else:\n        # Validation phase\n        with torch.no_grad():\n            for di in range(target_tensor.size(0)):\n                decoder_output, output_hidden = decoder(decoder_input, batch_size, output_hidden)\n                loss += criterion(decoder_output, target_tensor[di])\n                decoder_input = decoder_output.argmax(dim=1)\n\n    # Backpropagation and optimization in training phase\n    if is_training:\n        loss.backward()\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n\n    # Return the average loss per target length\n    return loss.item() / target_tensor.size(0)\n\n\n# Calculate the accuracyWithoutAttn of the Seq2SeqWithoutAttn model\ndef accuracyWithoutAttn(encoder, decoder, loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang):\n    with torch.no_grad():\n        total = 0\n        correct = 0\n\n        for batch_x, batch_y in loader:\n            # Initialize encoder hidden state\n            encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n            input_variable = Variable(batch_x.transpose(0, 1))\n            target_variable = Variable(batch_y.transpose(0, 1))\n\n            # Check if LSTM and initialize cell state\n            if cell_type == \"LSTM\":\n                encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n                encoder_hidden = (encoder_hidden, encoder_cell_state)\n\n            # input_length = input_variable.size()[0]\n            # target_length = target_variable.size()[0]\n\n            output = torch.LongTensor(target_variable.size()[0], batch_size)\n\n            # Initialize encoder outputs\n            # encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hidden_size))\n            # encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n            # Encoder forward pass\n            for ei in range(input_variable.size()[0]):\n                encoder_hidden = encoder(input_variable[ei], batch_size, encoder_hidden)[1]\n\n            decoder_input = Variable(torch.LongTensor([Start_Symbol] * batch_size))\n            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n            decoder_hidden = encoder_hidden\n\n            # Decoder forward pass\n            for di in range(target_variable.size()[0]):\n                decoder_output, decoder_hidden = decoder(decoder_input, batch_size, decoder_hidden)\n                topi = decoder_output.data.topk(1)[1]\n                output[di] = torch.cat(tuple(topi))\n                decoder_input = torch.cat(tuple(topi))\n\n            output = output.transpose(0, 1)\n\n            # Calculate accuracyWithoutAttn\n            for di in range(output.size()[0]):\n                ignore = [Start_Symbol, End_Symbol, Padding]\n                sent = [output_lang.index2char[letter.item()] for letter in output[di] if letter not in ignore]\n                y = [output_lang.index2char[letter.item()] for letter in batch_y[di] if letter not in ignore]\n                if sent == y:\n                    correct += 1\n                total += 1\n\n    return (correct / total) * 100\n\n# Train and evaluate the Seq2SeqWithoutAttn model\ndef seq2seqWithoutAttn(encoder, decoder, train_loader, val_loader, test_loader, lr, optimizer, epochs, max_length_word, num_layers_enc, output_lang):\n    max_length = max_length_word - 1\n    # Define the optimizer and criterion\n    encoder_optimizer = optim.NAdam(encoder.parameters(), lr=lr) if optimizer == \"nadam\" else optim.Adam(encoder.parameters(), lr=lr)\n    decoder_optimizer = optim.NAdam(decoder.parameters(), lr=lr) if optimizer == \"nadam\" else optim.Adam(decoder.parameters(), lr=lr)\n    criterion = nn.NLLLoss()\n\n    for epoch in range(epochs):\n        train_loss_total = 0\n        val_loss_total = 0\n\n        # Training phase\n        for batch_x, batch_y in train_loader:\n            batch_x = Variable(batch_x.transpose(0, 1))\n            batch_y = Variable(batch_y.transpose(0, 1))\n            # Calculate the training loss\n            loss = calc_lossWithoutAttn(encoder, decoder, batch_x, batch_y, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training=True)\n            train_loss_total += loss\n\n        train_loss_avg = train_loss_total / len(train_loader)\n        print(f\"Epoch: {epoch} | Train Loss: {train_loss_avg:.4f} |\", end=\"\")\n\n        # Validation phase\n        for batch_x, batch_y in val_loader:\n            batch_x = Variable(batch_x.transpose(0, 1))\n            batch_y = Variable(batch_y.transpose(0, 1))\n            # Calculate the validation loss\n            loss = calc_lossWithoutAttn(encoder, decoder, batch_x, batch_y, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training=False)\n            val_loss_total += loss\n\n        val_loss_avg = val_loss_total / len(val_loader)\n        print(f\"Val Loss: {val_loss_avg:.4f} |\", end=\"\")\n\n        # Calculate validation accuracyWithoutAttn\n        val_acc = accuracyWithoutAttn(encoder, decoder, val_loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang)\n        val_acc /= 100\n        print(f\"Val Accuracy: {val_acc:.4%}\")\n        \n        if epochs-1==epoch :\n            test_acc = accuracyWithoutAttn(encoder, decoder, test_loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang)\n            test_acc /= 100\n            print(f\"Test Accuracy: {test_acc:.4%}\")\n            \n\n\ndef prepareData(dir):\n\n    input_lang = Vocabulary()\n    output_lang = Vocabulary()\n\n    data = pd.read_csv(dir, sep=\",\", names=[\"input\", \"target\"])\n\n    input_list = data[\"input\"].to_list()\n    target_list = data[\"target\"].to_list()\n\n    max_target_length = max([len(txt) for txt in data[\"target\"].to_list()])\n\n    pairs = []\n    for i in range(len(target_list)):\n        pairs.append([input_list[i], target_list[i]])\n\n    max_input_length = max([len(txt) for txt in data[\"input\"].to_list()])\n    for pair in pairs:\n        input_lang.addWord(pair[0])\n        output_lang.addWord(pair[1])\n\n    prepared_data = {\n        \"input_lang\": input_lang,\n        \"output_lang\": output_lang,\n        \"pairs\": pairs,\n        \"max_input_length\": max_input_length,\n        \"max_target_length\": max_target_length,\n    }\n\n    return prepared_data\n\ndef helpindex(lang, word):\n    l=[]\n    for i in range(len(word)):\n        if word[i] not in lang.char2index.keys():\n            l.append(Unknown)\n        else:\n            l.append(lang.char2index[word[i]])\n    return l\n\ndef helpTensor(lang, word, max_length):\n    indexes = helpindex(lang, word)\n    indexes.append(End_Symbol)\n    indexes.extend([Padding] * (max_length - len(indexes)))\n    result = torch.LongTensor(indexes)\n    if use_cuda==False:\n        return result\n    else:\n        return result.cuda()\n\ndef MakeTensor(input_lang, output_lang, pairs, max_length):\n    res = []\n    for pair in pairs:\n        input_variable = helpTensor(input_lang, pair[0], max_length)\n        target_variable = helpTensor(output_lang, pair[1], max_length)\n        res.append((input_variable, target_variable))\n    return res\n\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, embedding_size,hidden_size,num_layers_encoder,cell_type,drop_out,bi_directional):\n        super(EncoderRNN, self).__init__()\n\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.num_layers_encoder = num_layers_encoder\n        self.cell_type = cell_type\n        self.drop_out = drop_out\n        self.bi_directional = bi_directional\n\n        self.embedding = nn.Embedding(input_size, self.embedding_size)\n        self.dropout = nn.Dropout(self.drop_out)\n\n        cell_map = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}\n        self.cell_layer = cell_map[self.cell_type](\n            self.embedding_size,\n            self.hidden_size,\n            num_layers=self.num_layers_encoder,\n            dropout=self.drop_out,\n            bidirectional=self.bi_directional,\n        )\n\n    def forward(self, input, batch_size, hidden):\n        embedded = self.dropout(self.embedding(input).view(1, batch_size, -1))\n        output, hidden = self.cell_layer(embedded, hidden)\n        return output, hidden\n\n    def initHidden(self, batch_size, num_layers_enc):\n        res = torch.zeros(\n            num_layers_enc * 2 if self.bi_directional else num_layers_enc,\n            batch_size,\n            self.hidden_size,\n        )\n        if use_cuda== False:\n            return res\n        else:\n            return res.cuda()\n\n\n\nclass DecoderAttention(nn.Module):\n    def __init__(\n        self,\n        hidden_size,\n        embedding_size,\n        cell_type,\n        num_layers_decoder,\n        drop_out,\n        max_length_word,\n        output_size,\n    ):\n\n        super(DecoderAttention, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.embedding_size = embedding_size\n        self.cell_type = cell_type\n        self.num_layers_decoder = num_layers_decoder\n        self.drop_out = drop_out\n        self.max_length_word = max_length_word\n\n        self.embedding = nn.Embedding(output_size, embedding_dim=self.embedding_size)\n        self.attention_layer = nn.Linear(\n            self.embedding_size + self.hidden_size, self.max_length_word\n        )\n        self.attention_combine = nn.Linear(\n            self.embedding_size + self.hidden_size, self.embedding_size\n        )\n        self.dropout = nn.Dropout(self.drop_out)\n\n        self.cell_layer = None\n        cell_map = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}\n\n        if self.cell_type in cell_map:\n            self.cell_layer = cell_map[self.cell_type](\n                self.embedding_size,\n                self.hidden_size,\n                num_layers=self.num_layers_decoder,\n                dropout=self.drop_out,\n            )\n\n        self.out = nn.Linear(self.hidden_size, output_size)\n\n    def forward(self, input, batch_size, hidden, encoder_outputs):\n\n        embedded = self.embedding(input).view(1, batch_size, -1)\n\n        attention_weights = None\n        if self.cell_type == \"LSTM\":\n            attention_weights = Function.softmax(\n                self.attention_layer(torch.cat((embedded[0], hidden[0][0]), 1)), dim=1\n            )\n\n        else:\n            attention_weights = Function.softmax(\n                self.attention_layer(torch.cat((embedded[0], hidden[0]), 1)), dim=1\n            )\n\n        attention_applied = torch.bmm(\n            attention_weights.view(batch_size, 1, self.max_length_word),\n            encoder_outputs,\n        ).view(1, batch_size, -1)\n        output = torch.cat((embedded[0], attention_applied[0]), 1)\n        output = self.attention_combine(output).unsqueeze(0)\n        output = Function.relu(output)\n        # if self.cell_type=RNN\" :\n        output, hidden = self.cell_layer(output, hidden)\n        output = Function.log_softmax(self.out(output[0]), dim=1)\n\n        return output, hidden, attention_weights\n\n\n\ndef train_and_val_with_attn(\n    encoder,\n    decoder,\n    encoder_optimizer,\n    decoder_optimizer,\n    input_tensor,\n    target_tensor,\n    criterion,\n    batch_size,\n    cell_type,\n    num_layers_enc,\n    max_length,is_training,\n    teacher_forcing_ratio=0.5,\n):\n\n    encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n    if cell_type == \"LSTM\":\n        encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n        encoder_hidden = (encoder_hidden, encoder_cell_state)\n\n    if is_training:\n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n\n    encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hidden_size))\n    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n    loss = 0\n\n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(\n            input_tensor[ei], batch_size, encoder_hidden\n        )\n        encoder_outputs[ei] = encoder_output[0]\n\n    decoder_input = Variable(torch.LongTensor([Start_Symbol] * batch_size))\n    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n    decoder_hidden = encoder_hidden\n    if is_training:\n        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n        if use_teacher_forcing == False:\n            for di in range(target_length):\n                decoder_output, decoder_hidden, decoder_attention = decoder(\n                    decoder_input,\n                    batch_size,\n                    decoder_hidden,\n                    encoder_outputs.reshape(batch_size, max_length, encoder.hidden_size),\n                )\n                #2 for loop ko bhar dal de\n                topv, topi = decoder_output.data.topk(1)\n                decoder_input = torch.cat(tuple(topi))\n\n                decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n                loss += criterion(decoder_output, target_tensor[di])\n        else:\n            for di in range(target_length):\n                decoder_output, decoder_hidden, decoder_attention = decoder(\n                    decoder_input,\n                    batch_size,\n                    decoder_hidden,\n                    encoder_outputs.reshape(batch_size, max_length, encoder.hidden_size),\n                )\n                loss += criterion(decoder_output, target_tensor[di])\n                decoder_input = target_tensor[di]\n            \n\n        loss.backward()\n\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n    else :\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input,\n                batch_size,\n                decoder_hidden,\n                encoder_outputs.reshape(batch_size, max_length, encoder.hidden_size),\n            )\n            topv, topi = decoder_output.data.topk(1)\n            decoder_input = torch.cat(tuple(topi))\n\n            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n            loss += criterion(decoder_output, target_tensor[di])\n\n\n    return loss.item() / target_length\n\n\n# batch_size,num_layers_enc,cell_type,output_lang,criterion,\ndef accuracy_with_attention(\n    encoder,\n    decoder,\n    loader,\n    batch_size,\n    num_layers_enc,\n    cell_type,\n    output_lang,\n    criterion,\n    max_length,\n):\n\n    with torch.no_grad():\n\n        # batch_size = configuration[\"batch_size\"]\n        total = 0\n        correct = 0\n\n        for batch_x, batch_y in loader:\n\n            encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n            input_variable = Variable(batch_x.transpose(0, 1))\n            target_variable = Variable(batch_y.transpose(0, 1))\n\n            if cell_type == \"LSTM\":\n                encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n                encoder_hidden = (encoder_hidden, encoder_cell_state)\n\n            input_length = input_variable.size()[0]\n            target_length = target_variable.size()[0]\n\n            output = torch.LongTensor(target_length, batch_size)\n\n            encoder_outputs = Variable(\n                torch.zeros(max_length, batch_size, encoder.hidden_size)\n            )\n            encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n            for ei in range(input_length):\n                encoder_output, encoder_hidden = encoder(\n                    input_variable[ei], batch_size, encoder_hidden\n                )\n                encoder_outputs[ei] = encoder_output[0]\n\n            decoder_input = Variable(torch.LongTensor([Start_Symbol] * batch_size))\n            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n            decoder_hidden = encoder_hidden\n\n            for di in range(target_length):\n                decoder_output, decoder_hidden, decoder_attention = decoder(\n                    decoder_input,\n                    batch_size,\n                    decoder_hidden,\n                    encoder_outputs.reshape(\n                        batch_size, max_length, encoder.hidden_size\n                    ),\n                )\n                topv, topi = decoder_output.data.topk(1)\n                decoder_input = torch.cat(tuple(topi))\n                output[di] = torch.cat(tuple(topi))\n\n            output = output.transpose(0, 1)\n            for di in range(output.size()[0]):\n                ignore = [Start_Symbol, End_Symbol, Padding]\n                sent = [\n                    output_lang.index2char[letter.item()]\n                    for letter in output[di]\n                    if letter not in ignore\n                ]\n                y = [\n                    output_lang.index2char[letter.item()]\n                    for letter in batch_y[di]\n                    if letter not in ignore\n                ]\n                if sent == y:\n                    correct += 1\n                total += 1\n\n    return (correct / total) * 100\n\n\ndef cal_val_loss_with_attn(\n    encoder,\n    decoder,\n    input_tensor,\n    target_tensor,\n    batch_size,\n    criterion,\n    cell_type,\n    num_layers_enc,\n    max_length,\n):\n\n    with torch.no_grad():\n\n        encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n        if cell_type == \"LSTM\":\n            encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n            encoder_hidden = (encoder_hidden, encoder_cell_state)\n\n        input_length = input_tensor.size()[0]\n        target_length = target_tensor.size()[0]\n\n        encoder_outputs = Variable(\n            torch.zeros(max_length, batch_size, encoder.hidden_size)\n        )\n        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n        loss = 0\n\n        for ei in range(input_length):\n            encoder_output, encoder_hidden = encoder(\n                input_tensor[ei], batch_size, encoder_hidden\n            )\n            encoder_outputs[ei] = encoder_output[0]\n\n        decoder_input = Variable(torch.LongTensor([Start_Symbol] * batch_size))\n        if use_cuda== True:\n            decoder_input = decoder_input.cuda()  \n        else :\n            decoder_input = decoder_input\n\n        decoder_hidden = encoder_hidden\n\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input,\n                batch_size,\n                decoder_hidden,\n                encoder_outputs.reshape(batch_size, max_length, encoder.hidden_size),\n            )\n            topv, topi = decoder_output.data.topk(1)\n            decoder_input = torch.cat(tuple(topi))\n\n            if use_cuda== True:\n                decoder_input = decoder_input.cuda()  \n            else :\n                decoder_input = decoder_input\n            loss += criterion(decoder_output, target_tensor[di])\n\n    return loss.item() / target_length\n\n\ndef Attention_seq2seq(\n    encoder,\n    decoder,\n    train_loader,\n    val_loader,\n    test_loader,\n    learning_rate,\n    optimizer,\n    epochs,\n    max_length_word,\n    attention,\n    num_layers_enc,\n    output_lang,\n):\n    max_length = max_length_word - 1\n    encoder_optimizer = (\n        optim.NAdam(encoder.parameters(), lr=learning_rate)\n        if optimizer == \"nadam\"\n        else optim.Adam(encoder.parameters(), lr=learning_rate)\n    )\n    decoder_optimizer = (\n        optim.NAdam(decoder.parameters(), lr=learning_rate)\n        if optimizer == \"nadam\"\n        else optim.Adam(decoder.parameters(), lr=learning_rate)\n    )\n    criterion = nn.NLLLoss()\n\n    for epoch in range(epochs):\n        train_loss_total, val_loss_total  =0, 0\n        \n        for batchx, batchy in train_loader:\n            batchx = Variable(batchx.transpose(0, 1))\n            batchy = Variable(batchy.transpose(0, 1))\n            loss = train_and_val_with_attn(\n                encoder,\n                decoder,\n                encoder_optimizer,\n                decoder_optimizer,\n                batchx,\n                batchy,\n                criterion,\n                batch_size,\n                cell_type,\n                num_layers_enc,\n                max_length + 1,\n                True, #is_training\n            )\n            train_loss_total += loss\n\n        train_loss_avg = train_loss_total / len(train_loader)\n        print(f\"Epoch: {epoch} | Train Loss: {train_loss_avg:.4f} | \", end=\"\")\n\n        for batchx, batchy in val_loader:\n            batchx = Variable(batchx.transpose(0, 1))\n            batchy = Variable(batchy.transpose(0, 1))\n            loss = train_and_val_with_attn(\n                encoder,\n                decoder,\n                encoder_optimizer,\n                decoder_optimizer,\n                batchx,\n                batchy,\n                criterion,\n                batch_size,\n                cell_type,\n                num_layers_enc,\n                max_length + 1,\n                False,#is_training=\n            )\n            val_loss_total += loss\n\n        val_loss_avg = val_loss_total / len(val_loader)\n        print(f\"Val Loss: {val_loss_avg:.4f} | \", end=\"\")\n        val_acc = accuracy_with_attention(\n            encoder,\n            decoder,\n            val_loader,\n            batch_size,\n            num_layers_enc,\n            cell_type,\n            output_lang,\n            criterion,\n            max_length + 1,\n        )\n        val_acc = val_acc / 100\n        print(f\"Val Accuracy: {val_acc:.4%}\")\n        if epochs-1==epoch:\n            test_acc = accuracy_with_attention(\n            encoder,\n            decoder,\n            test_loader,\n            batch_size,\n            num_layers_enc,\n            cell_type,\n            output_lang,\n            criterion,\n            max_length + 1,\n        )\n            test_acc = test_acc / 100\n            print(f\"Test Accuracy: {test_acc:.4%}\")\n\ndef to_dict(input_lang,output_lang,pairs,max_len):\n    d = {\n        \"input_lang\": input_lang,\n        \"output_lang\": output_lang,\n        \"pairs\": pairs,\n        \"max_len\": max_len\n    }\n    return d\n\ndef main(flag):\n    teacher_forcing_ratio = 0.5\n    optimizer = \"Nadam\"\n    learning_rate = 0.001\n    train_path = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_train.csv\"\n    validation_path = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_valid.csv\"\n    test_path = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_test.csv\"\n\n\n    if flag:\n        train_prepared_data = prepareData(train_path)\n        input_langs, output_langs, pairs = (\n            train_prepared_data[\"input_lang\"],\n            train_prepared_data[\"output_lang\"],\n            train_prepared_data[\"pairs\"],\n        )\n        print(\"train:sample:\", random.choice(pairs))\n        print(f\"Number of training examples: {len(pairs)}\")\n\n        max_input_length, max_target_length = (\n            train_prepared_data[\"max_input_length\"],\n            train_prepared_data[\"max_target_length\"],\n        )\n\n        # validation\n        val_prepared_data = prepareData(validation_path)\n        val_pairs = val_prepared_data[\"pairs\"]\n        print(\"validation:sample:\", random.choice(val_pairs))\n        print(f\"Number of validation examples: {len(val_pairs)}\")\n        # Test\n        max_input_length_val, max_target_length_val = (\n            val_prepared_data[\"max_input_length\"],\n            val_prepared_data[\"max_target_length\"],\n        )\n        test_prepared_data = prepareData(validation_path)\n        test_pairs = test_prepared_data[\"pairs\"]\n        print(\"Test:sample:\", random.choice(test_pairs))\n        print(f\"Number of Test examples: {len(test_pairs)}\")\n\n        max_input_length_test, max_target_length_test = (\n            test_prepared_data[\"max_input_length\"],\n            test_prepared_data[\"max_target_length\"],\n        )\n        max_len_all = (\n            max(\n                max_input_length,\n                max_target_length,\n                max_input_length_val,\n                max_target_length_val,\n                max_input_length_test,\n                max_target_length_test,\n            )\n            + 1\n        )\n\n        max_len = max(max_input_length, max_target_length) + 3\n        print(max_len)\n\n        pairs = MakeTensor(input_langs, output_langs, pairs, max_len)\n        val_pairs = MakeTensor(input_langs, output_langs, val_pairs, max_len)\n        test_pairs = MakeTensor(input_langs, output_langs, test_pairs, max_len)\n\n        train_loader = DataLoader(pairs, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_pairs, batch_size=batch_size, shuffle=True)\n        test_loader = DataLoader(test_pairs, batch_size=batch_size, shuffle=True)\n\n        encoder1 = EncoderRNN(\n            input_langs.n_chars,\n            embedding_size,\n            hidden_size,\n            num_layers_encoder,\n            cell_type,\n            drop_out,\n            bi_directional,\n        )\n        attndecoder1 = DecoderAttention(\n            hidden_size,\n            embedding_size,\n            cell_type,\n            num_layers_decoder,\n            drop_out,\n            max_len,\n            output_langs.n_chars,\n        )\n        if use_cuda== True:\n            encoder1 = encoder1.cuda()\n            attndecoder1 = attndecoder1.cuda()\n        print(\"with attention\")\n        attention = True\n        Attention_seq2seq(\n            encoder1,\n            attndecoder1,\n            train_loader,\n            val_loader,\n            test_loader,\n            learning_rate,\n            optimizer,\n            epochs,\n            max_len,\n            attention,\n            num_layers_encoder,\n            output_langs,\n        )\n    else:\n        # Prepare training data\n        _input_lang,_output_lang,_pairs,_max_len = prepareDataWithoutAttn(train_path)\n        train_prepared_data = to_dict(_input_lang,_output_lang,_pairs,_max_len)\n        input_langs, output_langs, pairs = train_prepared_data[\"input_lang\"], train_prepared_data[\"output_lang\"], train_prepared_data[\"pairs\"]\n        print(\"train:sample:\", random.choice(pairs))\n        print(f\"Number of training examples: {len(pairs)}\")\n        max_len = train_prepared_data[\"max_len\"]\n\n        # Prepare validation data\n        _input_lang,_output_lang,_pairs,_max_len = prepareDataWithoutAttn(validation_path)\n        val_prepared_data = to_dict(_input_lang,_output_lang,_pairs,_max_len)\n        val_pairs = val_prepared_data[\"pairs\"]\n        print(\"validation:sample:\", random.choice(val_pairs))\n        print(f\"Number of validation examples: {len(val_pairs)}\")\n        max_len_val = val_prepared_data[\"max_len\"]\n\n        # Prepare test data\n        _input_lang,_output_lang,_pairs,_max_len = prepareDataWithoutAttn(test_path)\n        test_prepared_data = to_dict(_input_lang,_output_lang,_pairs,_max_len)\n        test_pairs = test_prepared_data[\"pairs\"]\n        print(\"Test:sample:\", random.choice(test_pairs))\n        print(f\"Number of Test examples: {len(test_pairs)}\")\n\n        max_len_test = test_prepared_data[\"max_len\"]\n        max_len = max(max_len, max_len_val, max_len_test) + 4\n        print(max_len)\n\n        # Convert data to tensors and create data loaders\n        pairs = MakeTensorWithoutAttn(input_langs, output_langs, pairs, max_len)\n        val_pairs = MakeTensorWithoutAttn(input_langs, output_langs, val_pairs, max_len)\n        test_pairs = MakeTensorWithoutAttn(input_langs, output_langs, test_pairs, max_len)\n\n        train_loader = DataLoader(pairs, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_pairs, batch_size=batch_size, shuffle=True)\n        test_loader = DataLoader(test_pairs, batch_size=batch_size, shuffle=True)\n\n        # Create the encoder and decoder models\n        encoder1 = EncoderRNNWithoutAttn(input_langs.n_chars, embedding_size, hidden_size, num_layers_encoder, cell_type, drop_out, bi_directional)\n        decoder1 = DecoderRNNWithoutAttn(embedding_size, hidden_size, num_layers_encoder, cell_type, drop_out, bi_directional, output_langs.n_chars)\n\n        if use_cuda:\n            encoder1, decoder1 = encoder1.cuda(), decoder1.cuda()\n\n        print(\"vanilla seq2seqWithoutAttn\")\n        # Train and evaluate the Seq2SeqWithoutAttn model\n        seq2seqWithoutAttn(encoder1, decoder1, train_loader, val_loader, test_loader, learning_rate, optimizer, epochs, max_len, num_layers_encoder, output_langs)\n\n# attention_flag=False\n# main(attention_flag)\nattention_flag=True\nmain(attention_flag)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T14:08:03.070563Z","iopub.execute_input":"2024-05-14T14:08:03.071185Z","iopub.status.idle":"2024-05-14T14:20:17.751760Z","shell.execute_reply.started":"2024-05-14T14:08:03.071153Z","shell.execute_reply":"2024-05-14T14:20:17.750899Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"train:sample: ['malarme', 'मलारमे']\nNumber of training examples: 51200\nvalidation:sample: ['javadekar', 'जावड़ेकर']\nNumber of validation examples: 4096\nTest:sample: ['asparshyataa', 'अस्पृश्यता']\nNumber of Test examples: 4096\n27\nwith attention\nEpoch: 0 | Train Loss: 0.8616 | Val Loss: 0.5230 | Val Accuracy: 5.9082%\nEpoch: 1 | Train Loss: 0.4696 | Val Loss: 0.4193 | Val Accuracy: 19.2383%\nEpoch: 2 | Train Loss: 0.3846 | Val Loss: 0.3711 | Val Accuracy: 21.1182%\nEpoch: 3 | Train Loss: 0.3340 | Val Loss: 0.3477 | Val Accuracy: 23.1445%\nEpoch: 4 | Train Loss: 0.3138 | Val Loss: 0.3524 | Val Accuracy: 28.6133%\nTest Accuracy: 28.8574%\n","output_type":"stream"}]},{"cell_type":"code","source":" learning_rate, optimizer, epochs, max_len, num_layers_encoder, output_langs","metadata":{"execution":{"iopub.status.busy":"2024-05-14T14:33:43.749746Z","iopub.execute_input":"2024-05-14T14:33:43.750164Z","iopub.status.idle":"2024-05-14T14:33:43.756809Z","shell.execute_reply.started":"2024-05-14T14:33:43.750132Z","shell.execute_reply":"2024-05-14T14:33:43.755846Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(0.001, 'Sgd', 5, 30, 2, <__main__.Vocabulary at 0x78753fea2350>)"},"metadata":{}}]},{"cell_type":"code","source":"num_layers_encoder","metadata":{"execution":{"iopub.status.busy":"2024-05-14T14:33:48.237185Z","iopub.execute_input":"2024-05-14T14:33:48.238020Z","iopub.status.idle":"2024-05-14T14:33:48.243184Z","shell.execute_reply.started":"2024-05-14T14:33:48.237988Z","shell.execute_reply":"2024-05-14T14:33:48.242278Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}