{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5527494,"sourceType":"datasetVersion","datasetId":3186665},{"sourceId":8409812,"sourceType":"datasetVersion","datasetId":5005059}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-14T14:55:13.212953Z","iopub.execute_input":"2024-05-14T14:55:13.213570Z","iopub.status.idle":"2024-05-14T14:55:13.855431Z","shell.execute_reply.started":"2024-05-14T14:55:13.213536Z","shell.execute_reply":"2024-05-14T14:55:13.854532Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/aksharantar-sampled/aksharantar_sampled/brx/brx_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/brx/brx_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/brx/brx_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/tam/tam_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/tam/tam_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/tam/tam_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/mni/mni_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/mni/mni_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/mni/mni_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/urd/urd_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/urd/urd_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/urd/urd_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/kok/kok_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/kok/kok_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/kok/kok_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/mai/mai_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/mai/mai_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/mai/mai_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/guj/guj_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/guj/guj_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/guj/guj_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/ben/ben_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/ben/ben_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/ben/ben_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/tel/tel_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/tel/tel_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/tel/tel_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/kas/kas_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/kas/kas_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/kas/kas_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/kan/kan_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/kan/kan_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/kan/kan_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/mal/mal_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/mal/mal_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/mal/mal_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/pan/pan_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/pan/pan_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/pan/pan_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/asm/asm_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/asm/asm_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/asm/asm_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/sid/sid_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/sid/sid_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/sid/sid_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/mar/mar_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/mar/mar_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/mar/mar_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/san/san_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/san/san_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/san/san_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_valid.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/ori/ori_test.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/ori/ori_train.csv\n/kaggle/input/aksharantar-sampled/aksharantar_sampled/ori/ori_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/brx/brx_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/brx/brx_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/brx/brx_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/tam/tam_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/tam/tam_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/tam/tam_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/mni/mni_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/mni/mni_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/mni/mni_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/urd/urd_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/urd/urd_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/urd/urd_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/kok/kok_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/kok/kok_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/kok/kok_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/mai/mai_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/mai/mai_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/mai/mai_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/guj/guj_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/guj/guj_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/guj/guj_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/ben/ben_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/ben/ben_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/ben/ben_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/tel/tel_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/tel/tel_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/tel/tel_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/kas/kas_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/kas/kas_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/kas/kas_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/kan/kan_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/kan/kan_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/kan/kan_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/mal/mal_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/mal/mal_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/mal/mal_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/pan/pan_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/pan/pan_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/pan/pan_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/asm/asm_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/asm/asm_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/asm/asm_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/sid/sid_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/sid/sid_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/sid/sid_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/mar/mar_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/mar/mar_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/mar/mar_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/san/san_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/san/san_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/san/san_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/hin/hin_valid.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/hin/hin_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/hin/hin_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/ori/ori_test.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/ori/ori_train.csv\n/kaggle/input/aksharantar-sampled1/aksharantar_sampled-20240514T080825Z-001/aksharantar_sampled/aksharantar_sampled/ori/ori_valid.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb\nimport wandb\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T14:54:36.803592Z","iopub.execute_input":"2024-05-14T14:54:36.803967Z","iopub.status.idle":"2024-05-14T14:55:13.211080Z","shell.execute_reply.started":"2024-05-14T14:54:36.803935Z","shell.execute_reply":"2024-05-14T14:55:13.210126Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"#import necessary libraries\nimport os\nimport wandb\nimport torch\nimport torch.nn as nn\nimport random\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as Function\nimport argparse\n\n# Check if CUDA is available\nuse_cuda = torch.cuda.is_available()\n\n# Set the device type to CUDA if available, otherwise use CPU\nif use_cuda:\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n  \nF=Function\nStart_Symbol, End_Symbol, Unknown, Padding = 0, 1, 2, 3\n\nclass Vocabulary:\n    def __init__(self):\n        self.char2count = {}\n        self.char2index = {}\n        self.n_chars = 4\n        self.index2char = {0: \"<\", 1: \">\", 2: \"?\", 3: \".\"}\n\n\n    def addWord(self, word):\n        for char in word:\n            if char not in self.char2index:\n                self.char2index[char] = self.n_chars\n                self.index2char[self.n_chars] = char\n                self.char2count[char] = 1\n                self.n_chars += 1\n            else:\n                self.char2count[char] += 1\n\n            \ndef prepareData(dir):\n\n    input_lang = Vocabulary()\n    output_lang = Vocabulary()\n\n    data = pd.read_csv(dir, sep=\",\", names=[\"input\", \"target\"])\n\n    input_list = data[\"input\"].to_list()\n    target_list = data[\"target\"].to_list()\n\n    max_target_length = max([len(txt) for txt in data[\"target\"].to_list()])\n\n    pairs = []\n    for i in range(len(target_list)):\n        pairs.append([input_list[i], target_list[i]])\n\n    max_input_length = max([len(txt) for txt in data[\"input\"].to_list()])\n    for pair in pairs:\n        input_lang.addWord(pair[0])\n        output_lang.addWord(pair[1])\n\n    prepared_data = {\n        \"input_lang\": input_lang,\n        \"output_lang\": output_lang,\n        \"pairs\": pairs,\n        \"max_input_length\": max_input_length,\n        \"max_target_length\": max_target_length,\n    }\n\n    return prepared_data\n\ndef helpindex(lang, word):\n    l=[]\n    for i in range(len(word)):\n        if word[i] not in lang.char2index.keys():\n            l.append(Unknown)\n        else:\n            l.append(lang.char2index[word[i]])\n    return l\n\ndef helpTensor(lang, word, max_length):\n    indexes = helpindex(lang, word)\n    indexes.append(End_Symbol)\n    indexes.extend([Padding] * (max_length - len(indexes)))\n    result = torch.LongTensor(indexes)\n    if use_cuda==False:\n        return result\n    else:\n        return result.cuda()\n\ndef MakeTensor(input_lang, output_lang, pairs, max_length):\n    res = []\n    for pair in pairs:\n        input_variable = helpTensor(input_lang, pair[0], max_length)\n        target_variable = helpTensor(output_lang, pair[1], max_length)\n        res.append((input_variable, target_variable))\n    return res\n\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, embedding_size,hidden_size,num_layers_encoder,cell_type,drop_out,bi_directional):\n        super(EncoderRNN, self).__init__()\n\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.num_layers_encoder = num_layers_encoder\n        self.cell_type = cell_type\n        self.drop_out = drop_out\n        self.bi_directional = bi_directional\n\n        self.embedding = nn.Embedding(input_size, self.embedding_size)\n        self.dropout = nn.Dropout(self.drop_out)\n\n        cell_map = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}\n        self.cell_layer = cell_map[self.cell_type](\n            self.embedding_size,\n            self.hidden_size,\n            num_layers=self.num_layers_encoder,\n            dropout=self.drop_out,\n            bidirectional=self.bi_directional,\n        )\n\n    def forward(self, input, batch_size, hidden):\n        embedded = self.dropout(self.embedding(input).view(1, batch_size, -1))\n        output, hidden = self.cell_layer(embedded, hidden)\n        return output, hidden\n\n    def initHidden(self, batch_size, num_layers_enc):\n        res = torch.zeros(\n            num_layers_enc * 2 if self.bi_directional else num_layers_enc,\n            batch_size,\n            self.hidden_size,\n        )\n        if use_cuda== False:\n            return res\n        else:\n            return res.cuda()\n\n\n\nclass DecoderAttention(nn.Module):\n    def __init__(\n        self,\n        hidden_size,\n        embedding_size,\n        cell_type,\n        num_layers_decoder,\n        drop_out,\n        max_length_word,\n        output_size,\n    ):\n\n        super(DecoderAttention, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.embedding_size = embedding_size\n        self.cell_type = cell_type\n        self.num_layers_decoder = num_layers_decoder\n        self.drop_out = drop_out\n        self.max_length_word = max_length_word\n\n        self.embedding = nn.Embedding(output_size, embedding_dim=self.embedding_size)\n        self.attention_layer = nn.Linear(\n            self.embedding_size + self.hidden_size, self.max_length_word\n        )\n        self.attention_combine = nn.Linear(\n            self.embedding_size + self.hidden_size, self.embedding_size\n        )\n        self.dropout = nn.Dropout(self.drop_out)\n\n        self.cell_layer = None\n        cell_map = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}\n\n        if self.cell_type in cell_map:\n            self.cell_layer = cell_map[self.cell_type](\n                self.embedding_size,\n                self.hidden_size,\n                num_layers=self.num_layers_decoder,\n                dropout=self.drop_out,\n            )\n\n        self.out = nn.Linear(self.hidden_size, output_size)\n\n    def forward(self, input, batch_size, hidden, encoder_outputs):\n\n        embedded = self.embedding(input).view(1, batch_size, -1)\n\n        attention_weights = None\n        if self.cell_type == \"LSTM\":\n            attention_weights = Function.softmax(\n                self.attention_layer(torch.cat((embedded[0], hidden[0][0]), 1)), dim=1\n            )\n\n        else:\n            attention_weights = Function.softmax(\n                self.attention_layer(torch.cat((embedded[0], hidden[0]), 1)), dim=1\n            )\n\n        attention_applied = torch.bmm(\n            attention_weights.view(batch_size, 1, self.max_length_word),\n            encoder_outputs,\n        ).view(1, batch_size, -1)\n        output = torch.cat((embedded[0], attention_applied[0]), 1)\n        output = self.attention_combine(output).unsqueeze(0)\n        output = Function.relu(output)\n        # if self.cell_type=RNN\" :\n        output, hidden = self.cell_layer(output, hidden)\n        output = Function.log_softmax(self.out(output[0]), dim=1)\n\n        return output, hidden, attention_weights\n\n\n\ndef train_and_val_with_attn(\n    encoder,\n    decoder,\n    encoder_optimizer,\n    decoder_optimizer,\n    input_tensor,\n    target_tensor,\n    criterion,\n    batch_size,\n    cell_type,\n    num_layers_enc,\n    max_length,is_training,\n    teacher_forcing_ratio=0.5,\n):\n\n    encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n    if cell_type == \"LSTM\":\n        encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n        encoder_hidden = (encoder_hidden, encoder_cell_state)\n\n    if is_training:\n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n\n    encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hidden_size))\n    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n    loss = 0\n\n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(\n            input_tensor[ei], batch_size, encoder_hidden\n        )\n        encoder_outputs[ei] = encoder_output[0]\n\n    decoder_input = Variable(torch.LongTensor([Start_Symbol] * batch_size))\n    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n    decoder_hidden = encoder_hidden\n    if is_training:\n        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n        if use_teacher_forcing == False:\n            for di in range(target_length):\n                decoder_output, decoder_hidden, decoder_attention = decoder(\n                    decoder_input,\n                    batch_size,\n                    decoder_hidden,\n                    encoder_outputs.reshape(batch_size, max_length, encoder.hidden_size),\n                )\n                #2 for loop ko bhar dal de\n                topv, topi = decoder_output.data.topk(1)\n                decoder_input = torch.cat(tuple(topi))\n\n                decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n                loss += criterion(decoder_output, target_tensor[di])\n        else:\n            for di in range(target_length):\n                decoder_output, decoder_hidden, decoder_attention = decoder(\n                    decoder_input,\n                    batch_size,\n                    decoder_hidden,\n                    encoder_outputs.reshape(batch_size, max_length, encoder.hidden_size),\n                )\n                loss += criterion(decoder_output, target_tensor[di])\n                decoder_input = target_tensor[di]\n            \n\n        loss.backward()\n\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n    else :\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input,\n                batch_size,\n                decoder_hidden,\n                encoder_outputs.reshape(batch_size, max_length, encoder.hidden_size),\n            )\n            topv, topi = decoder_output.data.topk(1)\n            decoder_input = torch.cat(tuple(topi))\n\n            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n            loss += criterion(decoder_output, target_tensor[di])\n\n\n    return loss.item() / target_length\n\n\n# batch_size,num_layers_enc,cell_type,output_lang,criterion,\ndef accuracy_with_attention(\n    encoder,\n    decoder,\n    loader,\n    batch_size,\n    num_layers_enc,\n    cell_type,\n    output_lang,\n    criterion,\n    max_length,\n):\n\n    with torch.no_grad():\n\n        # batch_size = configuration[\"batch_size\"]\n        total = 0\n        correct = 0\n\n        for batch_x, batch_y in loader:\n\n            encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n            input_variable = Variable(batch_x.transpose(0, 1))\n            target_variable = Variable(batch_y.transpose(0, 1))\n\n            if cell_type == \"LSTM\":\n                encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n                encoder_hidden = (encoder_hidden, encoder_cell_state)\n\n            input_length = input_variable.size()[0]\n            target_length = target_variable.size()[0]\n\n            output = torch.LongTensor(target_length, batch_size)\n\n            encoder_outputs = Variable(\n                torch.zeros(max_length, batch_size, encoder.hidden_size)\n            )\n            encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n            for ei in range(input_length):\n                encoder_output, encoder_hidden = encoder(\n                    input_variable[ei], batch_size, encoder_hidden\n                )\n                encoder_outputs[ei] = encoder_output[0]\n\n            decoder_input = Variable(torch.LongTensor([Start_Symbol] * batch_size))\n            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n            decoder_hidden = encoder_hidden\n\n            for di in range(target_length):\n                decoder_output, decoder_hidden, decoder_attention = decoder(\n                    decoder_input,\n                    batch_size,\n                    decoder_hidden,\n                    encoder_outputs.reshape(\n                        batch_size, max_length, encoder.hidden_size\n                    ),\n                )\n                topv, topi = decoder_output.data.topk(1)\n                decoder_input = torch.cat(tuple(topi))\n                output[di] = torch.cat(tuple(topi))\n\n            output = output.transpose(0, 1)\n            for di in range(output.size()[0]):\n                ignore = [Start_Symbol, End_Symbol, Padding]\n                sent = [\n                    output_lang.index2char[letter.item()]\n                    for letter in output[di]\n                    if letter not in ignore\n                ]\n                y = [\n                    output_lang.index2char[letter.item()]\n                    for letter in batch_y[di]\n                    if letter not in ignore\n                ]\n                if sent == y:\n                    correct += 1\n                total += 1\n\n    return (correct / total) * 100\n\n\ndef cal_val_loss_with_attn(\n    encoder,\n    decoder,\n    input_tensor,\n    target_tensor,\n    batch_size,\n    criterion,\n    cell_type,\n    num_layers_enc,\n    max_length,\n):\n\n    with torch.no_grad():\n\n        encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n        if cell_type == \"LSTM\":\n            encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n            encoder_hidden = (encoder_hidden, encoder_cell_state)\n\n        input_length = input_tensor.size()[0]\n        target_length = target_tensor.size()[0]\n\n        encoder_outputs = Variable(\n            torch.zeros(max_length, batch_size, encoder.hidden_size)\n        )\n        encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n        loss = 0\n\n        for ei in range(input_length):\n            encoder_output, encoder_hidden = encoder(\n                input_tensor[ei], batch_size, encoder_hidden\n            )\n            encoder_outputs[ei] = encoder_output[0]\n\n        decoder_input = Variable(torch.LongTensor([Start_Symbol] * batch_size))\n        if use_cuda== True:\n            decoder_input = decoder_input.cuda()  \n        else :\n            decoder_input = decoder_input\n\n        decoder_hidden = encoder_hidden\n\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input,\n                batch_size,\n                decoder_hidden,\n                encoder_outputs.reshape(batch_size, max_length, encoder.hidden_size),\n            )\n            topv, topi = decoder_output.data.topk(1)\n            decoder_input = torch.cat(tuple(topi))\n\n            if use_cuda== True:\n                decoder_input = decoder_input.cuda()  \n            else :\n                decoder_input = decoder_input\n            loss += criterion(decoder_output, target_tensor[di])\n\n    return loss.item() / target_length\n\n\ndef Attention_seq2seq(\n    encoder,\n    decoder,\n    train_loader,\n    val_loader,\n    test_loader,\n    learning_rate,\n    optimizer,\n    epochs,\n    max_length_word,\n    attention,\n    num_layers_enc,\n    output_lang,\n    batch_size,\n    cell_type\n):\n    max_length = max_length_word - 1\n    batch_size = 1024\n    encoder_optimizer = (\n        optim.NAdam(encoder.parameters(), lr=learning_rate)\n        if optimizer == \"nadam\"\n        else optim.Adam(encoder.parameters(), lr=learning_rate)\n    )\n    decoder_optimizer = (\n        optim.NAdam(decoder.parameters(), lr=learning_rate)\n        if optimizer == \"nadam\"\n        else optim.Adam(decoder.parameters(), lr=learning_rate)\n    )\n    criterion = nn.NLLLoss()\n\n    for epoch in range(epochs):\n        train_loss_total, val_loss_total  =0, 0\n        \n        for batchx, batchy in train_loader:\n            batchx = Variable(batchx.transpose(0, 1))\n            batchy = Variable(batchy.transpose(0, 1))\n            loss = train_and_val_with_attn(\n                encoder,\n                decoder,\n                encoder_optimizer,\n                decoder_optimizer,\n                batchx,\n                batchy,\n                criterion,\n                batch_size,\n                cell_type,\n                num_layers_enc,\n                max_length + 1,\n                True, #is_training\n            )\n            train_loss_total += loss\n\n        train_loss_avg = train_loss_total / len(train_loader)\n        print(f\"Epoch: {epoch} | Train Loss: {train_loss_avg:.4f} | \", end=\"\")\n\n        for batchx, batchy in val_loader:\n            batchx = Variable(batchx.transpose(0, 1))\n            batchy = Variable(batchy.transpose(0, 1))\n            loss = train_and_val_with_attn(\n                encoder,\n                decoder,\n                encoder_optimizer,\n                decoder_optimizer,\n                batchx,\n                batchy,\n                criterion,\n                batch_size,\n                cell_type,\n                num_layers_enc,\n                max_length + 1,\n                False,#is_training=\n            )\n            val_loss_total += loss\n\n        val_loss_avg = val_loss_total / len(val_loader)\n        print(f\"Val Loss: {val_loss_avg:.4f} | \", end=\"\")\n        val_acc = accuracy_with_attention(\n            encoder,\n            decoder,\n            val_loader,\n            batch_size,\n            num_layers_enc,\n            cell_type,\n            output_lang,\n            criterion,\n            max_length + 1,\n        )\n        val_acc = val_acc / 100\n        print(f\"Val Accuracy: {val_acc:.4%}\")\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"training_loss\": train_loss_avg,\n            \"validation_accuracy\": val_acc,\n            \"validation_loss\": val_loss_avg\n        })\n        if epochs-1==epoch:\n            test_acc = accuracy_with_attention(\n            encoder,\n            decoder,\n            test_loader,\n            batch_size,\n            num_layers_enc,\n            cell_type,\n            output_lang,\n            criterion,\n            max_length + 1,\n        )\n            test_acc = test_acc / 100\n            print(f\"Test Accuracy: {test_acc:.4%}\")\n            wandb.log({\"test_accuracy\": test_acc})\n\ndef to_dict(input_lang,output_lang,pairs,max_len):\n    d = {\n        \"input_lang\": input_lang,\n        \"output_lang\": output_lang,\n        \"pairs\": pairs,\n        \"max_len\": max_len\n    }\n    return d\n\n\n   ","metadata":{"execution":{"iopub.status.busy":"2024-05-14T14:55:22.494452Z","iopub.execute_input":"2024-05-14T14:55:22.495040Z","iopub.status.idle":"2024-05-14T14:55:26.314509Z","shell.execute_reply.started":"2024-05-14T14:55:22.495009Z","shell.execute_reply":"2024-05-14T14:55:26.313667Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"sweep_configuration = {\n    \"method\": \"bayes\",\n    \"metric\": {\n        \"name\": \"validation_accuracy\",\n        \"goal\": \"maximize\"\n    },\n    \"parameters\": {\n        \"embed_size\": {\n            \"values\": [32,64,128]\n        },\n        \"hidden_size\": {\n            \"values\": [128, 256, 512]\n        },\n        \"cell_type\": {\n            \"values\": [\"GRU\", \"LSTM\", \"RNN\"]\n        },\n        \"num_layers\": {\n            \"values\": [1, 2, 3]\n        },\n        \"dropout\": {\n            \"values\": [0, 0.1, 0.2]\n        },\n        \"learning_rate\": {\n            \"values\": [0.0005, 0.001, 0.005]\n        },\n        \"optimizer\": {\n            \"values\": [\"Adam\",\"Nadam\"]\n        },\n        \"teacher_forcing_ratio\": {\n            \"values\": [0.5, 0.75, 0.25]\n        }\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-14T14:55:40.816729Z","iopub.execute_input":"2024-05-14T14:55:40.817415Z","iopub.status.idle":"2024-05-14T14:55:40.824366Z","shell.execute_reply.started":"2024-05-14T14:55:40.817381Z","shell.execute_reply":"2024-05-14T14:55:40.823273Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"count = 0\n\ndef train_sweep():\n    global count\n    count += 1\n    run = wandb.init()\n    config = wandb.config\n    run.name = \"Attn_Model_embed_size {}_hidden_size {}_cell_type {}_num_layers {} _dropout {} _learning_rate {} _optimizer {} _teacher_forcing_ratio {}\".format(config.embed_size, config.hidden_size, config.cell_type, config.num_layers, config.dropout, config.learning_rate, config.optimizer, config.teacher_forcing_ratio)\n    \n    optimizer = config.optimizer\n    learning_rate = config.learning_rate\n    teacher_forcing_ratio = config.teacher_forcing_ratio\n    hidden_size = config.hidden_size\n    input_lang = \"eng\"\n    target_lang = \"hin\"\n    cell_type = \"LSTM\"\n    num_layers_encoder = config.num_layers\n    num_layers_decoder = config.num_layers\n    drop_out = config.dropout\n    epochs = 60\n    embedding_size = config.embed_size\n    bi_directional = False\n    batch_size = 1024\n    \n    \n    \n    \n    \n    train_path = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_train.csv\"\n    validation_path = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_valid.csv\"\n    test_path = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/hin/hin_test.csv\"\n    train_prepared_data = prepareData(train_path)\n    input_langs, output_langs, pairs = (\n            train_prepared_data[\"input_lang\"],\n            train_prepared_data[\"output_lang\"],\n            train_prepared_data[\"pairs\"],\n        )\n\n    max_input_length, max_target_length = (\n            train_prepared_data[\"max_input_length\"],\n            train_prepared_data[\"max_target_length\"],\n        )\n\n        # validation\n    val_prepared_data = prepareData(validation_path)\n    val_pairs = val_prepared_data[\"pairs\"]\n        # Test\n    max_input_length_val, max_target_length_val = (\n    val_prepared_data[\"max_input_length\"],\n    val_prepared_data[\"max_target_length\"],\n        )\n    test_prepared_data = prepareData(validation_path)\n    test_pairs = test_prepared_data[\"pairs\"]\n\n    max_input_length_test, max_target_length_test = (\n    test_prepared_data[\"max_input_length\"],\n    test_prepared_data[\"max_target_length\"],\n        )\n    max_len_all = (\n            max(\n                max_input_length,\n                max_target_length,\n                max_input_length_val,\n                max_target_length_val,\n                max_input_length_test,\n                max_target_length_test,\n            )\n            + 1\n        )\n\n    max_len = max(max_input_length, max_target_length) + 3\n\n\n    pairs = MakeTensor(input_langs, output_langs, pairs, max_len)\n    val_pairs = MakeTensor(input_langs, output_langs, val_pairs, max_len)\n    test_pairs = MakeTensor(input_langs, output_langs, test_pairs, max_len)\n\n    train_loader = DataLoader(pairs, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_pairs, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_pairs, batch_size=batch_size, shuffle=True)\n    encoder1 = EncoderRNN(\n            input_langs.n_chars,\n            embedding_size,\n            hidden_size,\n            num_layers_encoder,\n            cell_type,\n            drop_out,\n            bi_directional,\n        )\n    attndecoder1 = DecoderAttention(\n            hidden_size,\n            embedding_size,\n            cell_type,\n            num_layers_decoder,\n            drop_out,\n            max_len,\n            output_langs.n_chars,\n        )\n    if use_cuda== True:\n            encoder1 = encoder1.cuda()\n            attndecoder1 = attndecoder1.cuda()\n    print(\"with attention\")\n    attention = True\n    Attention_seq2seq(\n            encoder1,\n            attndecoder1,\n            train_loader,\n            val_loader,\n            test_loader,\n            learning_rate,\n            optimizer,\n            epochs,\n            max_len,\n            attention,\n            num_layers_encoder,\n            output_langs,\n            batch_size,\n            cell_type\n        )","metadata":{"execution":{"iopub.status.busy":"2024-05-14T14:55:50.367234Z","iopub.execute_input":"2024-05-14T14:55:50.367969Z","iopub.status.idle":"2024-05-14T14:55:50.383386Z","shell.execute_reply.started":"2024-05-14T14:55:50.367936Z","shell.execute_reply":"2024-05-14T14:55:50.382316Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"wandb_id = wandb.sweep(sweep_configuration, project=\"DL_A-03_AttentionRNN\")\nwandb.agent(wandb_id, train_sweep, count=10)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T14:55:56.851168Z","iopub.execute_input":"2024-05-14T14:55:56.851785Z","iopub.status.idle":"2024-05-14T19:35:41.410289Z","shell.execute_reply.started":"2024-05-14T14:55:56.851752Z","shell.execute_reply":"2024-05-14T19:35:41.409261Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Create sweep with ID: pvpipb29\nSweep URL: https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 911n2p4h with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.75\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma22c019\u001b[0m (\u001b[33mbeliever12\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_145601-911n2p4h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/911n2p4h' target=\"_blank\">gentle-sweep-1</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/911n2p4h' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/911n2p4h</a>"},"metadata":{}},{"name":"stdout","text":"with attention\nEpoch: 0 | Train Loss: 1.8920 | Val Loss: 1.1099 | Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.2380 | Val Loss: 1.0867 | Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.2111 | Val Loss: 1.0270 | Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 1.1219 | Val Loss: 0.9395 | Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 1.0877 | Val Loss: 0.9335 | Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 1.0803 | Val Loss: 0.9249 | Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 1.0733 | Val Loss: 0.9184 | Val Accuracy: 0.0000%\nEpoch: 7 | Train Loss: 1.0633 | Val Loss: 0.9191 | Val Accuracy: 0.0000%\nEpoch: 8 | Train Loss: 1.0593 | Val Loss: 0.9161 | Val Accuracy: 0.0000%\nEpoch: 9 | Train Loss: 1.0469 | Val Loss: 0.9106 | Val Accuracy: 0.0000%\nEpoch: 10 | Train Loss: 1.0568 | Val Loss: 0.9853 | Val Accuracy: 0.0000%\nEpoch: 11 | Train Loss: 1.0356 | Val Loss: 0.9196 | Val Accuracy: 0.0000%\nEpoch: 12 | Train Loss: 1.0086 | Val Loss: 0.8887 | Val Accuracy: 0.0000%\nEpoch: 13 | Train Loss: 0.9741 | Val Loss: 0.9238 | Val Accuracy: 0.0000%\nEpoch: 14 | Train Loss: 0.9734 | Val Loss: 0.8688 | Val Accuracy: 0.0000%\nEpoch: 15 | Train Loss: 0.9713 | Val Loss: 0.8696 | Val Accuracy: 0.0000%\nEpoch: 16 | Train Loss: 0.9502 | Val Loss: 0.8512 | Val Accuracy: 0.0000%\nEpoch: 17 | Train Loss: 0.9365 | Val Loss: 0.8303 | Val Accuracy: 0.0244%\nEpoch: 18 | Train Loss: 0.9367 | Val Loss: 0.8084 | Val Accuracy: 0.0000%\nEpoch: 19 | Train Loss: 0.9095 | Val Loss: 0.7851 | Val Accuracy: 0.0488%\nEpoch: 20 | Train Loss: 0.8547 | Val Loss: 0.7622 | Val Accuracy: 0.0732%\nEpoch: 21 | Train Loss: 0.8474 | Val Loss: 0.7284 | Val Accuracy: 0.2686%\nEpoch: 22 | Train Loss: 0.8068 | Val Loss: 0.6931 | Val Accuracy: 0.2930%\nEpoch: 23 | Train Loss: 0.7676 | Val Loss: 0.7094 | Val Accuracy: 0.7324%\nEpoch: 24 | Train Loss: 0.7418 | Val Loss: 0.6446 | Val Accuracy: 1.1475%\nEpoch: 25 | Train Loss: 0.7094 | Val Loss: 0.5984 | Val Accuracy: 1.4648%\nEpoch: 26 | Train Loss: 0.6645 | Val Loss: 0.5741 | Val Accuracy: 2.6123%\nEpoch: 27 | Train Loss: 0.6448 | Val Loss: 0.5739 | Val Accuracy: 3.6621%\nEpoch: 28 | Train Loss: 0.6193 | Val Loss: 0.5437 | Val Accuracy: 4.7119%\nEpoch: 29 | Train Loss: 0.5881 | Val Loss: 0.5161 | Val Accuracy: 5.5908%\nEpoch: 30 | Train Loss: 0.5607 | Val Loss: 0.4978 | Val Accuracy: 6.4697%\nEpoch: 31 | Train Loss: 0.5288 | Val Loss: 0.4750 | Val Accuracy: 7.4707%\nEpoch: 32 | Train Loss: 0.5356 | Val Loss: 0.4689 | Val Accuracy: 9.5703%\nEpoch: 33 | Train Loss: 0.4850 | Val Loss: 0.4577 | Val Accuracy: 11.0352%\nEpoch: 34 | Train Loss: 0.4846 | Val Loss: 0.4585 | Val Accuracy: 12.7441%\nEpoch: 35 | Train Loss: 0.4458 | Val Loss: 0.4329 | Val Accuracy: 14.4043%\nEpoch: 36 | Train Loss: 0.4488 | Val Loss: 0.4255 | Val Accuracy: 14.9658%\nEpoch: 37 | Train Loss: 0.4499 | Val Loss: 0.4210 | Val Accuracy: 14.6484%\nEpoch: 38 | Train Loss: 0.4187 | Val Loss: 0.4173 | Val Accuracy: 17.0898%\nEpoch: 39 | Train Loss: 0.4282 | Val Loss: 0.4021 | Val Accuracy: 16.2842%\nEpoch: 40 | Train Loss: 0.4132 | Val Loss: 0.4042 | Val Accuracy: 17.2852%\nEpoch: 41 | Train Loss: 0.4290 | Val Loss: 0.3936 | Val Accuracy: 16.5283%\nEpoch: 42 | Train Loss: 0.4033 | Val Loss: 0.3847 | Val Accuracy: 19.3604%\nEpoch: 43 | Train Loss: 0.3755 | Val Loss: 0.3931 | Val Accuracy: 19.8242%\nEpoch: 44 | Train Loss: 0.3671 | Val Loss: 0.3758 | Val Accuracy: 20.4834%\nEpoch: 45 | Train Loss: 0.3556 | Val Loss: 0.3838 | Val Accuracy: 22.0703%\nEpoch: 46 | Train Loss: 0.3657 | Val Loss: 0.3743 | Val Accuracy: 20.4346%\nEpoch: 47 | Train Loss: 0.3709 | Val Loss: 0.3636 | Val Accuracy: 20.8008%\nEpoch: 48 | Train Loss: 0.3455 | Val Loss: 0.4034 | Val Accuracy: 21.2158%\nEpoch: 49 | Train Loss: 0.3669 | Val Loss: 0.3839 | Val Accuracy: 21.3867%\nEpoch: 50 | Train Loss: 0.3393 | Val Loss: 0.3718 | Val Accuracy: 22.7051%\nEpoch: 51 | Train Loss: 0.3369 | Val Loss: 0.3574 | Val Accuracy: 22.3633%\nEpoch: 52 | Train Loss: 0.3510 | Val Loss: 0.3612 | Val Accuracy: 21.5820%\nEpoch: 53 | Train Loss: 0.3562 | Val Loss: 0.3587 | Val Accuracy: 20.9961%\nEpoch: 54 | Train Loss: 0.3237 | Val Loss: 0.3575 | Val Accuracy: 24.3652%\nEpoch: 55 | Train Loss: 0.3257 | Val Loss: 0.3606 | Val Accuracy: 23.7793%\nEpoch: 56 | Train Loss: 0.3108 | Val Loss: 0.3615 | Val Accuracy: 24.1699%\nEpoch: 57 | Train Loss: 0.2927 | Val Loss: 0.3615 | Val Accuracy: 25.5859%\nEpoch: 58 | Train Loss: 0.2958 | Val Loss: 0.3577 | Val Accuracy: 25.0488%\nEpoch: 59 | Train Loss: 0.2868 | Val Loss: 0.3651 | Val Accuracy: 26.6113%\nTest Accuracy: 26.7822%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▅▅▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▄▄▅▅▅▆▆▆▇▆▇▇▇▇▇▇██</td></tr><tr><td>validation_loss</td><td>██▆▆▆▆▆▇▆▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.26782</td></tr><tr><td>training_loss</td><td>0.28677</td></tr><tr><td>validation_accuracy</td><td>0.26611</td></tr><tr><td>validation_loss</td><td>0.3651</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">gentle-sweep-1</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/911n2p4h' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/911n2p4h</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_145601-911n2p4h/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6s9qq9db with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Nadam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.25\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_151901-6s9qq9db</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/6s9qq9db' target=\"_blank\">revived-sweep-2</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/6s9qq9db' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/6s9qq9db</a>"},"metadata":{}},{"name":"stdout","text":"with attention\nEpoch: 0 | Train Loss: 1.8448 | Val Loss: 1.1046 | Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.2293 | Val Loss: 1.0415 | Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.1425 | Val Loss: 0.9570 | Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 1.1041 | Val Loss: 0.9409 | Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 1.0906 | Val Loss: 0.9365 | Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 1.0787 | Val Loss: 0.9254 | Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 1.0597 | Val Loss: 0.9235 | Val Accuracy: 0.0000%\nEpoch: 7 | Train Loss: 1.0469 | Val Loss: 0.9319 | Val Accuracy: 0.0000%\nEpoch: 8 | Train Loss: 1.0442 | Val Loss: 0.9570 | Val Accuracy: 0.0244%\nEpoch: 9 | Train Loss: 1.0357 | Val Loss: 0.9228 | Val Accuracy: 0.0000%\nEpoch: 10 | Train Loss: 1.0206 | Val Loss: 0.9264 | Val Accuracy: 0.0000%\nEpoch: 11 | Train Loss: 1.0061 | Val Loss: 0.9126 | Val Accuracy: 0.0000%\nEpoch: 12 | Train Loss: 1.0117 | Val Loss: 0.8931 | Val Accuracy: 0.0000%\nEpoch: 13 | Train Loss: 0.9857 | Val Loss: 0.8757 | Val Accuracy: 0.0244%\nEpoch: 14 | Train Loss: 0.9356 | Val Loss: 0.8562 | Val Accuracy: 0.0000%\nEpoch: 15 | Train Loss: 0.9237 | Val Loss: 0.8284 | Val Accuracy: 0.0732%\nEpoch: 16 | Train Loss: 0.8844 | Val Loss: 0.7991 | Val Accuracy: 0.0488%\nEpoch: 17 | Train Loss: 0.8928 | Val Loss: 0.7609 | Val Accuracy: 0.1221%\nEpoch: 18 | Train Loss: 0.8298 | Val Loss: 0.7558 | Val Accuracy: 0.1709%\nEpoch: 19 | Train Loss: 0.8098 | Val Loss: 0.6952 | Val Accuracy: 0.2197%\nEpoch: 20 | Train Loss: 0.7670 | Val Loss: 0.6659 | Val Accuracy: 0.8789%\nEpoch: 21 | Train Loss: 0.7232 | Val Loss: 0.6193 | Val Accuracy: 1.2451%\nEpoch: 22 | Train Loss: 0.6874 | Val Loss: 0.5856 | Val Accuracy: 2.1240%\nEpoch: 23 | Train Loss: 0.6427 | Val Loss: 0.5598 | Val Accuracy: 3.1982%\nEpoch: 24 | Train Loss: 0.6103 | Val Loss: 0.5400 | Val Accuracy: 4.2236%\nEpoch: 25 | Train Loss: 0.5852 | Val Loss: 0.5287 | Val Accuracy: 6.0059%\nEpoch: 26 | Train Loss: 0.5527 | Val Loss: 0.5129 | Val Accuracy: 6.4453%\nEpoch: 27 | Train Loss: 0.5450 | Val Loss: 0.4744 | Val Accuracy: 7.5684%\nEpoch: 28 | Train Loss: 0.5072 | Val Loss: 0.4725 | Val Accuracy: 9.0576%\nEpoch: 29 | Train Loss: 0.5084 | Val Loss: 0.4523 | Val Accuracy: 9.4482%\nEpoch: 30 | Train Loss: 0.4663 | Val Loss: 0.4501 | Val Accuracy: 11.2793%\nEpoch: 31 | Train Loss: 0.4630 | Val Loss: 0.4355 | Val Accuracy: 13.2324%\nEpoch: 32 | Train Loss: 0.4358 | Val Loss: 0.4405 | Val Accuracy: 12.7441%\nEpoch: 33 | Train Loss: 0.4396 | Val Loss: 0.4179 | Val Accuracy: 13.9893%\nEpoch: 34 | Train Loss: 0.4364 | Val Loss: 0.4026 | Val Accuracy: 14.0381%\nEpoch: 35 | Train Loss: 0.4114 | Val Loss: 0.4216 | Val Accuracy: 15.6494%\nEpoch: 36 | Train Loss: 0.3852 | Val Loss: 0.4125 | Val Accuracy: 18.1396%\nEpoch: 37 | Train Loss: 0.4191 | Val Loss: 0.3967 | Val Accuracy: 16.8701%\nEpoch: 38 | Train Loss: 0.3830 | Val Loss: 0.3888 | Val Accuracy: 18.6279%\nEpoch: 39 | Train Loss: 0.3787 | Val Loss: 0.3941 | Val Accuracy: 18.7744%\nEpoch: 40 | Train Loss: 0.3575 | Val Loss: 0.3999 | Val Accuracy: 18.8477%\nEpoch: 41 | Train Loss: 0.3727 | Val Loss: 0.3726 | Val Accuracy: 20.6787%\nEpoch: 42 | Train Loss: 0.3944 | Val Loss: 0.3652 | Val Accuracy: 19.2627%\nEpoch: 43 | Train Loss: 0.3721 | Val Loss: 0.3680 | Val Accuracy: 21.0938%\nEpoch: 44 | Train Loss: 0.3399 | Val Loss: 0.3800 | Val Accuracy: 22.4121%\nEpoch: 45 | Train Loss: 0.3609 | Val Loss: 0.3562 | Val Accuracy: 20.5811%\nEpoch: 46 | Train Loss: 0.3464 | Val Loss: 0.3547 | Val Accuracy: 21.3867%\nEpoch: 47 | Train Loss: 0.3198 | Val Loss: 0.3714 | Val Accuracy: 24.7559%\nEpoch: 48 | Train Loss: 0.3431 | Val Loss: 0.3546 | Val Accuracy: 21.0693%\nEpoch: 49 | Train Loss: 0.3283 | Val Loss: 0.3508 | Val Accuracy: 24.9023%\nEpoch: 50 | Train Loss: 0.3131 | Val Loss: 0.3552 | Val Accuracy: 24.1943%\nEpoch: 51 | Train Loss: 0.2921 | Val Loss: 0.3740 | Val Accuracy: 25.4395%\nEpoch: 52 | Train Loss: 0.3142 | Val Loss: 0.3519 | Val Accuracy: 25.9277%\nEpoch: 53 | Train Loss: 0.3008 | Val Loss: 0.3519 | Val Accuracy: 27.3193%\nEpoch: 54 | Train Loss: 0.2880 | Val Loss: 0.3510 | Val Accuracy: 26.3916%\nEpoch: 55 | Train Loss: 0.3013 | Val Loss: 0.3469 | Val Accuracy: 25.0488%\nEpoch: 56 | Train Loss: 0.3077 | Val Loss: 0.3417 | Val Accuracy: 26.3916%\nEpoch: 57 | Train Loss: 0.3095 | Val Loss: 0.3472 | Val Accuracy: 24.0967%\nEpoch: 58 | Train Loss: 0.3078 | Val Loss: 0.3327 | Val Accuracy: 25.9766%\nEpoch: 59 | Train Loss: 0.2843 | Val Loss: 0.3396 | Val Accuracy: 27.4414%\nTest Accuracy: 27.7100%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▃▄▄▅▅▆▅▆▆▆▆▆▆▆▇▇██▇▇█</td></tr><tr><td>validation_loss</td><td>█▇▇▆▆▆▆▆▆▆▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.2771</td></tr><tr><td>training_loss</td><td>0.28431</td></tr><tr><td>validation_accuracy</td><td>0.27441</td></tr><tr><td>validation_loss</td><td>0.33964</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">revived-sweep-2</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/6s9qq9db' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/6s9qq9db</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_151901-6s9qq9db/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ghi09al0 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.75\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_154215-ghi09al0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/ghi09al0' target=\"_blank\">floral-sweep-3</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/ghi09al0' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/ghi09al0</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"with attention\nEpoch: 0 | Train Loss: 2.2979 | Val Loss: 1.1615 | Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.2564 | Val Loss: 1.1085 | Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.2130 | Val Loss: 1.0808 | Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 1.1328 | Val Loss: 0.9813 | Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 1.0976 | Val Loss: 0.9638 | Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 1.0602 | Val Loss: 0.9512 | Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 1.0536 | Val Loss: 0.9415 | Val Accuracy: 0.0000%\nEpoch: 7 | Train Loss: 1.0364 | Val Loss: 0.9461 | Val Accuracy: 0.0000%\nEpoch: 8 | Train Loss: 1.0329 | Val Loss: 0.9445 | Val Accuracy: 0.0000%\nEpoch: 9 | Train Loss: 1.0203 | Val Loss: 0.9302 | Val Accuracy: 0.0000%\nEpoch: 10 | Train Loss: 1.0188 | Val Loss: 0.9374 | Val Accuracy: 0.0000%\nEpoch: 11 | Train Loss: 1.0189 | Val Loss: 0.9316 | Val Accuracy: 0.0000%\nEpoch: 12 | Train Loss: 0.9967 | Val Loss: 0.9224 | Val Accuracy: 0.0000%\nEpoch: 13 | Train Loss: 1.0096 | Val Loss: 0.9166 | Val Accuracy: 0.0000%\nEpoch: 14 | Train Loss: 0.9888 | Val Loss: 0.9255 | Val Accuracy: 0.0000%\nEpoch: 15 | Train Loss: 0.9946 | Val Loss: 0.8950 | Val Accuracy: 0.0244%\nEpoch: 16 | Train Loss: 0.9768 | Val Loss: 0.8937 | Val Accuracy: 0.0244%\nEpoch: 17 | Train Loss: 0.9698 | Val Loss: 0.8847 | Val Accuracy: 0.0244%\nEpoch: 18 | Train Loss: 0.9665 | Val Loss: 0.8803 | Val Accuracy: 0.0000%\nEpoch: 19 | Train Loss: 0.9753 | Val Loss: 0.8631 | Val Accuracy: 0.0244%\nEpoch: 20 | Train Loss: 0.9715 | Val Loss: 0.8569 | Val Accuracy: 0.0244%\nEpoch: 21 | Train Loss: 0.9285 | Val Loss: 0.8605 | Val Accuracy: 0.0244%\nEpoch: 22 | Train Loss: 0.9190 | Val Loss: 0.8590 | Val Accuracy: 0.0000%\nEpoch: 23 | Train Loss: 0.9208 | Val Loss: 0.8477 | Val Accuracy: 0.0732%\nEpoch: 24 | Train Loss: 0.9152 | Val Loss: 0.8517 | Val Accuracy: 0.0732%\nEpoch: 25 | Train Loss: 0.8935 | Val Loss: 0.8321 | Val Accuracy: 0.0488%\nEpoch: 26 | Train Loss: 0.8948 | Val Loss: 0.8114 | Val Accuracy: 0.0244%\nEpoch: 27 | Train Loss: 0.9039 | Val Loss: 0.8034 | Val Accuracy: 0.1221%\nEpoch: 28 | Train Loss: 0.8704 | Val Loss: 0.8314 | Val Accuracy: 0.1953%\nEpoch: 29 | Train Loss: 0.8563 | Val Loss: 0.8103 | Val Accuracy: 0.1465%\nEpoch: 30 | Train Loss: 0.8498 | Val Loss: 0.7785 | Val Accuracy: 0.0977%\nEpoch: 31 | Train Loss: 0.8420 | Val Loss: 0.7640 | Val Accuracy: 0.1953%\nEpoch: 32 | Train Loss: 0.8226 | Val Loss: 0.7658 | Val Accuracy: 0.2441%\nEpoch: 33 | Train Loss: 0.8267 | Val Loss: 0.7523 | Val Accuracy: 0.2686%\nEpoch: 34 | Train Loss: 0.7759 | Val Loss: 0.7595 | Val Accuracy: 0.3906%\nEpoch: 35 | Train Loss: 0.7891 | Val Loss: 0.7720 | Val Accuracy: 0.7080%\nEpoch: 36 | Train Loss: 0.7855 | Val Loss: 0.7225 | Val Accuracy: 0.7812%\nEpoch: 37 | Train Loss: 0.7908 | Val Loss: 0.7175 | Val Accuracy: 0.6104%\nEpoch: 38 | Train Loss: 0.7576 | Val Loss: 0.7057 | Val Accuracy: 1.0254%\nEpoch: 39 | Train Loss: 0.7473 | Val Loss: 0.7055 | Val Accuracy: 1.1230%\nEpoch: 40 | Train Loss: 0.7453 | Val Loss: 0.6999 | Val Accuracy: 1.0254%\nEpoch: 41 | Train Loss: 0.7018 | Val Loss: 0.6965 | Val Accuracy: 1.4648%\nEpoch: 42 | Train Loss: 0.7609 | Val Loss: 0.6635 | Val Accuracy: 0.9521%\nEpoch: 43 | Train Loss: 0.7573 | Val Loss: 0.6562 | Val Accuracy: 1.1963%\nEpoch: 44 | Train Loss: 0.7208 | Val Loss: 0.6535 | Val Accuracy: 1.6357%\nEpoch: 45 | Train Loss: 0.7100 | Val Loss: 0.6419 | Val Accuracy: 2.1484%\nEpoch: 46 | Train Loss: 0.6807 | Val Loss: 0.6433 | Val Accuracy: 2.2461%\nEpoch: 47 | Train Loss: 0.6791 | Val Loss: 0.6331 | Val Accuracy: 2.7588%\nEpoch: 48 | Train Loss: 0.6969 | Val Loss: 0.6245 | Val Accuracy: 2.4414%\nEpoch: 49 | Train Loss: 0.6307 | Val Loss: 0.6383 | Val Accuracy: 3.6865%\nEpoch: 50 | Train Loss: 0.6562 | Val Loss: 0.6167 | Val Accuracy: 2.7832%\nEpoch: 51 | Train Loss: 0.6811 | Val Loss: 0.6072 | Val Accuracy: 3.7598%\nEpoch: 52 | Train Loss: 0.6842 | Val Loss: 0.6038 | Val Accuracy: 4.1992%\nEpoch: 53 | Train Loss: 0.6278 | Val Loss: 0.6062 | Val Accuracy: 4.1992%\nEpoch: 54 | Train Loss: 0.6437 | Val Loss: 0.5883 | Val Accuracy: 4.7607%\nEpoch: 55 | Train Loss: 0.6161 | Val Loss: 0.5822 | Val Accuracy: 4.5410%\nEpoch: 56 | Train Loss: 0.6010 | Val Loss: 0.6060 | Val Accuracy: 5.2002%\nEpoch: 57 | Train Loss: 0.6098 | Val Loss: 0.5753 | Val Accuracy: 4.8096%\nEpoch: 58 | Train Loss: 0.6276 | Val Loss: 0.5595 | Val Accuracy: 4.9805%\nEpoch: 59 | Train Loss: 0.6168 | Val Loss: 0.5568 | Val Accuracy: 5.2246%\nTest Accuracy: 5.2490%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▄▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▃▂▂▃▄▄▄▆▆▇▇▇▇█</td></tr><tr><td>validation_loss</td><td>█▇▆▆▅▆▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.05249</td></tr><tr><td>training_loss</td><td>0.61677</td></tr><tr><td>validation_accuracy</td><td>0.05225</td></tr><tr><td>validation_loss</td><td>0.55681</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">floral-sweep-3</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/ghi09al0' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/ghi09al0</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_154215-ghi09al0/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0wutm11c with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Nadam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.25\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_160354-0wutm11c</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/0wutm11c' target=\"_blank\">tough-sweep-4</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/0wutm11c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/0wutm11c</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"with attention\nEpoch: 0 | Train Loss: 1.7433 | Val Loss: 1.0974 | Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.2177 | Val Loss: 1.0819 | Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.1852 | Val Loss: 1.0690 | Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 1.1679 | Val Loss: 1.1283 | Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 1.1693 | Val Loss: 1.0964 | Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 1.1284 | Val Loss: 1.0172 | Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 1.0721 | Val Loss: 1.0388 | Val Accuracy: 0.0000%\nEpoch: 7 | Train Loss: 1.0309 | Val Loss: 0.9356 | Val Accuracy: 0.0000%\nEpoch: 8 | Train Loss: 1.0334 | Val Loss: 0.9424 | Val Accuracy: 0.0000%\nEpoch: 9 | Train Loss: 1.0111 | Val Loss: 0.9111 | Val Accuracy: 0.0000%\nEpoch: 10 | Train Loss: 0.9826 | Val Loss: 0.9277 | Val Accuracy: 0.0244%\nEpoch: 11 | Train Loss: 0.9854 | Val Loss: 0.8788 | Val Accuracy: 0.0244%\nEpoch: 12 | Train Loss: 0.9754 | Val Loss: 0.8688 | Val Accuracy: 0.0000%\nEpoch: 13 | Train Loss: 0.9392 | Val Loss: 0.8758 | Val Accuracy: 0.0244%\nEpoch: 14 | Train Loss: 0.9293 | Val Loss: 0.8182 | Val Accuracy: 0.0977%\nEpoch: 15 | Train Loss: 0.8836 | Val Loss: 0.8221 | Val Accuracy: 0.0977%\nEpoch: 16 | Train Loss: 0.8544 | Val Loss: 0.7617 | Val Accuracy: 0.3418%\nEpoch: 17 | Train Loss: 0.8123 | Val Loss: 0.7192 | Val Accuracy: 0.4150%\nEpoch: 18 | Train Loss: 0.7820 | Val Loss: 0.6839 | Val Accuracy: 0.7568%\nEpoch: 19 | Train Loss: 0.7517 | Val Loss: 0.6469 | Val Accuracy: 1.6602%\nEpoch: 20 | Train Loss: 0.6750 | Val Loss: 0.6271 | Val Accuracy: 2.4902%\nEpoch: 21 | Train Loss: 0.6316 | Val Loss: 0.5909 | Val Accuracy: 3.2471%\nEpoch: 22 | Train Loss: 0.6026 | Val Loss: 0.5853 | Val Accuracy: 5.0049%\nEpoch: 23 | Train Loss: 0.5816 | Val Loss: 0.5367 | Val Accuracy: 4.9805%\nEpoch: 24 | Train Loss: 0.5455 | Val Loss: 0.5149 | Val Accuracy: 7.2266%\nEpoch: 25 | Train Loss: 0.5139 | Val Loss: 0.5290 | Val Accuracy: 9.4482%\nEpoch: 26 | Train Loss: 0.5141 | Val Loss: 0.4909 | Val Accuracy: 9.5459%\nEpoch: 27 | Train Loss: 0.4952 | Val Loss: 0.4748 | Val Accuracy: 10.6201%\nEpoch: 28 | Train Loss: 0.4882 | Val Loss: 0.4666 | Val Accuracy: 11.6211%\nEpoch: 29 | Train Loss: 0.4718 | Val Loss: 0.4415 | Val Accuracy: 12.5732%\nEpoch: 30 | Train Loss: 0.4555 | Val Loss: 0.4372 | Val Accuracy: 13.8672%\nEpoch: 31 | Train Loss: 0.4325 | Val Loss: 0.4457 | Val Accuracy: 16.0400%\nEpoch: 32 | Train Loss: 0.4328 | Val Loss: 0.4287 | Val Accuracy: 15.8691%\nEpoch: 33 | Train Loss: 0.4353 | Val Loss: 0.4157 | Val Accuracy: 14.4043%\nEpoch: 34 | Train Loss: 0.4105 | Val Loss: 0.4094 | Val Accuracy: 16.7480%\nEpoch: 35 | Train Loss: 0.3713 | Val Loss: 0.4229 | Val Accuracy: 20.9961%\nEpoch: 36 | Train Loss: 0.3689 | Val Loss: 0.4142 | Val Accuracy: 19.7021%\nEpoch: 37 | Train Loss: 0.3911 | Val Loss: 0.3897 | Val Accuracy: 20.6543%\nEpoch: 38 | Train Loss: 0.4035 | Val Loss: 0.3733 | Val Accuracy: 17.4072%\nEpoch: 39 | Train Loss: 0.3823 | Val Loss: 0.3768 | Val Accuracy: 21.6309%\nEpoch: 40 | Train Loss: 0.3761 | Val Loss: 0.3836 | Val Accuracy: 18.9453%\nEpoch: 41 | Train Loss: 0.3580 | Val Loss: 0.3860 | Val Accuracy: 21.0693%\nEpoch: 42 | Train Loss: 0.3484 | Val Loss: 0.3707 | Val Accuracy: 23.0469%\nEpoch: 43 | Train Loss: 0.3361 | Val Loss: 0.3716 | Val Accuracy: 24.5850%\nEpoch: 44 | Train Loss: 0.3468 | Val Loss: 0.3573 | Val Accuracy: 24.0234%\nEpoch: 45 | Train Loss: 0.3112 | Val Loss: 0.3755 | Val Accuracy: 24.9023%\nEpoch: 46 | Train Loss: 0.2975 | Val Loss: 0.3821 | Val Accuracy: 25.6104%\nEpoch: 47 | Train Loss: 0.3275 | Val Loss: 0.3562 | Val Accuracy: 24.4873%\nEpoch: 48 | Train Loss: 0.3198 | Val Loss: 0.3574 | Val Accuracy: 26.4893%\nEpoch: 49 | Train Loss: 0.3127 | Val Loss: 0.3538 | Val Accuracy: 27.0264%\nEpoch: 50 | Train Loss: 0.3058 | Val Loss: 0.3513 | Val Accuracy: 26.7334%\nEpoch: 51 | Train Loss: 0.3129 | Val Loss: 0.3456 | Val Accuracy: 27.0508%\nEpoch: 52 | Train Loss: 0.2910 | Val Loss: 0.3564 | Val Accuracy: 28.9307%\nEpoch: 53 | Train Loss: 0.2786 | Val Loss: 0.3819 | Val Accuracy: 26.3428%\nEpoch: 54 | Train Loss: 0.2915 | Val Loss: 0.3558 | Val Accuracy: 26.6357%\nEpoch: 55 | Train Loss: 0.2718 | Val Loss: 0.3650 | Val Accuracy: 28.3203%\nEpoch: 56 | Train Loss: 0.2794 | Val Loss: 0.3395 | Val Accuracy: 28.7842%\nEpoch: 57 | Train Loss: 0.2572 | Val Loss: 0.3461 | Val Accuracy: 28.1250%\nEpoch: 58 | Train Loss: 0.2940 | Val Loss: 0.3316 | Val Accuracy: 27.4902%\nEpoch: 59 | Train Loss: 0.2730 | Val Loss: 0.3338 | Val Accuracy: 28.4668%\nTest Accuracy: 28.5645%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.007 MB uploaded\\r'), FloatProgress(value=0.18696998250100955, max=1.…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▄▄▄▅▄▅▆▆▆▆▇▇▇▇▇███▇███</td></tr><tr><td>validation_loss</td><td>████▇▆▆▆▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.28564</td></tr><tr><td>training_loss</td><td>0.27295</td></tr><tr><td>validation_accuracy</td><td>0.28467</td></tr><tr><td>validation_loss</td><td>0.33385</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">tough-sweep-4</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/0wutm11c' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/0wutm11c</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_160354-0wutm11c/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3fte60tb with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Nadam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.25\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_162643-3fte60tb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/3fte60tb' target=\"_blank\">dashing-sweep-5</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/3fte60tb' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/3fte60tb</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"with attention\nEpoch: 0 | Train Loss: 1.7437 | Val Loss: 1.0853 | Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.2108 | Val Loss: 1.0824 | Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.1828 | Val Loss: 1.0675 | Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 1.1801 | Val Loss: 1.0984 | Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 1.1669 | Val Loss: 1.0920 | Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 1.1343 | Val Loss: 1.0536 | Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 1.0976 | Val Loss: 0.9952 | Val Accuracy: 0.0000%\nEpoch: 7 | Train Loss: 1.0442 | Val Loss: 0.9410 | Val Accuracy: 0.0000%\nEpoch: 8 | Train Loss: 1.0206 | Val Loss: 0.9370 | Val Accuracy: 0.0000%\nEpoch: 9 | Train Loss: 1.0123 | Val Loss: 0.9230 | Val Accuracy: 0.0000%\nEpoch: 10 | Train Loss: 0.9930 | Val Loss: 0.9177 | Val Accuracy: 0.0000%\nEpoch: 11 | Train Loss: 0.9845 | Val Loss: 0.9120 | Val Accuracy: 0.0000%\nEpoch: 12 | Train Loss: 0.9734 | Val Loss: 0.8724 | Val Accuracy: 0.0000%\nEpoch: 13 | Train Loss: 0.9339 | Val Loss: 0.8425 | Val Accuracy: 0.0000%\nEpoch: 14 | Train Loss: 0.9068 | Val Loss: 0.8042 | Val Accuracy: 0.0732%\nEpoch: 15 | Train Loss: 0.9031 | Val Loss: 0.7591 | Val Accuracy: 0.1221%\nEpoch: 16 | Train Loss: 0.8396 | Val Loss: 0.7479 | Val Accuracy: 0.4395%\nEpoch: 17 | Train Loss: 0.7847 | Val Loss: 0.7135 | Val Accuracy: 0.8057%\nEpoch: 18 | Train Loss: 0.7242 | Val Loss: 0.6607 | Val Accuracy: 1.3672%\nEpoch: 19 | Train Loss: 0.7050 | Val Loss: 0.6084 | Val Accuracy: 2.2705%\nEpoch: 20 | Train Loss: 0.6526 | Val Loss: 0.5857 | Val Accuracy: 3.5645%\nEpoch: 21 | Train Loss: 0.6310 | Val Loss: 0.5469 | Val Accuracy: 3.1982%\nEpoch: 22 | Train Loss: 0.5984 | Val Loss: 0.5295 | Val Accuracy: 5.6152%\nEpoch: 23 | Train Loss: 0.5436 | Val Loss: 0.5331 | Val Accuracy: 8.2520%\nEpoch: 24 | Train Loss: 0.5303 | Val Loss: 0.4934 | Val Accuracy: 8.7158%\nEpoch: 25 | Train Loss: 0.4940 | Val Loss: 0.4830 | Val Accuracy: 9.7900%\nEpoch: 26 | Train Loss: 0.4874 | Val Loss: 0.4661 | Val Accuracy: 11.4746%\nEpoch: 27 | Train Loss: 0.4862 | Val Loss: 0.4486 | Val Accuracy: 11.6211%\nEpoch: 28 | Train Loss: 0.4694 | Val Loss: 0.4379 | Val Accuracy: 12.3047%\nEpoch: 29 | Train Loss: 0.4395 | Val Loss: 0.4464 | Val Accuracy: 13.7695%\nEpoch: 30 | Train Loss: 0.4309 | Val Loss: 0.4307 | Val Accuracy: 16.0889%\nEpoch: 31 | Train Loss: 0.4175 | Val Loss: 0.4201 | Val Accuracy: 16.2842%\nEpoch: 32 | Train Loss: 0.4034 | Val Loss: 0.4339 | Val Accuracy: 15.7227%\nEpoch: 33 | Train Loss: 0.3967 | Val Loss: 0.4063 | Val Accuracy: 16.3574%\nEpoch: 34 | Train Loss: 0.3718 | Val Loss: 0.4152 | Val Accuracy: 19.5068%\nEpoch: 35 | Train Loss: 0.3849 | Val Loss: 0.3984 | Val Accuracy: 19.8486%\nEpoch: 36 | Train Loss: 0.4015 | Val Loss: 0.3825 | Val Accuracy: 19.1895%\nEpoch: 37 | Train Loss: 0.3431 | Val Loss: 0.3939 | Val Accuracy: 21.7041%\nEpoch: 38 | Train Loss: 0.3660 | Val Loss: 0.3949 | Val Accuracy: 20.4590%\nEpoch: 39 | Train Loss: 0.3647 | Val Loss: 0.3793 | Val Accuracy: 22.2656%\nEpoch: 40 | Train Loss: 0.3569 | Val Loss: 0.3772 | Val Accuracy: 21.9727%\nEpoch: 41 | Train Loss: 0.3471 | Val Loss: 0.3693 | Val Accuracy: 23.1934%\nEpoch: 42 | Train Loss: 0.3391 | Val Loss: 0.3642 | Val Accuracy: 23.7061%\nEpoch: 43 | Train Loss: 0.3253 | Val Loss: 0.3772 | Val Accuracy: 23.8525%\nEpoch: 44 | Train Loss: 0.3286 | Val Loss: 0.3676 | Val Accuracy: 22.9492%\nEpoch: 45 | Train Loss: 0.2970 | Val Loss: 0.3730 | Val Accuracy: 25.3662%\nEpoch: 46 | Train Loss: 0.3040 | Val Loss: 0.3688 | Val Accuracy: 26.6846%\nEpoch: 47 | Train Loss: 0.3133 | Val Loss: 0.3560 | Val Accuracy: 24.7314%\nEpoch: 48 | Train Loss: 0.3337 | Val Loss: 0.3480 | Val Accuracy: 26.0010%\nEpoch: 49 | Train Loss: 0.2970 | Val Loss: 0.3545 | Val Accuracy: 26.6357%\nEpoch: 50 | Train Loss: 0.2765 | Val Loss: 0.3551 | Val Accuracy: 28.1738%\nEpoch: 51 | Train Loss: 0.2818 | Val Loss: 0.3671 | Val Accuracy: 27.9053%\nEpoch: 52 | Train Loss: 0.3083 | Val Loss: 0.3492 | Val Accuracy: 26.0498%\nEpoch: 53 | Train Loss: 0.2842 | Val Loss: 0.3554 | Val Accuracy: 27.1973%\nEpoch: 54 | Train Loss: 0.2805 | Val Loss: 0.3578 | Val Accuracy: 25.6104%\nEpoch: 55 | Train Loss: 0.2835 | Val Loss: 0.3432 | Val Accuracy: 28.3203%\nEpoch: 56 | Train Loss: 0.2679 | Val Loss: 0.3479 | Val Accuracy: 28.6133%\nEpoch: 57 | Train Loss: 0.2597 | Val Loss: 0.3452 | Val Accuracy: 29.0771%\nEpoch: 58 | Train Loss: 0.2680 | Val Loss: 0.3406 | Val Accuracy: 27.9541%\nEpoch: 59 | Train Loss: 0.2701 | Val Loss: 0.3452 | Val Accuracy: 30.3223%\nTest Accuracy: 29.7852%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▃▃▄▄▅▅▅▆▅▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>validation_loss</td><td>████▇▇▆▆▆▆▅▅▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.29785</td></tr><tr><td>training_loss</td><td>0.27008</td></tr><tr><td>validation_accuracy</td><td>0.30322</td></tr><tr><td>validation_loss</td><td>0.34522</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dashing-sweep-5</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/3fte60tb' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/3fte60tb</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_162643-3fte60tb/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wvcq42kz with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Nadam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.25\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_164933-wvcq42kz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/wvcq42kz' target=\"_blank\">curious-sweep-6</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/wvcq42kz' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/wvcq42kz</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"with attention\nEpoch: 0 | Train Loss: 1.6825 | Val Loss: 1.0862 | Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.1954 | Val Loss: 1.0964 | Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.1674 | Val Loss: 1.0267 | Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 1.1210 | Val Loss: 0.9970 | Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 1.0639 | Val Loss: 0.9338 | Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 1.0274 | Val Loss: 0.9398 | Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 0.9975 | Val Loss: 0.9298 | Val Accuracy: 0.0000%\nEpoch: 7 | Train Loss: 0.9806 | Val Loss: 0.9040 | Val Accuracy: 0.0244%\nEpoch: 8 | Train Loss: 0.9779 | Val Loss: 0.8526 | Val Accuracy: 0.0732%\nEpoch: 9 | Train Loss: 0.8797 | Val Loss: 0.8408 | Val Accuracy: 0.2197%\nEpoch: 10 | Train Loss: 0.8749 | Val Loss: 0.7929 | Val Accuracy: 0.2930%\nEpoch: 11 | Train Loss: 0.8378 | Val Loss: 0.7624 | Val Accuracy: 0.2441%\nEpoch: 12 | Train Loss: 0.7896 | Val Loss: 0.7243 | Val Accuracy: 0.8789%\nEpoch: 13 | Train Loss: 0.7277 | Val Loss: 0.6536 | Val Accuracy: 1.8066%\nEpoch: 14 | Train Loss: 0.6914 | Val Loss: 0.6039 | Val Accuracy: 2.7100%\nEpoch: 15 | Train Loss: 0.6374 | Val Loss: 0.5749 | Val Accuracy: 5.0293%\nEpoch: 16 | Train Loss: 0.6156 | Val Loss: 0.5330 | Val Accuracy: 4.6875%\nEpoch: 17 | Train Loss: 0.5374 | Val Loss: 0.5591 | Val Accuracy: 8.8379%\nEpoch: 18 | Train Loss: 0.5509 | Val Loss: 0.5007 | Val Accuracy: 9.2529%\nEpoch: 19 | Train Loss: 0.5220 | Val Loss: 0.4733 | Val Accuracy: 9.3506%\nEpoch: 20 | Train Loss: 0.4792 | Val Loss: 0.4667 | Val Accuracy: 12.4268%\nEpoch: 21 | Train Loss: 0.4542 | Val Loss: 0.4624 | Val Accuracy: 12.8418%\nEpoch: 22 | Train Loss: 0.4638 | Val Loss: 0.4515 | Val Accuracy: 13.9893%\nEpoch: 23 | Train Loss: 0.4373 | Val Loss: 0.4311 | Val Accuracy: 16.5039%\nEpoch: 24 | Train Loss: 0.4274 | Val Loss: 0.4215 | Val Accuracy: 16.2598%\nEpoch: 25 | Train Loss: 0.4219 | Val Loss: 0.4195 | Val Accuracy: 16.2598%\nEpoch: 26 | Train Loss: 0.3924 | Val Loss: 0.4320 | Val Accuracy: 18.3838%\nEpoch: 27 | Train Loss: 0.3837 | Val Loss: 0.4091 | Val Accuracy: 20.1904%\nEpoch: 28 | Train Loss: 0.3905 | Val Loss: 0.3941 | Val Accuracy: 19.2139%\nEpoch: 29 | Train Loss: 0.3581 | Val Loss: 0.3939 | Val Accuracy: 22.0947%\nEpoch: 30 | Train Loss: 0.3749 | Val Loss: 0.3858 | Val Accuracy: 20.3613%\nEpoch: 31 | Train Loss: 0.3505 | Val Loss: 0.3782 | Val Accuracy: 22.4854%\nEpoch: 32 | Train Loss: 0.3332 | Val Loss: 0.3963 | Val Accuracy: 23.7549%\nEpoch: 33 | Train Loss: 0.3454 | Val Loss: 0.3709 | Val Accuracy: 22.9492%\nEpoch: 34 | Train Loss: 0.3397 | Val Loss: 0.3760 | Val Accuracy: 22.4121%\nEpoch: 35 | Train Loss: 0.3426 | Val Loss: 0.3564 | Val Accuracy: 23.5596%\nEpoch: 36 | Train Loss: 0.3210 | Val Loss: 0.3764 | Val Accuracy: 26.4160%\nEpoch: 37 | Train Loss: 0.3268 | Val Loss: 0.3560 | Val Accuracy: 25.4639%\nEpoch: 38 | Train Loss: 0.3223 | Val Loss: 0.3619 | Val Accuracy: 26.0010%\nEpoch: 39 | Train Loss: 0.3147 | Val Loss: 0.3580 | Val Accuracy: 25.8301%\nEpoch: 40 | Train Loss: 0.3177 | Val Loss: 0.3595 | Val Accuracy: 26.6602%\nEpoch: 41 | Train Loss: 0.2838 | Val Loss: 0.3539 | Val Accuracy: 29.4434%\nEpoch: 42 | Train Loss: 0.2876 | Val Loss: 0.3623 | Val Accuracy: 28.4912%\nEpoch: 43 | Train Loss: 0.3157 | Val Loss: 0.3405 | Val Accuracy: 27.5391%\nEpoch: 44 | Train Loss: 0.2920 | Val Loss: 0.3568 | Val Accuracy: 29.9072%\nEpoch: 45 | Train Loss: 0.2945 | Val Loss: 0.3357 | Val Accuracy: 29.4434%\nEpoch: 46 | Train Loss: 0.2523 | Val Loss: 0.3701 | Val Accuracy: 29.8828%\nEpoch: 47 | Train Loss: 0.2814 | Val Loss: 0.3433 | Val Accuracy: 28.1982%\nEpoch: 48 | Train Loss: 0.2797 | Val Loss: 0.3348 | Val Accuracy: 28.9062%\nEpoch: 49 | Train Loss: 0.2870 | Val Loss: 0.3232 | Val Accuracy: 28.8818%\nEpoch: 50 | Train Loss: 0.2693 | Val Loss: 0.3299 | Val Accuracy: 30.1025%\nEpoch: 51 | Train Loss: 0.2509 | Val Loss: 0.3328 | Val Accuracy: 30.7617%\nEpoch: 52 | Train Loss: 0.2741 | Val Loss: 0.3284 | Val Accuracy: 30.4932%\nEpoch: 53 | Train Loss: 0.2504 | Val Loss: 0.3336 | Val Accuracy: 31.4697%\nEpoch: 54 | Train Loss: 0.2483 | Val Loss: 0.3349 | Val Accuracy: 31.0059%\nEpoch: 55 | Train Loss: 0.2466 | Val Loss: 0.3415 | Val Accuracy: 31.9336%\nEpoch: 56 | Train Loss: 0.2344 | Val Loss: 0.3375 | Val Accuracy: 30.6641%\nEpoch: 57 | Train Loss: 0.2607 | Val Loss: 0.3195 | Val Accuracy: 29.6387%\nEpoch: 58 | Train Loss: 0.2083 | Val Loss: 0.3493 | Val Accuracy: 32.6416%\nEpoch: 59 | Train Loss: 0.2163 | Val Loss: 0.3392 | Val Accuracy: 32.6172%\nTest Accuracy: 32.6172%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▃▃▄▄▄▄▅▅▅▆▆▆▇▆▇▇▇▇▇▇▇▇████▇█</td></tr><tr><td>validation_loss</td><td>██▇▇▆▆▆▅▅▄▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.32617</td></tr><tr><td>training_loss</td><td>0.21634</td></tr><tr><td>validation_accuracy</td><td>0.32617</td></tr><tr><td>validation_loss</td><td>0.3392</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">curious-sweep-6</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/wvcq42kz' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/wvcq42kz</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_164933-wvcq42kz/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vvb29xf5 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Nadam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.75\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_171303-vvb29xf5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/vvb29xf5' target=\"_blank\">fine-sweep-7</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/vvb29xf5' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/vvb29xf5</a>"},"metadata":{}},{"name":"stdout","text":"with attention\nEpoch: 0 | Train Loss: 1.3823 | Val Loss: 1.0658 | Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.2049 | Val Loss: 1.0422 | Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.1317 | Val Loss: 0.9565 | Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 1.0917 | Val Loss: 0.9466 | Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 1.0935 | Val Loss: 0.9345 | Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 1.0820 | Val Loss: 1.0058 | Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 1.0837 | Val Loss: 1.0022 | Val Accuracy: 0.0000%\nEpoch: 7 | Train Loss: 1.0663 | Val Loss: 0.9221 | Val Accuracy: 0.0000%\nEpoch: 8 | Train Loss: 1.0573 | Val Loss: 0.9091 | Val Accuracy: 0.0000%\nEpoch: 9 | Train Loss: 1.0249 | Val Loss: 0.8968 | Val Accuracy: 0.0000%\nEpoch: 10 | Train Loss: 0.9973 | Val Loss: 0.8542 | Val Accuracy: 0.0000%\nEpoch: 11 | Train Loss: 0.9707 | Val Loss: 0.8620 | Val Accuracy: 0.0000%\nEpoch: 12 | Train Loss: 0.9605 | Val Loss: 0.8330 | Val Accuracy: 0.0244%\nEpoch: 13 | Train Loss: 0.9469 | Val Loss: 0.8196 | Val Accuracy: 0.0000%\nEpoch: 14 | Train Loss: 0.9224 | Val Loss: 0.7868 | Val Accuracy: 0.0488%\nEpoch: 15 | Train Loss: 0.8908 | Val Loss: 0.7848 | Val Accuracy: 0.0977%\nEpoch: 16 | Train Loss: 0.8569 | Val Loss: 0.7464 | Val Accuracy: 0.0732%\nEpoch: 17 | Train Loss: 0.8108 | Val Loss: 0.7155 | Val Accuracy: 0.1465%\nEpoch: 18 | Train Loss: 0.7813 | Val Loss: 0.6992 | Val Accuracy: 0.3418%\nEpoch: 19 | Train Loss: 0.7640 | Val Loss: 0.6554 | Val Accuracy: 0.4883%\nEpoch: 20 | Train Loss: 0.7449 | Val Loss: 0.6641 | Val Accuracy: 0.4395%\nEpoch: 21 | Train Loss: 0.7226 | Val Loss: 0.6216 | Val Accuracy: 1.1230%\nEpoch: 22 | Train Loss: 0.6922 | Val Loss: 0.5980 | Val Accuracy: 1.6113%\nEpoch: 23 | Train Loss: 0.6755 | Val Loss: 0.5713 | Val Accuracy: 2.3438%\nEpoch: 24 | Train Loss: 0.6600 | Val Loss: 0.5673 | Val Accuracy: 2.0996%\nEpoch: 25 | Train Loss: 0.6260 | Val Loss: 0.5559 | Val Accuracy: 3.4424%\nEpoch: 26 | Train Loss: 0.6081 | Val Loss: 0.5298 | Val Accuracy: 3.9551%\nEpoch: 27 | Train Loss: 0.5903 | Val Loss: 0.5195 | Val Accuracy: 5.4199%\nEpoch: 28 | Train Loss: 0.5581 | Val Loss: 0.5100 | Val Accuracy: 5.3955%\nEpoch: 29 | Train Loss: 0.5549 | Val Loss: 0.4821 | Val Accuracy: 6.4697%\nEpoch: 30 | Train Loss: 0.5249 | Val Loss: 0.4716 | Val Accuracy: 7.6660%\nEpoch: 31 | Train Loss: 0.4979 | Val Loss: 0.4754 | Val Accuracy: 9.5215%\nEpoch: 32 | Train Loss: 0.4816 | Val Loss: 0.4476 | Val Accuracy: 9.7900%\nEpoch: 33 | Train Loss: 0.4451 | Val Loss: 0.4914 | Val Accuracy: 12.7441%\nEpoch: 34 | Train Loss: 0.4480 | Val Loss: 0.4371 | Val Accuracy: 11.5479%\nEpoch: 35 | Train Loss: 0.4323 | Val Loss: 0.4457 | Val Accuracy: 14.1846%\nEpoch: 36 | Train Loss: 0.4122 | Val Loss: 0.4283 | Val Accuracy: 15.3076%\nEpoch: 37 | Train Loss: 0.4034 | Val Loss: 0.4234 | Val Accuracy: 16.5527%\nEpoch: 38 | Train Loss: 0.3955 | Val Loss: 0.4064 | Val Accuracy: 16.0400%\nEpoch: 39 | Train Loss: 0.3711 | Val Loss: 0.4087 | Val Accuracy: 17.2119%\nEpoch: 40 | Train Loss: 0.3501 | Val Loss: 0.4107 | Val Accuracy: 18.9209%\nEpoch: 41 | Train Loss: 0.3302 | Val Loss: 0.4054 | Val Accuracy: 20.2637%\nEpoch: 42 | Train Loss: 0.3242 | Val Loss: 0.4031 | Val Accuracy: 19.0918%\nEpoch: 43 | Train Loss: 0.3012 | Val Loss: 0.4273 | Val Accuracy: 20.8740%\nEpoch: 44 | Train Loss: 0.3059 | Val Loss: 0.3928 | Val Accuracy: 21.2158%\nEpoch: 45 | Train Loss: 0.2994 | Val Loss: 0.3841 | Val Accuracy: 20.7764%\nEpoch: 46 | Train Loss: 0.2933 | Val Loss: 0.3977 | Val Accuracy: 22.2656%\nEpoch: 47 | Train Loss: 0.2831 | Val Loss: 0.3938 | Val Accuracy: 24.0723%\nEpoch: 48 | Train Loss: 0.2555 | Val Loss: 0.3925 | Val Accuracy: 24.6338%\nEpoch: 49 | Train Loss: 0.2499 | Val Loss: 0.3957 | Val Accuracy: 24.6826%\nEpoch: 50 | Train Loss: 0.2625 | Val Loss: 0.3863 | Val Accuracy: 23.1689%\nEpoch: 51 | Train Loss: 0.2492 | Val Loss: 0.3945 | Val Accuracy: 22.9248%\nEpoch: 52 | Train Loss: 0.2380 | Val Loss: 0.3993 | Val Accuracy: 24.6826%\nEpoch: 53 | Train Loss: 0.2238 | Val Loss: 0.3942 | Val Accuracy: 25.0000%\nEpoch: 54 | Train Loss: 0.2294 | Val Loss: 0.4011 | Val Accuracy: 25.8789%\nEpoch: 55 | Train Loss: 0.2083 | Val Loss: 0.4250 | Val Accuracy: 25.9521%\nEpoch: 56 | Train Loss: 0.2040 | Val Loss: 0.4150 | Val Accuracy: 25.8545%\nEpoch: 57 | Train Loss: 0.2060 | Val Loss: 0.4259 | Val Accuracy: 26.0498%\nEpoch: 58 | Train Loss: 0.1899 | Val Loss: 0.4235 | Val Accuracy: 28.0029%\nEpoch: 59 | Train Loss: 0.1882 | Val Loss: 0.3893 | Val Accuracy: 26.7822%\nTest Accuracy: 27.5146%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▇▆▆▆▆▆▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▄▄▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>validation_loss</td><td>██▇▇▇▇▆▆▆▅▅▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.27515</td></tr><tr><td>training_loss</td><td>0.18824</td></tr><tr><td>validation_accuracy</td><td>0.26782</td></tr><tr><td>validation_loss</td><td>0.38928</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fine-sweep-7</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/vvb29xf5' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/vvb29xf5</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_171303-vvb29xf5/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xclnet9j with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Nadam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.25\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_174831-xclnet9j</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/xclnet9j' target=\"_blank\">lemon-sweep-8</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/xclnet9j' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/xclnet9j</a>"},"metadata":{}},{"name":"stdout","text":"with attention\nEpoch: 0 | Train Loss: 1.5887 | Val Loss: 1.0814 | Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.1964 | Val Loss: 0.9735 | Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.1060 | Val Loss: 0.9331 | Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 1.0846 | Val Loss: 0.9269 | Val Accuracy: 0.0244%\nEpoch: 4 | Train Loss: 1.0712 | Val Loss: 0.9225 | Val Accuracy: 0.0244%\nEpoch: 5 | Train Loss: 1.0664 | Val Loss: 0.9312 | Val Accuracy: 0.0000%\nEpoch: 6 | Train Loss: 1.0481 | Val Loss: 0.9295 | Val Accuracy: 0.0000%\nEpoch: 7 | Train Loss: 1.0285 | Val Loss: 0.9402 | Val Accuracy: 0.0000%\nEpoch: 8 | Train Loss: 1.0191 | Val Loss: 0.9538 | Val Accuracy: 0.0000%\nEpoch: 9 | Train Loss: 0.9942 | Val Loss: 0.8970 | Val Accuracy: 0.0000%\nEpoch: 10 | Train Loss: 0.9810 | Val Loss: 0.8907 | Val Accuracy: 0.0000%\nEpoch: 11 | Train Loss: 0.9642 | Val Loss: 0.8352 | Val Accuracy: 0.0488%\nEpoch: 12 | Train Loss: 0.8995 | Val Loss: 0.7952 | Val Accuracy: 0.0732%\nEpoch: 13 | Train Loss: 0.8736 | Val Loss: 0.7557 | Val Accuracy: 0.1709%\nEpoch: 14 | Train Loss: 0.7983 | Val Loss: 0.7059 | Val Accuracy: 0.3662%\nEpoch: 15 | Train Loss: 0.7523 | Val Loss: 0.6470 | Val Accuracy: 1.0742%\nEpoch: 16 | Train Loss: 0.6948 | Val Loss: 0.5663 | Val Accuracy: 2.3682%\nEpoch: 17 | Train Loss: 0.6066 | Val Loss: 0.5279 | Val Accuracy: 4.6631%\nEpoch: 18 | Train Loss: 0.5693 | Val Loss: 0.4769 | Val Accuracy: 7.4707%\nEpoch: 19 | Train Loss: 0.5071 | Val Loss: 0.4488 | Val Accuracy: 11.1328%\nEpoch: 20 | Train Loss: 0.4562 | Val Loss: 0.4629 | Val Accuracy: 11.7920%\nEpoch: 21 | Train Loss: 0.4415 | Val Loss: 0.4109 | Val Accuracy: 15.6250%\nEpoch: 22 | Train Loss: 0.4288 | Val Loss: 0.3913 | Val Accuracy: 17.7490%\nEpoch: 23 | Train Loss: 0.3860 | Val Loss: 0.3830 | Val Accuracy: 19.4580%\nEpoch: 24 | Train Loss: 0.3929 | Val Loss: 0.3913 | Val Accuracy: 17.5049%\nEpoch: 25 | Train Loss: 0.3577 | Val Loss: 0.3682 | Val Accuracy: 21.8994%\nEpoch: 26 | Train Loss: 0.3394 | Val Loss: 0.3613 | Val Accuracy: 23.3154%\nEpoch: 27 | Train Loss: 0.3550 | Val Loss: 0.3531 | Val Accuracy: 21.4844%\nEpoch: 28 | Train Loss: 0.3424 | Val Loss: 0.3411 | Val Accuracy: 22.6807%\nEpoch: 29 | Train Loss: 0.3217 | Val Loss: 0.3603 | Val Accuracy: 23.0713%\nEpoch: 30 | Train Loss: 0.3155 | Val Loss: 0.3455 | Val Accuracy: 25.4639%\nEpoch: 31 | Train Loss: 0.3081 | Val Loss: 0.3236 | Val Accuracy: 26.6113%\nEpoch: 32 | Train Loss: 0.2854 | Val Loss: 0.3374 | Val Accuracy: 28.1250%\nEpoch: 33 | Train Loss: 0.2915 | Val Loss: 0.3255 | Val Accuracy: 29.3457%\nEpoch: 34 | Train Loss: 0.3014 | Val Loss: 0.3165 | Val Accuracy: 26.9775%\nEpoch: 35 | Train Loss: 0.2605 | Val Loss: 0.3549 | Val Accuracy: 27.7344%\nEpoch: 36 | Train Loss: 0.2555 | Val Loss: 0.3301 | Val Accuracy: 29.1260%\nEpoch: 37 | Train Loss: 0.2342 | Val Loss: 0.3263 | Val Accuracy: 31.8359%\nEpoch: 38 | Train Loss: 0.2385 | Val Loss: 0.3179 | Val Accuracy: 29.5166%\nEpoch: 39 | Train Loss: 0.2199 | Val Loss: 0.3232 | Val Accuracy: 30.3955%\nEpoch: 40 | Train Loss: 0.2214 | Val Loss: 0.3244 | Val Accuracy: 32.7393%\nEpoch: 41 | Train Loss: 0.2288 | Val Loss: 0.3497 | Val Accuracy: 30.2002%\nEpoch: 42 | Train Loss: 0.2107 | Val Loss: 0.3262 | Val Accuracy: 33.7402%\nEpoch: 43 | Train Loss: 0.2125 | Val Loss: 0.3102 | Val Accuracy: 32.5195%\nEpoch: 44 | Train Loss: 0.1932 | Val Loss: 0.3299 | Val Accuracy: 32.8857%\nEpoch: 45 | Train Loss: 0.1778 | Val Loss: 0.3259 | Val Accuracy: 33.7646%\nEpoch: 46 | Train Loss: 0.1864 | Val Loss: 0.3242 | Val Accuracy: 32.5684%\nEpoch: 47 | Train Loss: 0.1993 | Val Loss: 0.3148 | Val Accuracy: 32.1777%\nEpoch: 48 | Train Loss: 0.1761 | Val Loss: 0.3332 | Val Accuracy: 32.6172%\nEpoch: 49 | Train Loss: 0.1816 | Val Loss: 0.3162 | Val Accuracy: 33.0322%\nEpoch: 50 | Train Loss: 0.1546 | Val Loss: 0.3281 | Val Accuracy: 35.3760%\nEpoch: 51 | Train Loss: 0.1669 | Val Loss: 0.3297 | Val Accuracy: 33.4473%\nEpoch: 52 | Train Loss: 0.1472 | Val Loss: 0.3399 | Val Accuracy: 34.2773%\nEpoch: 53 | Train Loss: 0.1341 | Val Loss: 0.3431 | Val Accuracy: 34.4482%\nEpoch: 54 | Train Loss: 0.1363 | Val Loss: 0.3543 | Val Accuracy: 34.0820%\nEpoch: 55 | Train Loss: 0.1369 | Val Loss: 0.3384 | Val Accuracy: 34.4238%\nEpoch: 56 | Train Loss: 0.1215 | Val Loss: 0.3478 | Val Accuracy: 34.3018%\nEpoch: 57 | Train Loss: 0.1172 | Val Loss: 0.3525 | Val Accuracy: 33.4229%\nEpoch: 58 | Train Loss: 0.1255 | Val Loss: 0.3588 | Val Accuracy: 34.5703%\nEpoch: 59 | Train Loss: 0.1037 | Val Loss: 0.3657 | Val Accuracy: 34.1797%\nTest Accuracy: 34.2285%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▆▆▆▅▅▅▅▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▃▄▅▅▅▅▆▆▆▇▆▇▇▇█████████████</td></tr><tr><td>validation_loss</td><td>█▇▇▇▇▇▆▆▅▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.34229</td></tr><tr><td>training_loss</td><td>0.10372</td></tr><tr><td>validation_accuracy</td><td>0.3418</td></tr><tr><td>validation_loss</td><td>0.36565</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lemon-sweep-8</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/xclnet9j' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/xclnet9j</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_174831-xclnet9j/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f72road4 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Nadam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.25\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_182348-f72road4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/f72road4' target=\"_blank\">feasible-sweep-9</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/f72road4' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/f72road4</a>"},"metadata":{}},{"name":"stdout","text":"with attention\nEpoch: 0 | Train Loss: 1.4723 | Val Loss: 1.0920 | Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.2071 | Val Loss: 1.0824 | Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.1693 | Val Loss: 1.0342 | Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 1.1139 | Val Loss: 0.9294 | Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 1.0413 | Val Loss: 0.9053 | Val Accuracy: 0.0244%\nEpoch: 5 | Train Loss: 0.9864 | Val Loss: 0.8520 | Val Accuracy: 0.0244%\nEpoch: 6 | Train Loss: 0.9329 | Val Loss: 0.8083 | Val Accuracy: 0.1465%\nEpoch: 7 | Train Loss: 0.8609 | Val Loss: 0.7355 | Val Accuracy: 0.1221%\nEpoch: 8 | Train Loss: 0.8103 | Val Loss: 0.6690 | Val Accuracy: 0.5371%\nEpoch: 9 | Train Loss: 0.7249 | Val Loss: 0.5947 | Val Accuracy: 1.9043%\nEpoch: 10 | Train Loss: 0.5875 | Val Loss: 0.5441 | Val Accuracy: 5.9326%\nEpoch: 11 | Train Loss: 0.5534 | Val Loss: 0.4541 | Val Accuracy: 10.7178%\nEpoch: 12 | Train Loss: 0.4507 | Val Loss: 0.4564 | Val Accuracy: 16.0645%\nEpoch: 13 | Train Loss: 0.4320 | Val Loss: 0.4120 | Val Accuracy: 18.3350%\nEpoch: 14 | Train Loss: 0.3989 | Val Loss: 0.3816 | Val Accuracy: 20.1660%\nEpoch: 15 | Train Loss: 0.3735 | Val Loss: 0.3562 | Val Accuracy: 21.5576%\nEpoch: 16 | Train Loss: 0.3505 | Val Loss: 0.3537 | Val Accuracy: 22.1924%\nEpoch: 17 | Train Loss: 0.3346 | Val Loss: 0.3382 | Val Accuracy: 25.8301%\nEpoch: 18 | Train Loss: 0.3111 | Val Loss: 0.3512 | Val Accuracy: 23.1689%\nEpoch: 19 | Train Loss: 0.2759 | Val Loss: 0.3428 | Val Accuracy: 27.9541%\nEpoch: 20 | Train Loss: 0.2640 | Val Loss: 0.3346 | Val Accuracy: 29.5654%\nEpoch: 21 | Train Loss: 0.2604 | Val Loss: 0.3315 | Val Accuracy: 30.7129%\nEpoch: 22 | Train Loss: 0.2708 | Val Loss: 0.3209 | Val Accuracy: 28.3691%\nEpoch: 23 | Train Loss: 0.2652 | Val Loss: 0.3102 | Val Accuracy: 31.2012%\nEpoch: 24 | Train Loss: 0.2364 | Val Loss: 0.3267 | Val Accuracy: 30.8838%\nEpoch: 25 | Train Loss: 0.2391 | Val Loss: 0.3272 | Val Accuracy: 31.3965%\nEpoch: 26 | Train Loss: 0.2068 | Val Loss: 0.3397 | Val Accuracy: 33.7402%\nEpoch: 27 | Train Loss: 0.2060 | Val Loss: 0.3193 | Val Accuracy: 32.8613%\nEpoch: 28 | Train Loss: 0.2245 | Val Loss: 0.3158 | Val Accuracy: 31.5674%\nEpoch: 29 | Train Loss: 0.1925 | Val Loss: 0.3279 | Val Accuracy: 34.6191%\nEpoch: 30 | Train Loss: 0.1703 | Val Loss: 0.3331 | Val Accuracy: 34.5947%\nEpoch: 31 | Train Loss: 0.1753 | Val Loss: 0.3172 | Val Accuracy: 33.7891%\nEpoch: 32 | Train Loss: 0.1668 | Val Loss: 0.3331 | Val Accuracy: 35.6689%\nEpoch: 33 | Train Loss: 0.1441 | Val Loss: 0.3361 | Val Accuracy: 36.8408%\nEpoch: 34 | Train Loss: 0.1630 | Val Loss: 0.3130 | Val Accuracy: 34.7168%\nEpoch: 35 | Train Loss: 0.1387 | Val Loss: 0.3415 | Val Accuracy: 34.2773%\nEpoch: 36 | Train Loss: 0.1336 | Val Loss: 0.3407 | Val Accuracy: 35.4980%\nEpoch: 37 | Train Loss: 0.1196 | Val Loss: 0.3555 | Val Accuracy: 34.5703%\nEpoch: 38 | Train Loss: 0.1102 | Val Loss: 0.3550 | Val Accuracy: 34.9121%\nEpoch: 39 | Train Loss: 0.1095 | Val Loss: 0.3638 | Val Accuracy: 34.3994%\nEpoch: 40 | Train Loss: 0.0924 | Val Loss: 0.3769 | Val Accuracy: 36.3281%\nEpoch: 41 | Train Loss: 0.0927 | Val Loss: 0.3686 | Val Accuracy: 36.1328%\nEpoch: 42 | Train Loss: 0.0892 | Val Loss: 0.3836 | Val Accuracy: 37.3535%\nEpoch: 43 | Train Loss: 0.0820 | Val Loss: 0.4095 | Val Accuracy: 35.1807%\nEpoch: 44 | Train Loss: 0.0796 | Val Loss: 0.3795 | Val Accuracy: 34.7168%\nEpoch: 45 | Train Loss: 0.0747 | Val Loss: 0.3792 | Val Accuracy: 35.8887%\nEpoch: 46 | Train Loss: 0.0765 | Val Loss: 0.3893 | Val Accuracy: 36.2549%\nEpoch: 47 | Train Loss: 0.0631 | Val Loss: 0.4151 | Val Accuracy: 35.7910%\nEpoch: 48 | Train Loss: 0.0573 | Val Loss: 0.4255 | Val Accuracy: 36.0107%\nEpoch: 49 | Train Loss: 0.0576 | Val Loss: 0.4248 | Val Accuracy: 36.3525%\nEpoch: 50 | Train Loss: 0.0567 | Val Loss: 0.4486 | Val Accuracy: 35.4248%\nEpoch: 51 | Train Loss: 0.0477 | Val Loss: 0.4310 | Val Accuracy: 35.9619%\nEpoch: 52 | Train Loss: 0.0496 | Val Loss: 0.4343 | Val Accuracy: 35.2295%\nEpoch: 53 | Train Loss: 0.0424 | Val Loss: 0.4470 | Val Accuracy: 35.6201%\nEpoch: 54 | Train Loss: 0.0425 | Val Loss: 0.4502 | Val Accuracy: 35.7422%\nEpoch: 55 | Train Loss: 0.0391 | Val Loss: 0.4537 | Val Accuracy: 36.7188%\nEpoch: 56 | Train Loss: 0.0427 | Val Loss: 0.4475 | Val Accuracy: 36.7188%\nEpoch: 57 | Train Loss: 0.0403 | Val Loss: 0.4573 | Val Accuracy: 36.3281%\nEpoch: 58 | Train Loss: 0.0363 | Val Loss: 0.4811 | Val Accuracy: 35.8643%\nEpoch: 59 | Train Loss: 0.0311 | Val Loss: 0.4784 | Val Accuracy: 36.9873%\nTest Accuracy: 36.1572%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▂▄▄▅▅▅▆▇▆▇▇▇▇▇▇███▇▇█████████████</td></tr><tr><td>validation_loss</td><td>██▇▆▅▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.36157</td></tr><tr><td>training_loss</td><td>0.03113</td></tr><tr><td>validation_accuracy</td><td>0.36987</td></tr><tr><td>validation_loss</td><td>0.47844</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">feasible-sweep-9</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/f72road4' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/f72road4</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_182348-f72road4/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7gg2pvgv with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.25\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_185946-7gg2pvgv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/7gg2pvgv' target=\"_blank\">pleasant-sweep-10</a></strong> to <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/sweeps/pvpipb29</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/7gg2pvgv' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/7gg2pvgv</a>"},"metadata":{}},{"name":"stdout","text":"with attention\nEpoch: 0 | Train Loss: 1.4341 | Val Loss: 1.0865 | Val Accuracy: 0.0000%\nEpoch: 1 | Train Loss: 1.2039 | Val Loss: 1.0282 | Val Accuracy: 0.0000%\nEpoch: 2 | Train Loss: 1.1934 | Val Loss: 1.0467 | Val Accuracy: 0.0000%\nEpoch: 3 | Train Loss: 1.1436 | Val Loss: 1.0648 | Val Accuracy: 0.0000%\nEpoch: 4 | Train Loss: 1.1052 | Val Loss: 0.9437 | Val Accuracy: 0.0000%\nEpoch: 5 | Train Loss: 1.0466 | Val Loss: 0.8992 | Val Accuracy: 0.0488%\nEpoch: 6 | Train Loss: 1.0069 | Val Loss: 0.8633 | Val Accuracy: 0.0244%\nEpoch: 7 | Train Loss: 0.9641 | Val Loss: 0.8312 | Val Accuracy: 0.0732%\nEpoch: 8 | Train Loss: 0.9277 | Val Loss: 0.7627 | Val Accuracy: 0.1465%\nEpoch: 9 | Train Loss: 0.8851 | Val Loss: 0.7721 | Val Accuracy: 0.0244%\nEpoch: 10 | Train Loss: 0.8572 | Val Loss: 0.7389 | Val Accuracy: 0.1465%\nEpoch: 11 | Train Loss: 0.8175 | Val Loss: 0.7084 | Val Accuracy: 0.2197%\nEpoch: 12 | Train Loss: 0.8054 | Val Loss: 0.6824 | Val Accuracy: 0.3662%\nEpoch: 13 | Train Loss: 0.7565 | Val Loss: 0.6494 | Val Accuracy: 0.5859%\nEpoch: 14 | Train Loss: 0.7363 | Val Loss: 0.6322 | Val Accuracy: 1.1230%\nEpoch: 15 | Train Loss: 0.7020 | Val Loss: 0.5994 | Val Accuracy: 2.1729%\nEpoch: 16 | Train Loss: 0.6787 | Val Loss: 0.5764 | Val Accuracy: 2.9297%\nEpoch: 17 | Train Loss: 0.6471 | Val Loss: 0.5488 | Val Accuracy: 4.1016%\nEpoch: 18 | Train Loss: 0.6164 | Val Loss: 0.5194 | Val Accuracy: 5.7617%\nEpoch: 19 | Train Loss: 0.5819 | Val Loss: 0.5109 | Val Accuracy: 7.4463%\nEpoch: 20 | Train Loss: 0.5673 | Val Loss: 0.4790 | Val Accuracy: 8.5693%\nEpoch: 21 | Train Loss: 0.5339 | Val Loss: 0.4653 | Val Accuracy: 9.5703%\nEpoch: 22 | Train Loss: 0.5088 | Val Loss: 0.4528 | Val Accuracy: 11.3525%\nEpoch: 23 | Train Loss: 0.4761 | Val Loss: 0.4427 | Val Accuracy: 12.1582%\nEpoch: 24 | Train Loss: 0.4612 | Val Loss: 0.4244 | Val Accuracy: 13.8428%\nEpoch: 25 | Train Loss: 0.4413 | Val Loss: 0.4220 | Val Accuracy: 12.7930%\nEpoch: 26 | Train Loss: 0.4251 | Val Loss: 0.4194 | Val Accuracy: 15.5029%\nEpoch: 27 | Train Loss: 0.3993 | Val Loss: 0.4086 | Val Accuracy: 16.2354%\nEpoch: 28 | Train Loss: 0.3817 | Val Loss: 0.4062 | Val Accuracy: 16.8457%\nEpoch: 29 | Train Loss: 0.3725 | Val Loss: 0.3979 | Val Accuracy: 15.5762%\nEpoch: 30 | Train Loss: 0.3647 | Val Loss: 0.4050 | Val Accuracy: 17.1875%\nEpoch: 31 | Train Loss: 0.3655 | Val Loss: 0.3914 | Val Accuracy: 18.4570%\nEpoch: 32 | Train Loss: 0.3223 | Val Loss: 0.4065 | Val Accuracy: 19.8730%\nEpoch: 33 | Train Loss: 0.3269 | Val Loss: 0.3931 | Val Accuracy: 19.8486%\nEpoch: 34 | Train Loss: 0.3347 | Val Loss: 0.3750 | Val Accuracy: 19.3848%\nEpoch: 35 | Train Loss: 0.3155 | Val Loss: 0.3831 | Val Accuracy: 19.8975%\nEpoch: 36 | Train Loss: 0.3126 | Val Loss: 0.4194 | Val Accuracy: 15.7959%\nEpoch: 37 | Train Loss: 0.2899 | Val Loss: 0.3852 | Val Accuracy: 21.8262%\nEpoch: 38 | Train Loss: 0.2812 | Val Loss: 0.4069 | Val Accuracy: 21.7773%\nEpoch: 39 | Train Loss: 0.2790 | Val Loss: 0.3837 | Val Accuracy: 22.6807%\nEpoch: 40 | Train Loss: 0.2704 | Val Loss: 0.3920 | Val Accuracy: 22.5830%\nEpoch: 41 | Train Loss: 0.2649 | Val Loss: 0.3902 | Val Accuracy: 22.4121%\nEpoch: 42 | Train Loss: 0.2504 | Val Loss: 0.3903 | Val Accuracy: 22.7295%\nEpoch: 43 | Train Loss: 0.2547 | Val Loss: 0.3871 | Val Accuracy: 23.4619%\nEpoch: 44 | Train Loss: 0.2445 | Val Loss: 0.3973 | Val Accuracy: 23.0469%\nEpoch: 45 | Train Loss: 0.2463 | Val Loss: 0.3914 | Val Accuracy: 24.3652%\nEpoch: 46 | Train Loss: 0.2242 | Val Loss: 0.3995 | Val Accuracy: 23.6816%\nEpoch: 47 | Train Loss: 0.2207 | Val Loss: 0.3966 | Val Accuracy: 24.0723%\nEpoch: 48 | Train Loss: 0.2275 | Val Loss: 0.3976 | Val Accuracy: 24.1943%\nEpoch: 49 | Train Loss: 0.2071 | Val Loss: 0.4151 | Val Accuracy: 24.0723%\nEpoch: 50 | Train Loss: 0.1982 | Val Loss: 0.4086 | Val Accuracy: 25.3662%\nEpoch: 51 | Train Loss: 0.1979 | Val Loss: 0.4071 | Val Accuracy: 23.5352%\nEpoch: 52 | Train Loss: 0.2023 | Val Loss: 0.4191 | Val Accuracy: 23.5596%\nEpoch: 53 | Train Loss: 0.1765 | Val Loss: 0.4240 | Val Accuracy: 23.9502%\nEpoch: 54 | Train Loss: 0.1878 | Val Loss: 0.4184 | Val Accuracy: 25.7568%\nEpoch: 55 | Train Loss: 0.1727 | Val Loss: 0.4276 | Val Accuracy: 25.7812%\nEpoch: 56 | Train Loss: 0.1770 | Val Loss: 0.4364 | Val Accuracy: 25.1221%\nEpoch: 57 | Train Loss: 0.1711 | Val Loss: 0.4330 | Val Accuracy: 26.1719%\nEpoch: 58 | Train Loss: 0.1847 | Val Loss: 0.4266 | Val Accuracy: 25.8789%\nEpoch: 59 | Train Loss: 0.1642 | Val Loss: 0.4361 | Val Accuracy: 25.1953%\nTest Accuracy: 25.5615%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_loss</td><td>█▇▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▃▃▄▄▅▄▅▆▆▆▆▆▅▇▇▇▇▇█▇▇▇▇▇████</td></tr><tr><td>validation_loss</td><td>█▇█▇▆▅▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>60</td></tr><tr><td>test_accuracy</td><td>0.25562</td></tr><tr><td>training_loss</td><td>0.16425</td></tr><tr><td>validation_accuracy</td><td>0.25195</td></tr><tr><td>validation_loss</td><td>0.43613</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">pleasant-sweep-10</strong> at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/7gg2pvgv' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN/runs/7gg2pvgv</a><br/> View project at: <a href='https://wandb.ai/believer12/DL_A-03_AttentionRNN' target=\"_blank\">https://wandb.ai/believer12/DL_A-03_AttentionRNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_185946-7gg2pvgv/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}